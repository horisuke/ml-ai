## TensorFlow/Kerasメモ
* Tensorflowとは
    * Googleが中心に開発しているOSSの機械学習ライブラリ。もともとはGoogle内部で使用するためにGoogle Brainチームによって開発されたもの。
    * Googlr Brainの成果はGoogle検索ランキングやGoogleフォトの画像分類、音声認識などの商用サービスで利用されている。
* 深層学習とは
    * 人間の「脳」のニューロンを模したニューラルネットワークを何層にも重ね、大規模にした機械学習の一手法
    * 2012年のILSVRC(ImageNet Large Scale Visual Recognition Challenge)という画像認識コンペティションで深層学習を採用したチームが圧倒的大差で優勝した。
    * また同じ2012年にGoogleが「深層学習によって教師データなしで猫の概念を自動で学習できた」ことを発表。
    * 2012年に起きたこれらの成果が現在の深層学習・AIブームに繋がっている。
* 深層学習でできること
    * 大きく画像処理、自然言語処理、音声処理、強化学習などを行うことができる。
    * 画像分類
        * 画像に写っているものを推定するタスクのこと。
            * この写真に写っているのは0～9のどの数字か？
            * この顔写真は男性か女性か？
        * ILSVRCではフラミンゴやゴンドラといった1000クラスで分類を行い、その精度を競っていた。
        * いわゆる教師あり学習と呼ばれるもので、事前に多くの画像と正解データが必要となる。ILSVRCでは約1000万枚のデータを用いている。
            * 教師あり学習：事前に与えられた正解データを元に学習する機械学習手法でラベルを推定する分類問題、連続値を推定する回帰問題などがある。
        * 画像収集はWebスクレイピングなどで半自動で行うことが多いが、正解データは人の手で作る必要があり、手間と時間がかかる。
        * そういう背景から、以下に少ないデータで学習するかという研究も活発に行われている。
            * 転移学習
                * ある問題を解くために学習したモデルを流用して別の問題を解くモデルを構築する手法
            * one-shot learning
                * 1つもしくはごく少数のサンプルのみで学習する機械学習の手法
        * 画像分類は各種クラウドサービスがAPIを提供している。機能や精度に違いはあるものの、専門的な知識なしに使えるようになっている。
            * Google Cloud Platform：Cloud Vision API
            * Microsoft Azure：Computer Vision API
            * Amazon AWS：Amazon Rekognition
            * IBM Cloud：Visual Recognition
    * 物体検出
        * 画像分類が「それが何か」を推定するのに対し、物体検出は1つ以上の物体が「何」で画像の「どこ」にあるかを推定すること。
        * 物体検出も各種クラウドサービスでAPIが提供されている。
        * Tensorflowとしては、Tensorflow Object Detection APIが利用可能となっており、画像データを準備すれば、その画像に関する物体検出をすぐに体験できる。
    * セグメンテーション
        * 物体を矩形領域で囲む物体検出に対し、セグメンテーションは物体をピクセル単位で物体ごとに推定すること。
        * 矩形領域を推定する物体検出に比べて、上位の技術のように考えられることもあるが、物体の数を数えたい場合などはセグメンテーションでは物体の重なりにより、うまく数えることができない、などデメリットもある。
    * 画像変換・画風変換
        * ある画像を別の画像に変換すること。
        * 例えば、風景写真を画家が描いた絵のように変換する画風変換も画像変換の1つ。
        * 画風変換は"A Neural Algorithm of Artistic Style"という論文で高速に変換できる手法が提案されている。
        * 他にも"pix2pix"という手法により、線画から写真を生成する手法や、"Cycle GAN"という手法により、夏の景色から冬の景色を生成する手法、白黒写真に自動的に着色する手法などが提案されている。
    * 超解像
        * 低解像画像から高解像画像を生成すること。2015年にwaifu2xというWebサービスも登場している。
        * 超解像も画像変換の一種だが、入力画像と出力画像のサイズが異なる、という特徴がある。
    * 画像生成
        * 画風変換や超解像のような入力データを画像として別の画像を生成するのに対し、ランダムな数値やテキストなどから別の画像を生成すること。
        * GAN(Generative Adversarial Network)と呼ばれる手法を元に画像生成する研究が進められている。
            * 多段の隠れ層と畳み込み層を持つネットワークを持つGANをDCGAN(Deep Convolutional Generative Adversarial Network)と呼ぶ。
        * 多くの改善手法も多数発表されており、リアルな画像生成ができるようになってきている。
        * GANは文章や時系列データの生成にGANを利用する研究も進められているため、画像生成にとどまらず、さまざまなタスクに利用されていく可能性がある。
    * 文章分類
        * その文章がどのカテゴリに属するかを推定するタスクのこと。
        * 有名な応用例としては、スパムメールの自動判定などが挙げられる。
        * 文書分類は様々なタスクの基礎となるタスクのため、自然言語処理の様々なアルゴリズムで利用されている。
            * 対話システムではユーザからの入力文の意図(Intent)を抽出する必要があるが、そこでも文書分類が使用されている。
            * GoogleのSmart ReplyではSmart Replyの対象にするかどうかに関して文書分類が使用されている。
    * 対話文生成
        * 画像生成と同様、文章を生成する。
        * 2015年の"A Neural Conversational Model"という論文では、映画の字幕データやIT系ヘルプデスクのやり取りのデータを使って、自然な会話文を生成できることを示した。
    * 機械翻訳
        * 機械翻訳の歴史は古く、当時はルールベースの手法だった。その後、統計的な手法を経て、現在では深層学習が使われるようになっている。
        * 2016年11月のGoogle翻訳のアップデートで英日翻訳の精度が劇的に改善したが、このアップデートで翻訳アルゴリズムが深層学習を用いた手法(GNMT：Google's Neural Machine Translation)に置き換わったからだとされている。
        * GNMTは詳細が公開されており、そのベースは対話文生成で用いられている技術と同じである。
    * 文書要約
        * 自然言語処理の主要な分野の1つだが、文書要約には様々な技術がある。
            * 単一文書要約：1つの長い文章を短くする
            * 複数文書要約：Twitterなどの短い文章が大量にある場合、それらを代表する文章(tweet)を抽出する
        * 上記のような抽出的アプローチだけでなく、必ずしも文章に含まれているとは限らない単語を使った文を生成する「生成的アプローチ」も研究が進んでいる。
            * 2016年にGoogle Brainチームが「ニュース冒頭文から良い見出しを生成するアルゴリズム」を発表
            * ニュースの冒頭の文章から見出しを生成し、人間的な要約を実現。
    * 対話システム
        * レストランのレコメンドなど特定の目的の達成を目指す「タスク指向」の対話システムとMicrosoftのりんなに代表される、特定の目的を持たない「非タスク指向」の対話システムがある。
        * ただ対話システムの実際の現場では、深層学習を用いた高度な手法よりも、コントロールしやすいルールベースの手法がまだ主流である。
        * 実用的な対話システムを深層学習で実現するためには、様々な要素を組み合わせる必要があり、単一の深層学習アルゴリズムだけでは人間的な対話がまだできるようにはなっていない。
    * 音声認識
        * 画像処理や自然言語処理と並び、音声認識も深層学習の主要な応用分野である。
        * 実アプリケーションでも深層学習の利用がされており、iPhoneやAndroid端末の音声認識の精度向上に繋がっている。
    * 音声合成・音楽生成
        * 2013年以降、音声合成や音楽生成の分野でも深層学習が利用され始めている。
            * Google Brainチームの取り組みの1つに深層学習をアートに適用する「Project Magenta」がある。
            * Project Magentaの成果物であるPerformance RNNを使うと、単純なMIDI信号からプロが演奏したような出力を得ることができる。
            * Google DeepMindによるWaveNetも2016年に注目を集めた。これまでの音楽生成は出力がMIDIの楽譜の場合が多かったが、WaveNetでは直接波形を生成し、質の高い音楽生成を実現している。
    * 声質変換
        * 普通の音声合成はテキストから声を生成するのに対し、声色だけを変換する技術、いわゆるボイスチェンジャーの発展系。
        * 声質変換技術を使うことで自分の話している内容を別の人の声に変換することができる。
    * ゲームの攻略
        * 2015年、DeepMindが開発したDQNがAtari2600というゲームにおいて、人間のゲーマー同等の強さを示した。
            * DQNはDeep Q Networkの略で深層学習と強化学習の一種Q Learningを組み合わせた手法。これまでは深層学習と強化学習の組み合わせで学習を安定させることができなかったが、様々な工夫により、安定的な学習を可能にした初めての例である。
        * 2016年には人間には勝てないと言われていた囲碁でDeepMindが開発したAlphaGoがトップ棋士を圧倒した。
        * DQNもAlphaGoも深層学習と強化学習を組み合わせた深層強化学習と呼ばれる手法である。
            * AlphaGoはDeepMind開発の囲碁プログラムで様々なバージョンがある。
                * AlphaGo Lee：トップ棋士に勝利したプログラム
                * AlphaGo Master：ネット碁でプロ棋士に60連勝を達成したプログラム
                * AlphaGo Zero：人間の棋譜を全く学習せず、3日間の学習でAlphaGo Lee/Masterに圧勝したプログラム
                * Alpha Zero：AlphaGo Zeroを将棋やチェスにも対応可能にしたプログラム
    * システムの制御
        * Preferred Networksは深層強化学習を使ってロボットカーが障害物を自動で避ける動作をゼロから学習するデモを公開。
        * DeepMindは"Deep Reinforcement Learning for Robotic Ⅳlanipulation with Asynchronous(Dff Policy Updates"でロボットアームにドアの開け方を自動学習させている。
    * 時系列データの予測・分類
        * 自然言語や音声データに限らず、深層学習は様々な時系列データに応用できる。
            * 株価予測
            * 行動推定(人間の体につけたジャイロセンサーのデータから歩いている、階段上がっているといった行動を推定する)
    * 異常検知
        * 他の多くのデータとは振る舞いが異なるデータを検出する技術のこと。
            * クレジットカードの不正検知、システムの故障検知、不良品の検出など
        * 異常検知自体は以前から行われていたが、深層学習により、精度の向上、対象データの拡大が行える。
* TensorFlowの特徴
    * 有向非巡回グラフ(DAG：Directed acyclic graph)
        * TensorFlowは有向非巡回グラフをベースとした処理系である。
        * Tensor(テンソル)はベクトルや行列を一般化した概念。
        * テンソル同士の演算結果はテンソルになるため、複雑な演算であっても、お互いに矢印で結ばれたループの内ネットワーク(=有向非巡回グラフ)で表現できる。
        * このネットワークは計算グラフとも呼ばれる。
        * 減速として、TensorFlowはPythonでこの計算グラフの定義を行い、その定義が完了してから複雑な処理をまとめて一気に行うことで高速な演算を実現している。
        * 加えて、グラフの実行は処理の速いC++で行う。また一気に計算することでその計算結果をPython側に転送するのもコストが小さくて済む。
    * いろいろな環境で動作
        * 基本的にCPUでもGPUでも同じコードを動かすことが可能。
        * Pythonを使って定義し、構築したグラフを保存し、それを別言語から呼び出すことも可能。
        * つまり、iPhoneやAndroid端末でも構築したグラフを動作させることができる。
        * "TensorFlow Lite"では、深層学習のモデルが通常、数百MBにもなってしまう場合にそのモデルを圧縮し、より制約のある端末でもモデルを使用することができる。
        * "TensorFlow.js"では、JavaScriptでTensorFlowのモデルを高速に実行できる。
        * "TensorFlow Serving"では、構築したモデルを簡単にAPIとして提供できる。
    * 分散処理
        * TensorFlowのホワイトペーパーではその分散処理の仕組みを中心に紹介されており、TensorFlowの大きな強みと言える。
        * 深層学習は計算量が非常に大きく、分散処理が必要になるケースが多くある。
        * 一般には分散処理を使いこなすのは非常に高度な技術力が必要となるが、TensorFlowを用いることでかんたんに分散処理を記述できる。
    * TensorBoardによる可視化
        * 深層学習はブラックボックスと言われるが、その表現力の高さゆえに何が起きているかわかりづらい、という問題がある。
        * TenaorFlowに付属するTensorBoardというツールは、学習時の損失関数の経過や中間層の様子、抽出した特徴量の埋め込みによる可視化などができ、今何が起きているかを理解する助けになる機能が備わっている。
        * 可視化した結果からデバッグや構築したモデルを理解したりできる。
    * 様々なレベルのAPIとエコシステム
        * 細かい制御が可能な低レベルAPIから高レベルAPIまで幅広くカバーしている。
        * 最新のver.ではCore APIの上にLayersやKeras、Estimatorなどの高レベルなAPIが提供されている。
        * pre-made EstimatorはEstimatorに含まれる基本的なネットワーク構造が決まっていて、パラメータを設定するだけですぐに学習を始められる仕組みで、TensorFlowの敷居がかなり下がる。
        * pre-made Estimatorで対応できないネットワークはLayerやKerasを用いて、レゴブロックのようにモデルを組み合わせて構築できる。
        * 2017年3月にはGoogle Cloud Machine Learning Engine(MLEngine)が公開され、インフラを準備しなくても、CPU/GPUを利用した分散学習が可能で、構築したモデルをそのままAPIとして利用できる。
        * TensorFLowはコミュニティが非常に大きく、他の深層学習ライブラリを圧倒している。最新の研究論文コードはTensorFlowで実装されていることも多く、コードを読んで理解したり、試すことが容易となっている。
* Kerasとは
    * Francois Choletが中心となって開発している深層学習ライブラリ、もしくはAPI仕様のこと。
    * Kerasには2つの実装がある。1つはTensorFlowに統合されたもので、もう1つはバックエンドとして、TensorFlowに加え、TheanoやCNTKもサポートしている、独立したパッケージである。
    * 前者は「KerasとはAPI使用である」と再定義して実装をTensorFlowに統合している。
    * 後者はこれまで通り、複数のバックエンドを選べる形態を維持している。
    * 両者には若干の違いはあるものの、API仕様が統一されており、どちらも同じように利用できる。
    * モジュール構成がシンプルなのがKerasの特徴の1つ。
    * 深層学習のネットワークを構築する上でよく使われるものが適切な粒度でモジュール化されている。
    * それにより、レゴブロックを組み合わせるような感覚で深層学習モデルを構築できる。
* Define and Run/Define by Run
    * Define and RunはTensorFlowのように先に計算グラフを定義し、まとめて処理する概念である。
    * 通常のプログラミング言語とパラダイムが異なるため、学習コストが若干高い反面、高速化が容易である。
    * Define by RunはPreferred NetworksのChainer開発チームが初めて提唱した概念である。
    * 予め計算グラフを定義せず、グラフ定義と処理を同時に行う手法である。
    * 処理結果によって計算グラフを動的に変えられるため、実装がシンプルになり、デバッグ時にエラー箇所を把握しやすい。
    * 2016年までは高速化の観点からDefine and Runが主流だったが、2017年ごろからはChainer以外でDefine by Run型のライブラリが多く登場している。
    * TensorFlowもv1.5で導入されたEagar Executionを用いることでDefine by Runによる記述が可能になっている。
* 複数ライブラリ間のモデルの共有
    * これまではある深層学習ライブラリで実装したモデルはそのライブラリでしか使用できなかった。
    * 1度学習したモデルのパラメータを別のライブラリでも利用したいというニーズから、「モデルをどう共有するか」に着目したライブラリや使用が登場している。
        * ONNX(Open Neural Network Exchange：深層学習モデルを表現するための共通フォーマット)を用いると、ライブラリAで実装したモデルを読み込み、ライブラリBのモデルに変換することができる。
* 深層学習のエコシステム
    * 2016年ごろからAIや深層学習に関する認知が一般企業に広まってきた。
    * これまでの研究や実験レベルの内容から実サービスでどう活用するか、という話に徐々にシフトしている。
    * 研究ではモデルそのものが重要である一方、実サービスではモデルの監視やアップデート、外部システムとの連携など検討事項が多岐にわたる。
    * 近年では実サービスでの活用にあたっての煩わしさを吸収するサービスをクラウド事業者が提供し始めている。
        * GCPではDataFlowやDataLab、前述のML Engineを駆使することで、データの蓄積から前処理、モデルの構築とサービスのホスティングまでを一貫してクラウド上で実行することができる。
        * 2017年に公開されたColaboratoryは12時間の制約があるものの、GPUを利用して深層学習モデルの構築を行うことができ、GoogleDriveでコードをシェアすることができるようになっている。
        * AWSではSageMakerと呼ばれるフルマネージドなサービスを提供しており、Jupyter Notebookを使ったモデルの構築や学習、モデルのホスティングまでの一連の流れを簡単に行うことができる。
    * これらクラウド機能を利用することで、実サービスでの運用を見据えた形で深層学習に取り組める環境が整いつつある。
* 開発環境構築
    * Pythonの開発環境 Anaconda：省略
    * 動作確認環境 Jupyter Notebook：省略





















## 参考
* 現場で使える！TensorFlow開発入門 Kerasによる深層学習モデル構築手法
https://www.shoeisha.co.jp/book/detail/9784798154121
