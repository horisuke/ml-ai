
### パーセプトロン
* ローゼンブラットとおうアメリカの研究者が1957年に考案された。
* パーセプトロンはニューラルネットワークの起源ともなるアルゴリズム。
* パーセプトロンは複数の信号を入力として受け取り、1つの信号を出力する。
* 信号とは「流れ」のようなものだが、パーセプトロンにおける信号は「流す」「流さない」の2値のみを取る。
* ここであるパーセプトロンの受け取る入力信号をx_1, x_2とし、その重みをそれぞれw1, w2、出力信号をyとする。
* 入力信号はニューロン(パーセプトロン)に送られる際、固有の重みが乗算される(w_1x_1、w_2x_2)
* ニューロンでは送られてきた信号の総和が計算され、その総和がある値を超えた時に1を出力する。これをニューロンが発火すると呼ぶ。
* また、このある値を閾値と呼びθで表す。
* これを式で表すと、以下の通りとなる。
	* y = 0  (w_1x_1 + w2_x_2 ≦ θ)   (2.1)
	* 　= 1  (w_1x_1 + w2_x_2 ＞ θ)
* パーセプトロンは複数の入力それぞれに対し、固有の重みをもち、またその重みは各信号の重要性をコントロールする要素として働く。
* つまり、重みが大きいほどその重みに対応する入力信号は重要であると言える。


### パーセプトロンを用いた論理回路の実現
* パーセプトロンを使ってANDゲートを考える。
* ANDゲートは2入力1出力のゲートであり、以下の通り、2入力が共に1の場合のみ1を出力し、それ以外は0を出力するゲートである。
	* x_1 x_2  y
	*  0    0    0
	*  1    0    0
	*  0    1    0
	*  1    1    1
* このANDゲートをパーセプトロンで表現するということは、上記の真理値表を満たすようにw_1, w_2, θを決めることである。
* 実際、この真理値表を満たす(w_1, w_2, θ)の組み合わせは無限にある。
* (w_1, w_2, θ) = (0.5, 0.5, 0.7), (0.5, 0.5, 0.8), (1.0, 1.0, 1.0)などである。
* 次にNANDゲートを考える。
* NANDはNot ANDの意味でANDの真理値表の出力を逆にした以下のようなものになる。
	* x_1 x_2  y
	*  0    0    1
	*  1    0    1
	*  0    1    1
    *  1    1    0
* NANDの真理値表を満たす(w_1, w_2, θ)の組み合わせも無限にある。
* (w_1, w_2, θ) = (-0.5, -0.5, -0.7)など
* ANDゲートを実現するパラメータ値の符号を全て反転させることでNANDゲートを実現するパラメータ値となる。
* 同様にORゲートについても考える。
* ORゲートは入力が1つでも1であれば、出力が1となるゲートであり、真理値表は以下の通りとなる。
	* x_1 x_2  y
	*  0    0    0
	*  1    0    1
	*  0    1    1
	*  1    1    1
* これに関してもAND. NANDゲートの場合と同様、(w_1, w_2, θ)の組み合わせは無限に存在する。
* 以上の通り、パーセプトロンを用いることで、AND,NAND,ORゲートを実現することができる。
* また、いずれも2入力1出力の1つのパーセプトロンの構造で3つのゲートを実現できており、3つのゲートの違いはパラメータ(重みと閾値)のみである。
* これは同じ構造のパーセプトロンのパラメータ値を適切に変更することで様々な役割を果たすゲートに変更できることを意味している。


### パーセプトロンを用いた論理回路の実装
* ANDゲートの実装は以下の通りになる。
    ```python
	def AND(x1, x2):
	    w1, w2, theta = 0.5, 0.5, 0.7
	    tmp = x1*w1 + x2*w2
	    if tmp <= theta:
	        return 0
	    elif tmp > theta:
	        return 1
    ```
* パラメータw1, w2, thetaは関数内で初期化し、重み付き入力の総和が閾値を超えるか否かをif文で判定し、それぞれ1, 0を返す。
* 同様の手順でNAND, ORゲートも実装することが可能。


### パーセプトロンの式へのバイアスの導入

		* 次に閾値θをバイアスbに変更する。バイアスbはb = -θとして定義する。
		* これを用いると、(2.1)はθ = -bとして以下のように記述できる。

			* y = 0  (b + w_1x_1 + w2_x_2 ≦ 0)   (2.2)
			* 　= 1  (b + w_1x_1 + w2_x_2 ＞ 0)
		* この式より、重みは入力信号の重要度をコントロールするパラメータとして機能することが分かる。
		* 一方、バイアスはニューロンの発火のしやすさ(1の出力されやすさ)を調整するパラメータとして機能する。
		* b=-0.1の場合、バイアス値が大きく、ニューロンは発火しやすいが、b=-20.0の場合、バイアス値が小さく、ニューロンは発火しにくいと言える。


### Numpyを使ったパーセプトロンの式の実装

		* Numpyを使って(2.2)を実装すると以下のようになる。

			* import numpy as np
			* 
			* x = np.array([0, 1])
			* w = np.array([0.5, 0.5])
			* b = -0.7
			* 
			* np.sum(w*x) + b
		* Numpy配列の乗算は要素数が同じ場合、それぞれの要素同士が乗算され、np.sum(w*x)では各要素の総和が計算される。


	* パーセプトロンを用いた論理回路の実装(バイアス導入+Numpyを用いた実装)

		* バイアスを導入し、かつNumpyを用いたANDゲートの実装は以下のようになる。

			* def AND(x1, x2):
			*     x = np.array([x1, x2])
			*     w = np.array([0.5, 0.5])
			*     b = -0.7
			* 
			* tmp = np.sum(w*x) + b
			* if tmp <= 0:
			*     return 0
			* else tmp > 0:
			*     return 1
		* 同様にNANDゲートの実装は以下のようになる。

			* def NAND(x1, x2):
			*     x = np.array([x1, x2])
			*     w = np.array([-0.5, -0.5])
			*     b = 0.7
			* 
			* tmp = np.sum(w*x) + b
			* if tmp <= 0:
			*     return 0
			* else tmp > 0:
			*     return 1
		* ORゲートの実装は以下のようになる

			* def OR(x1, x2):
			*     x = np.array([x1, x2])
			*     w = np.array([0.5, 0.5])
			* b = -0.2
			* 
			* tmp = np.sum(w*x) + b
			* if tmp <= 0:
			*     return 0
			* else tmp > 0:
			*     return 1


	* パーセプトロンを用いたXORゲートの実装

		* AND, NAND, ORゲートと同様、XORゲートを考える。
		* XORゲートは排他的論理和と呼ばれる論理回路で以下の真理値表の通り、x1, x2の一方が1の時に1を出力する

			* x_1 x_2  y
			*  0    0    0
			*  1    0    1
			*  0    1    1
			*  1    1    0
		* 上記のXORゲートはパーセプトロンを用いて実装することはできない。
		* これを説明するために各論理回路ゲートを視覚的に考える。
		* まず、ORゲートを満たす(b, w_1, w_2)=(-0.5, 1.0, 1.0)を考える。
		* このとき、パーセプトロンの式(2.2)は以下のようになる。

			* y = 0  (-0.5 + x_1 + x_2 ≦ 0)   (2.3)
			* 　= 1  (-0.5 + x_1 + x_2 ＞ 0)
		* 上式で表されるパーセプトロンは-0.5 + x_1 + x_2 = 0の直線で分断される2つの領域を作る。
		* 加えて、2つの領域の一方は1を出力し、もう一方は0を出力することを表す。
		* ORゲートの場合、(x_1, x_2) = (0, 0)のとき、0を出力し、(x_1, x_2) = (0, 1), (1, 0), (1, 1)のとき、1を出力する。
		* つまり、ORゲートを満たすパーセプトロンの式において、直線-0.5 + x_1 + x_2 = 0で(0, 0)と(0, 1), (1, 0), (1, 1)を分割することが可能である。
		* 同じことをXORゲートについて考える。
		* つまり、(x_1, x_2) = (0, 0), (1, 1)と(x_1, x_2) = (1, 0), (0, 1)を直線で2つの領域に分割することを考える必要があるが、これは視覚的にも不可能であることがわかる。
		* ただし、分割を直線に限定しなければ、XORゲートでも2つの領域に分割することが可能。
		* これはパーセプトロンの大きな制約(1本の直線で分割した領域しか表現できないこと)と言える。


	* 多層パーセプトロンによるXORゲートの実現

		* XORゲートは制約により、単一パーセプトロンでは表現できない。
		* 一方、パーセプトロンを層として重ね、多層パーセプトロンを構成することでXORゲートを実現可能。
		* さらに単一のパーセプトロンで実現できるAND, NAND, ORを組み合わせることでXORを実現することが可能
		* 具体的には2入力をNANDゲート、ORゲートにそれぞれ入力し、各ゲートの出力をANDゲートに入力し、出力を得ることで、システム全体としてはXORゲートを実現することが可能。
		* これを真理値表で表現すると以下の通りになる。(NAND, ORゲートの出力をそれぞれs_1, s_2とする)

			* x_1 x_2 s_1 s_2   y
			*  0    0     1     0    0
			*  1    0     1     1    1
			*  0    1     1     1    1
			*  1    1     0     1    0
		* 単一パーセプトロンでは表現できないものを層を重ね、多層パーセプトロンにすることによって表現可能となったということは、層を重ねることでパーセプトロンはより柔軟な表現が可能となることを意味している。
	* 多層パーセプトロンを用いたXORゲートの実装

		* 上記のようにNAND, OR, ANDゲートを用いて、XORゲートを実装すると以下の通りとなる。

			* def XOR(x1, x2):
			*     s1 = NAND(x1, x2)
			*     s2 = OR(x1, x2)
			*     y = AND(s1, s2)
			*     return y


	* ニューラルネットワーク

		* パーセプトロンは複雑な非線形関数であっても、それを表現できる可能性を秘めており、コンピュータが行う難しい処理でも表現できる可能性がある。
		* 一方、パーセプトロンのパラメータを設定する作業(期待する入力・出力を満たすパラメータを設定する作業)が難しく、多くの場合、人手で行なわれている。
		* ニューラルネットワークはこの問題を解決することができる。
		* 具体的には適切なパラメータをデータから自動で学習することができる。
		* この点がニューラルネットワークの重要な性質の1つであり、パーセプトロンと多くの共通点を持っている中で異なる点と言える。
		* 代表的なニューラルネットワークは3層で構成され、左から順に入力層、中間層(隠れ層)、出力層と呼ばれる。
		* また、3層のニューラルネットワークにおいて、入力層、中間層、出力層は第0層、第1層、第2層と呼ぶ。
		* 視覚的には多層パーセプトロンとニューラルネットワークに違いはなく、またニューロン間の繋がり方に関しても同じである。
		* パーセプトロンは上述の通り、以下の式で表され、bはニューロンの発火しやすさ、w_1, w_2は各信号のの重要性をコントロールするパラメータとしての役割を果たす。

			* y = 0  (b + w_1x_1 + w2_x_2 ≦ 0)   (3.1) = (2.2)
			* 　= 1  (b + w_1x_1 + w2_x_2 ＞ 0)
		* ここでバイアスbを視覚化する場合は、入力信号が1(=定数)、重みbのニューロンからの入力として表現できる。
		* パーセプトロンはx_1, x_2と1の3入力がニューロンの入力を取る形とみなすことができる。
		* (3.1)をh(x)という関数を導入してよりシンプルな形で表現すると、以下のようになる。

			* y = h(b + w_1x_1 + w2_x_2)　(3.2)
			* h(x) = 0 (x ≦ 0)　(3.3)
			*        = 1 (x ＞ 0)


	* 活性化関数

		* h(x)のように入力信号の総和を出力信号に変換する関数を活性化関数と呼ぶ。
		* 活性化関数には入力信号の総和がどのように活性化するかを決定する役割がある。
		* さらに重み付き入力信号の総和をaとすることで、(3.2)は以下のように表現できる。

			* a = b + w_1x_1 + w2_x_2　(3.4)
			* y = h(a)　(3.5)
		* これらの定義により、aはニューロンの入力となり、活性化関数h()を作用させ、yがニューロンの出力(=次ニューロンの入力)と言える。
		* (3.3)の活性化関数はステップ関数、階段関数などと呼ばれ、閾値を境に出力が切り替わる関数である。
		* つまりパーセプトロンは活性化関数としてステップ関数を用いているネットワークと言うことができる。
		* また、ステップ関数以外の関数を活性化関数として使用することができる。
		* ニューラルネットワークでよく用いられる活性化関数の1つに以下のシグモイド関数がある。

			* h(x) = 1 / (1 + exp(-x))　(3.6)
		* シグモイド関数を用いたニューラルネットワークの信号の流れはパーセプトロンと同じである。
		* 重み付き入力信号の総和を計算し、シグモイド関数を作用し、出力信号を計算し、次のニューロンに送信する。
		* パーセプトロンとニューラルネットワークの違いは活性化関数のみであり、ニューロンの多層接続構造や信号伝達方法は全く同じである。


	* ステップ関数の実装とグラフ

		* ステップ関数は入力が0を超えたら1を出力し、それ以外は0を出力する関数なので、実装は以下のようになる。

			* def step_function(x):
			*     if x > 0:
			*         return 1
			*     else:
			*         return 0
		* 上記の実装はシンプルだが、引数xとして実数しか取ることができないため、Numpy配列を引数に取れるよう、以下のように修正する。

			* def step_function(x):
			*     y = x > 0
			*     return y.astype(np.int)
		* y = x > 0でNumpy配列の各要素の大きさを0と比較し、値が0より大きい場合はTrue, 0以下の場合はFalseを要素とする、xと同じ大きさでbool値を要素に持つNumpy配列を生成し、yに格納する。
		* astype()メソッドにnp.intを引数として渡すことにより、Trueの要素を1、Falseを0に変換し、要素の値を0, 1に変換してNumpy配列を関数のreturn値として返している。
		* 次にステップ関数は以下のようにmatplotlibを使うことにより、グラフを表示することができる。

			* import numpy as np
			* import matplotlib.pyplot as plt
			* 
			* def step_function(x):

				* return np.array(x > 0, dtype=np.int)
			* 
			* x = np.arange(-5.0, 5.0, 0.1)
			* y = step_function(x)
			* plt.plot(x, y)
			* plot.ylim(-0.1, 1,1)
			* plot.show()
		* np.arange()で-5.0～5.0の範囲で0.1刻みの値を要素に持つNumpy配列を生成し、それをStep_function()に渡す。
		* 次にmatplotlibに生成したx, yの値を渡し、表示している。


	* シグモイド関数の実装とグラフ

		* 同様にシグモイド関数の実装(Numpy配列を引数・return値として取れる実装)は以下のようになる。

			* def sigmoid(x):
			*     return 1 / (1 + np.exp(-(x))
		* シグモイド関数もmatplotlibを使うことにより、以下のようにグラフを表示することができる

			* import numpy as np
			* import matplotlib.pyplot as plt
			* 
			* def sigmoid(x):

				* return 1 / (1 + np.exp(-(x))
			* 
			* x = np.arange(-5.0, 5.0, 0.1)
			* y = sigmoid(x)
			* plt.plot(x, y)
			* plot.ylim(-0.1, 1,1)
			* plot.show()


	* ステップ関数とシグモイド関数の比較

		* ステップ関数とシグモイド関数の違いとして、以下が挙げられる。

			* ステップ関数は0を境に急に出力が変わるのに対し、シグモイド関数は滑らかな曲線で入力に対して出力が連続的に変化する。
			* ステップ関数は0か1のいずれかの値しか返さないのに対し、シグモイド関数は無限に取り得る実数を返す。これはパーセプトロンではニューロン間を0 or 1の2値の信号が流れるのに対し、ニューラルネットワークのニューロン間を流れる信号は連続的な実数値の信号が流れることを意味する。
		* ステップ関数とシグモイド関数の共通点としては、以下が挙げられる。

			* 両関数は滑らかさの観点では異なるが、入力が小さいときの出力は0に近づき、入力が大きいときの出力は1に近づく。
			* この性質は入力信号が重要であれば、大きな値を出力し、重要でなければ、小さな信号を出すと言える。
			* 入力信号がどんなに大きくても、またどんなに小さくても、出力信号の大きさを0～1の間に収めることができる。
			* 両関数はいずれも非線形関数である。


	* 活性化関数の条件

		* ニューラルネットワークでは活性化関数に非線形関数を用いる必要がある。
		* つまり、活性化関数に線形関数を用いてはならないことを意味する。
		* これは線形関数を活性化関数に用いてしまうと、ニューラルネットワークで層を深くする意味がなくなってしまうためである。
		* ここで活性化関数h(x)を線形関数cxとし(cは定数)、3層のニューラルネットワークを考える。
		* 出力の値yはy(x) = h(h(h(x)))となるが、h(x) = cxの場合、y(x) = h(h(h(x))) = c*c*c*x = ax(a = c^3)となる。
		* これは隠れ層のない1層のネットワークで表現できてしまうことを意味しており、ニューラルネットワークを多層にすることを意味している。
		* よって、多層のメリットを受けるためには活性化関数に非線形関数を用いる必要がある。


	* ReLU関数と実装

		* ニューラルネットワークでは古くからシグモイド関数が使用されてきたが、最近はReLU(Rectified Linear Unit)関数が主に用いられる。
		* ReLUは(3.7)で表され、入力が0を超えていれば、その入力をそのまま出力とし、入力が0以下であれば、出力は0となる関数である。

			* h(x) = x (x > 0)　(3.7)
			*        = 0 (x ≦ 0)
		* ReLU関数の実装は以下の通りとなる。　※maximum()は引数に指定された値のうち、大きい方を返す関数

			* def relu(x):
			*     return np.maximum(0, x)


	* 3層ニューラルネットワークの実装

		* 以下の3層ニューラルネットワークを実装する。

			* 入力層(第0層)：ニューロン2つ
			* 隠れ層第1層(第1層)：ニューロン3つ
			* 隠れ層第2層(第2層)：ニューロン2つ
			* 出力層(第3層)：ニューロン2つ
		* 重みwは以下のように定義する。

			* w_ij^k
			* i : 次層の接続されるニューロン番号
			* j : 前層の接続されるニューロン番号
			* k : 重みの層番号
		* w_34^1は第0層から第1層をつなぐ重みで前層(第0層)の4番目のニューロンと次層(第1層)の3番目のニューロンを重みを表す。
		* 重み付き入力の総和aは以下のように定義する。

			* a_i^k
			* i : 重み付き入力の総和のニューロン番号
			* k : 重み付き入力の総和の層番号
		* a_1^2は第2層1番目のニューロンの重み付き入力の総和を表す。
		* バイアスbは以下のように定義する

			* b_i^k
			* i : バイアスのニューロン番号
			* k : バイアスの層番号
		* b_2^1は第1層2番目のニューロンのバイアスを表す。
		* 上記を踏まえると、第1層1番目のニューロンの重み付き入力の総和aは以下のようになる。

			* a_1^1 = w_11^1*x_1  +  w_12^1*x_2  +  b_1^1　(3.8)
		* 第1層の全ニューロンの重み付き入力の総和を行列を用いて表すと、以下のようになる。

			* A^1 = X*W^1  +  B^1　(3.9)

				* A^1 = (a_1^1, a_2^1, a_3^1)
				* X = (x_1, x_2)
				* B^1 = (b_1^1, b_2^1, b_3^1)
				* W^1 = (w_11^1, w_21^1, w_31^1)
				*             (w_12^1, w_22^1, w_32^1)
		* これらの式を踏まえると、実装は以下の通りとなる。ここでの入力信号、重み、バイアスは適当な値とする。

			* # 第0層(入力層)～第1層の実装
			* X = np.array([1.0, 0.5])
			* W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
			* B1 = np.array([0.1, 0.2, 0.3])
			* 
			* A1 = np.dot(X, W1) + B1
			* Z1 = sigmoid(A1)　#活性化関数h()としてシグモイド関数を用いる
			* 
			* # 第1層～第2層の実装
			* W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
			* B2 = np.array([0.1, 0.2])
			* 
			* A2 = np.dot(Z1, W2) + B2　#第1層の出力Z1を第2層の入力とする
			* Z2 = sigmoid(A2)
			* 
			* # 第2層～第3層(出力層)の実装
			* def identity_function(x):
			*     return x
			* 
			* W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
			* B3 = np.array([0.1, 0.2])
			* 
			* A3 = np.dot(Z2, W3) + B3　#第1層の出力Z1を第2層の入力とする
			* Y = identify_function(A3)
		* 各層における実装はほぼ同じだが、第3層(出力層)における活性化関数のみがそれまでの隠れ層と異なる。
		* 上記の実装ではidentify_function()という関数(恒等関数)を出力層の活性化関数として使用する。

			* 恒等関数とはf(x) = xとなる関数のこと。
		* 出力層の活性化関数を恒等関数にする場合、入力がそのまま出力となるため、定義して使用する必要はないが、前層までの実装と統一するためにここでは設定している。
		* 出力層の活性化関数は解く問題によって決められる。

			* 回帰では恒等関数、2クラス分類ではシグモイド関数、多クラス分類ではソフトマックス関数を使うのが一般的。
		* 上記の3層ニューラルネットワークの実装を以下の通り、関数化する

			* init_network()　：　重みとバイアスを初期化し、それらを辞書型変数networkに格納する処理
			* forward()　：　入力信号を出力に変換する処理(forward方向への伝達処理)
		* 実装は以下の通り

			* def init_network():
			*     network = {}
			*     network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
			*     network['b1'] = np.array([0.1, 0.2, 0.3])
			*     network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
			*     network['b2'] = np.array([0.1, 0.2])
			*     network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
			*     network['b3'] = np.array([0.1, 0.2])
			* 
			*     return network
			* 
			* def forward(network, x):
			*     W1, W2, W3 = network['W1'], network['W2'], network['W3']
			*     b1, b2, b3 = network['b1'], network['b2'], network['b3']
			* 
			*     a1 = np.dot(x, W1) + b1
			*     z1 = sigmoid(a1)
			*     a2 = np.dot(z1, W2) + b2
			*     z2 = sigmoid(a2)
			*     a3 = np.dot(z2, W3) + b3
			*     y = identify_function(a3)
			* 
			*     return y
			* 
			* def identity_function(x):
			*     return x
			* 
			* network = init_network()
			* x = np.array([1.0, 0.5])
			* y = forword(network, x)


	* 出力層の設計

		* 上述の通り、ニューラルネットワークを用いて解く問題に応じて出力層の活性化関数を選択する必要がある。
		* 一般的には回帰では恒等関数、2クラス分類ではシグモイド関数、多クラス分類ではソフトマックス関数を使う。
		* 回帰で使用される恒等関数は入力をそのまま出力する関数で、f(x) = xを満たす。
		* 2クラス分類で使用されるシグモイド関数は上述の通り。
		* 多クラス分類で使用されるソフトマックス関数は以下の式で表される。

			* y_k = exp(a_k) / Σexp(a_i)　i= 1, ..., n　(3.10)
			* k : 出力層k番目のニューロンの出力
			* n : 出力層のニューロンの数
			* a_i : 出力層i番目のニューロンの重み付き入力の総和
		* 出力層のニューロンの数は解くべき問題に応じて適宜決める必要がある。
		* クラス分類を解く場合、分類したいクラスの数に設定するのが一般的となっている。


	* ソフトマックス関数の実装と特徴

		* ソフトマックス関数の実装は以下の通り

			* def softmax(a):
			*     exp_a = np.exp(a)
			*     sum_exp_a = np.sum(exp_a)
			*     y = exp_a / sum_exp_a
			* 
			*     return y
		* 上記のソフトマックス関数の実装は正しく表現されているが、指数関数の計算を含み、値が容易に大きくなる可能性があり、オーバーフローが発生する可能性が高くなる。
		* この問題を改善するために、(3.10)を以下のように変換する。

			* y_k = exp(a_k) / Σexp(a_i)　i= 1, ..., n
			*       = C * exp(a_k) / C * Σexp(a_i)　i= 1, ..., n
			*       = exp(a_k + log C) / Σexp(a_i + log C)　i= 1, ..., n
			*       = exp(a_k + C') / Σexp(a_i + C')　i= 1, ..., n　(3.11)
		* (3.10)→(3.11)の変換では、分母分子に定数C(=1)を掛け、exp内に移動させ、log Cとし、C' = log Cとしている。
		* これはソフトマックス関数において、exp部分を計算する場合、任意の定数の加算・減算を施しても、意味が変わらないことを意味している。
		* この性質を用い、上述のオーバーフローを回避するためにC'として、入力信号の最大値の負値を設定することが一般的となっている。
		* これを踏まえ、上述のソフトマックス関数の実装を以下の通り修正する。

			* def softmax(a):
			*     c = np.max(a)
			*     exp_a = np.exp(a - c)
			*     sum_exp_a = np.sum(exp_a)
			*     y = exp_a / sum_exp_a
			* 
			*     return y
		* ソフトマックス関数の出力は下記の例のように0~1.0の間の実数となり、出力の総和は1となる。

			* a = np.array([0.3, 2.9, 4.0])
			* y = softmax(a)
			* print(y)　#[0.01821127, 0.24519181, 0.73659691]
			* print(np.sum(y))　#1.0
		* 出力の総和が1になるという性質は、ソフトマックス関数の出力を確率として解釈することができることを意味する。
		* 上記の例では0番目~2番目の要素がそれぞれ、1.8％、24.5％、73.7%と解釈でき、最も確率が高い2番目の要素が最も確からしい答えと判断することができる。
		* または、1.8％の確率で要素1、24.5％の確率で要素2、73.7%の確率で要素3である、と解釈することもできる。
		* 一方、多クラス分類を解く場合、出力層の活性化関数(ソフトマックス関数)を省略することもできる。
		* これは、重み付き入力の総和a_iとソフトマックス関数の出力値y_iの大小関係は変わらないためである。
		* つまり、a_kが重み付き入力の総和の最小値の場合、y_iの最小値もy_kとなる。
		* これはexp()が単調増加関数であることに起因する。
		* ソフトマックス関数の計算は指数関数計算を含み、計算量が大きくなるため、ソフトマックス関数を省略することが多くなってきている。
		* 機械学習の推論処理では上記の通り、出力層のソフトマックス関数を省略することが一般的である。


	* 手書き数字(MNISTデータセット)認識 - データセット読み込み

		* ニューラルネットワークを用いた手書き数字画像(MNISTデータセット)の分類処理を実装する。
		* ここでは、学習は既に完了していると仮定し、学習済みのパラメータを使って推論処理のみを対象にする。
		* 推論処理はニューラルネットワークの順方向伝播(forward propagation)と呼ばれる。
		* MNISTは手書き数字の画像データセットで機械学習分野で有名なデータセットの１つ。
		* MNISTデータセットは0～9の数字画像で構成されており、訓練画像60000枚、テスト画像10000枚が用意されている。
		* 一般的には訓練画像を使って学習を行い、学習したモデルでテスト画像に対してどれだけ正しく分類できるかを計測する。
		* 画像データは28×28のグレースケール画像(1チャンネル)で各ピクセルは0～255の値をとる。
		* 各画像データに対しては、「7」、「1」など画像が表す正解のラベルが与えられている。
		* MNISTデータセットをダウンロードし、画像データをNumpy配列に変換するまでの実装は下記の通り。

			* import sys, os
			* sys.path.append(os.pardir)
			* from dataset.mnist import load_mnist
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
			* 
			* print(x_train.shape)  # (60000, 784)
			* print(t_train.shape)   # (60000, )
			* print(x_test.shape)   # (10000, 784)
			* print(t_test.shape)   # (10000, )
		* 始めにload_mnist関数を使ってMNISTデータセットを読み込む。
		* 初回読み込み時はMNISTデータセットのダウンロードを行なうため、時間がかかるが、2回目以降はローカルに保存されているデータを使用する。
		* load_mnist関数は(訓練画像, 訓練画像ラベル), (テスト画像, テスト画像ラベル)という形式で読み込んだデータセットを返す。
		* また、load_mnist関数は以下の引数(フラグ)を設定可能

			* normalize：画像の各ピクセル値を元の0～255から0.0～1.0に正規化するかどうかのフラグ
			* flatten：画像を1×28×28の2次元配列を平準化(1次配列化)するかどうかのフラグ
			* one_hot_label：ラベルをone-hot表現(正解ラベルの要素のみが1、それ以外の要素が0の配列表現)にするかどうかのフラグ


	* 手書き数字(MNISTデータセット)認識 - データセット表示

		* load_mnist関数で読み込んだ訓練データの1枚目を表示する実装は以下の通り。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from dataset.mnist import load_mnist
			* from PIL import Image
			* 
			* def img_show(img):
			*     pil_img = Image.fromarray(np.unit8(img))
			*     pil_img.show()
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
			* 
			* img = x_train[0]
			* label = t_train[0]
			* print(label)  #5
			* 
			* print(img.shape)  #(784, )
			* img = img.reshape(28, 28)
			* print(img.shape)  #(28, 28)
			* 
			* img_show(img)
		* load_mnist関数でflatten=Trueとして読み込んだ場合、画像として表示するにはreshape関数で元の形状に再変形する必要がある。
		* 画像表示時はfrom.array関数はNumpy配列からPILオブジェクトに変換する必要がある。


	* 手書き数字(MNISTデータセット)認識 - ニューラルネットワークによる推論

		* MNISTデータセットに対して推論処理を行なうニューラルネットワークは入力層に784(28×28)個のニューロン、出力層に10(0～9)個のニューロンで構成される。
		* 隠れ層は第1層に50個、第2層に100個のニューロンを持つものとして構成する。
		* 実装は以下の通り。
		* まずは以下の3つの関数を定義する

			* def get_data()：
			*     (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=True, one_hot_label=False)
			*     return x_test, t_test
			* 
			* def init_network()：
			*     with open("sample_weight.pkl", 'rb') as f:
			*         network = pickle.load(f)
			*     return network
			* 
			* def predict(network, x)：
			*     W1, W2, W3 = network['W1'], network['W2'], network['W3']
			*     b1, b2, b3 = network['b1'], network['b2'], network['b3']
			* 
			*     a1 = np.dot(x, W1) + b1
			*     z1 = sigmoid(a1)
			*     a2 = np.dot(z1, W2) + b2
			*     z2 = sigmoid(a2)
			*     a3 = np.dot(z2, W3) + b3
			*     y = softmax(a3)
			* 
			*     return y
		* get_data()関数では、loadしたMNISTデータセットからテストデータとそのラベルデータを取り出す。
		* init_network()関数では、pickleファイルとして保存しているsample_weight.pklから学習済みの重みとバイアスを読み込む。
		* 重みとバイアスは辞書型の変数として保存されている。
		* predict()関数では、モデルの重みとバイアス、テストデータを受け取り、推論を行ない、その結果を返す。
		* これらの関数を使った実装は下記の通り

			* x, t = get_data()
			* network = init_network()
			* 
			* accuracy_cnt = 0
			* for i in range(len(x))
			*     y = predict(network, x[i])
			*     p = np.argmax(y)
			*     if p == t[i]
			*         accuracy_cnt += 1
			* 
			* print("Accuracy:" + str(float(accuracy_int) / len(x)))
		* まずget_data()でテストデータ、テストデータのラベルを取得し、それぞれx, tに格納する。
		* 次にinit_network()でネットワークを生成する。
		* for文では、テストデータを1枚ずつ取り出し、networkと一緒にpredict()関数に渡す。
		* predict()関数は[0.1, 0.3,..., 0.04]といったNunPy配列を返すため、この配列の要素のうち、最も大きい値を持つindexをargmax()を使って取得する。
		* ここで取得したindexとラベルデータt[i]を比較し、一致している場合は、ネットワークの推論が正しかったとみなし、accuracy_cntを加算している。
		* accuracyをデータの数で割ることでネットワークの推論が正解した割合を求める。
		* ここで、"Accuracy:0.9352"と表示された場合、これはテストデータのうち、93.52％正しく分類できたことを示している。
		* load_mnst()関数ではnormalize=Trueとしており、これは画像の各ピクセルの値を255で割り、データの値を0.0～1.0に収めるように変換している。
		* この処理は正規化(normalization)と呼び、正規化をはじめ、ニューラルネットワークへの入力データに決まった変換などを行うことを前処理(pre-processing)と呼ぶ。


	* 手書き数字(MNISTデータセット)認識 - ニューラルネットワークによる推論のバッチ処理

		* 上記のpredict()内で処理されるデータの大きさはそれぞれ以下の通りである。

			* x：784
			* W1：784×50
			* W2：50×100
			* W3：100×10
			* y：10
		* ネットワーク全体としては、784の要素からなる1次元配列(元は28×28の2次元配列)が入力となり、10の要素からなる1次元配列が出力される。
		* 隠れ層の重み配列はその前後のニューロンの数に従い、第1層は784×50(入力層が784、隠れ層第1層が50のニューロンでそれぞれ構成されているため)などとなっている。
		* ここで、複数枚のデータをまとめて入力する場合を考える。
		* 例えば、100枚の画像をまとめて入力し、1回のpredict()関数で処理することを考える。
		* このためには下記の通り、入力・出力の配列の形状を変える必要がある。

			* x：100×784
			* W1：784×50
			* W2：50×100
			* W3：100×10
			* y：100×10
		* 上記のようなまとまりのある入力データをバッチ(batch)と呼ぶ。
		* バッチ処理はコンピュータでの計算上、大きな利点があり、1枚当たりの処理時間を大幅に短縮できる。
		* これは数値計算を行うライブラリの多くが大きな配列を効率的に処理できるような最適化が行われているためである。
		* また、ニューラルネットワークの計算におけるデータ転送時間がボトルネックとなる場合は、バッチ処理を行なうことで大域の負荷を軽減することができる。つまりデータの読み込みに対して演算に使用する時間を多くすることができる。
		* バッチ処理の実装は以下の通り。

			* x, t = get_data()
			* network = init_network()
			* 
			* batch_size = 100
			* accuracy_cnt = 0
			* for i in range(0, len(x), batch_size)
			*     x_batch = x[i:i+batch_size]
			*     y_batch = predict(network, x_batch)
			*     p = np.argmax(y_batch, axis=1)
			*     accuracy_cnt += np.sum(p == t[i:i+batch_size])
			* 
			* print("Accuracy:" + str(float(accuracy_int) / len(x)))
		* for文ではrange()関数を使用し、テスト画像データをbatch_sizeごとに分割し、各バッチサイズごとのテストデータをx_batchに格納する。
		* x_batchはnetworkと合わせてpredict()関数に渡し、推論結果をy_batchとして格納する。
		* 同様にargmax()関数を使って最大値を持つindexを取得するが、ここではaxis=1を指定し、1次元目の要素ごとに最大値を持つindexを取得している。
		* 最後に推論結果とラベルデータを比較する。np.sum(p == t[i:i+batch_size])で推論結果とラベルデータが一致するかどうかによって、True/Falseを要素として持つboolian配列を生成し、Trueの個数を算出する。


	* ニューラルネットワークの学習

		* 学習とは訓練データから先的な重みの値を自動で獲得することを指す。
		* また、学習を行うために損失関数という指標を導入し、そのが最も小さくなる重みを探し出すことを学習の目的とする。
		* その損失関数の最小値を探す手法として関数の傾きを使用した勾配法と呼ばれる手法を使用する。
		* ニューラルネットワークの学習はデータから行なうことができる。
		* つまり、重みの値をデータから自動で決定することができる。
		* パーセプトロンの例では真理値表を見ながら手作業でパラメータの値を設定していたが、その数は高々3個であった。
		* 実際のニューラルネットワークではその数は数千、数万に及ぶこともあり、その値を手作業で決めるのは不可能と言える。
		* 通常、何らかの問題を解決する場合(特に何らかのパターンを見つける必要がある場合)、人があれこれ考えて答えを出すことが一般的である。
		* その場合、人の経験や直感を手掛かりに試行錯誤を重ねて答えを出す。
		* 一方、機械学習では、人の介入を極力避け、データから答え(パターン)を探す。
		* 例えば、「5」という数字を認識するプログラムを実装するとする。つまり手書き画像が5か5じゃないかを判別するプログラムを実装することを意味する。
		* 「5」を正しく分類できるプログラムを自分で考えて設計するのは難しい問題と言える。
		* 人にとっては簡単に認識できるが、どういう規則性で「5」と認識したのかを明確に述べることが困難であることが要因である。
		* 機械学習では「5」を認識するアルゴリズムをゼロから考え出す代わりにデータを有効に活用して解決する。
		* 1つの方法として画像から特徴量を抽出してそのパターンを機械学習技術で学習する方法がある。
		* 特徴量とは入力データから本質的なデータを的確に抽出できるように設計した変換器を指し、画像の特徴量は通常、ベクトルとして記述される。
		* 特徴量を使用して画像データをベクトルに変換し、そのベクトルに対して機械学習で使われる識別l器(SVM/KNNなど)で学習させることができる。
		* これにより、データから「機械」が規則性を見つけ出すため、ゼロからアルゴリズムを考え出すより効率的に問題を解決でき、人への負担も軽減される。
		* ただし、特徴量は「人」が設計したものであり、問題に応じて適した特徴量を使わなければ、よい結果は得られない。
		* 犬の顔を見分けるためには「5」を認識する特徴量とは別の特徴量を考える必要がある可能性もある。
		* 一方、ニューラルネットワークでは、機械学習によるアプローチと異なり、人の介在しない手法と言える。
		* ニューラルネットワークでは画像に含まれる画像の特徴量も含め、「機械」が学習する。
		* さらにニューラルネットワークの利点は全ての問題を同じ流れで解くことができる点にある。
		* 解くべき問題が「5」の認識問題であっても、「犬」の認識問題であっても、それらは関係なく、ニューラルネットワークは与えられたデータを学習し、問題のパターンを発見しようと試みる。


	* 訓練データとテストデータ

		* 機械学習の問題では訓練データとテストデータにデータを2つに分け、学習を行うのが一般的である。
		* まず訓練データを使って学習を行い、最適なパラメータを探索する。
		* その後、テストデータを使って、訓練したモデルの実力を評価する。
		* これは、モデルの汎用的な能力(汎化能力)を正しく評価するためである。
		* 汎化能力とは未知のデータに対しての能力であり、これは機械学習で生成するモデルとして獲得すべき目標である。
		* 手元にある訓練データだけが上手く判別できたとしても、それは訓練データに含まれる文字だけを学習してしまっている可能性がある。
		* つまり、1つのデータセットだけでパラメータの学習と評価をしてしまうと、正しい評価が行えず、あるデータセットにはうまく対応できても、他のデータセットには対応できない、といったことが起こる。
		* このようにあるデータセットだけに過度に対応した状態を過学習(overfitting)と呼び、過学習を避けることは機械学習の重要な課題の1つである。


	* 損失関数

		* ニューラルネットワークの学習ではある指標によって現在の状態を表し、その指標を基準として最適な重みの探索を行う。
		* この指標を損失関数(loss function)と呼ぶ。損失関数は人にの関数を指定できるが、一般的には2乗和誤差や交差エントロピー誤差などが用いられる。
		* 損失関数はニューラルネットワークの性能の悪さを表す指標で、その時点でニューラルネットワークがどの程度教師データに適合していないか、を表す。
		* よって、損失関数を最小にすることはモデルの性能の良さを上げることを意味する。
		* 損失関数として用いられる有名な関数として、以下の2乗和誤差(mean squared error)がある

			* E = 1/2 * Σ(y_k - t_k)^2　(4.1)
			* y_k：ニューラルネットワークの出力
			* t_k：訓練データのラベル情報
			* k：訓練データの次元数(ここでは0～9の分類なので、次元は10)
		* これまでの例では、y_k, t_kは以下のようなデータである。

			* y = [0.1,  0.05,  0.6,  0.0,  0.05,  0.1,  0.0,  0.1,  0.0,  0.0]
			* t = [0,  0,  1,  0,  0,  0,  0,  0,  0,  0]
		* yはsoftmax関数の出力であり、確率として解釈できるので、上記の場合、画像が「0」の確率が0.1、画像が「1」の確率が0.05、画像が「2」の確率が0.6、…と解釈できる。
		* tは訓練データのラベル情報であり、正解となるラベルを1、それ以外を0として表現している。
		* 上記の場合、ラベル「2」が1となっているので、正解ラベルが「2」を表している。
		* このような表記法はone-hot表現と呼ばれる。
		* (4.1)で表される2乗和誤差はニューラルネットワークの出力と正解ラベルの各要素の差の2乗を計算し、その総和を求めている。
		* これを実装すると以下の通りとなる。

			* def mean_squared_error(y, t):
			*     return 0.5 * np.sum((y-t)**2)
		* 上記の関数に以下の値を渡すと関数はそれぞれ以下のような値を返す。

			* ラベル情報が2、ニューラルネットワークの出力も2の場合

				* t = [0,  0,  1,  0,  0,  0,  0,  0,  0,  0]
				* y = [0.1,  0.05,  0.6,  0.0,  0.05,  0.1,  0.0,  0.1,  0.0,  0.0]
				*     →    0.0975000...
			* ラベル情報が2、ニューラルネットワークの出力が7の場合

				* t = [0,  0,  1,  0,  0,  0,  0,  0,  0,  0]
				* y = [0.1,  0.05,  0.1,  0.0,  0.05,  0.1,  0.0,  0.6,  0.0,  0.0]
				*     →    0.5975000...
		* 上述の通り、ラベル情報とニューラルネットワークの出力が一致している場合、損失関数の値は小さくなっている。
		* つまり、1つ目のyを出力するネットワークは損失関数の値が小さいため、その性能が高く、逆に2つ目のyを出力するネットワークは損失関数の値が大きいため、その性能が低いと言える。
		* (4.2)で表される交差エントロピー誤差も(4.1)の2乗和誤差と同様によく用いられる損失関数で以下のように表される。

			* E = - Σ(t_k * log y_k)　(4.2)　※Σ：k
			* log：eを底とする自然対数
			* y_k：k番目のニューロンの出力
			* t_k：訓練データのk番目のラベル情報(one-hot表現)
			* k：訓練データの次元数(訓練データの枚数)
		* log xはx=1のとき1となり、xが0に近づくとlog xの値はどんどん小さくなる。
		* よって、(4.2)の値は正解ラベルに対する出力y_kの値が大きいほど小さくなる。
		* 交差エントロピー誤差の実装は以下の通り。

			* def cross_entropy_error(y, t):
			*     delta = 1e-7
			*     return -np.sum(t * np.log(y + delta))
		* deltaとして微小な値をyに足しているのは、np.log(0)が-inf(=-∞)となってしまうため、yが0の場合でも-∞とならないようにしている。
		* 同様に交差エントロピー誤差関数に以下の値を渡すと関数はそれぞれ以下のような値を返す。

			* ラベル情報が2、ニューラルネットワークの出力も2の場合

				* t = [0,  0,  1,  0,  0,  0,  0,  0,  0,  0]
				* y = [0.1,  0.05,  0.6,  0.0,  0.05,  0.1,  0.0,  0.1,  0.0,  0.0]
				*     →    0.510825457...
			* ラベル情報が2、ニューラルネットワークの出力が7の場合

				* t = [0,  0,  1,  0,  0,  0,  0,  0,  0,  0]
				* y = [0.1,  0.05,  0.1,  0.0,  0.05,  0.1,  0.0,  0.6,  0.0,  0.0]
				*     →    2.302584092...
		* 2乗和誤差関数の場合と同様、ラベルとニューラルネットワークの出力が一致している場合、関数の出力値は小さくなっている。
		* 損失関数を導入する意味について考える。
		* 例えば、数字認識の場合、認識精度が高くなるようなパラメータを獲得したいはずなので、損失関数ではなく、認識精度を指標としてパラメータを調整すればよいように感じる。
		* ニューラルネットワークでは最適なパラメータの探索として、損失関数の値ができるだけ小さくなるようなパラメータを選ぶ。
		* その際、パラメータの勾配(微分)を計算し、それを手掛かりにパラメータの値を更新する。
		* あるニューラルネットワークにおけるある重みに注目する。
		* その１つの重みの損失関数に対する微分は、その重みの値を少し変化させたときに損失関数がどのように変化するかを表す。
		* もしその微分の値がマイナスであれば、その重みをプラスの方向へ変化させると損失関数はマイナスに変化する。
		* 逆に微分の値がプラスであれば、重みをマイナスの方向に変化させると損失関数はマイナスに変化する。
		* 一方、微分の値が0となると、どちらに動かしても損失関数の値は変わらないため、重みの更新はそこでストップする。
		* つまり、認識精度をニューラルネットワークの学習の指標にしてしまうと、微分の値が0になり、パラメータの更新ができず、学習が進まなくなってしまうことがあるためである。
		* むしろ認識精度を指標にすると、パラメータの微分がほとんど0になってしまう。
		* 例えば、100枚の訓練データのうち、32枚を正しく認識できるニューラルネットワークがあるとすると、このときの認識精度は32％である。
		* ここで、認識精度を指標として重みを少し変更したとしても、認識精度は32％のままほとんど変わらない。
		* もし認識精度が改善されたとしても、33％、34％といった不連続な値へと変わる可能性がある。
		* 一方、損失関数を指標として重みを少し変更すると、損失関数も連続的に変化する。
		* 少しのパラメータの変化にはほとんど変化を見せず、変化する場合は、不連続にいきなり変化するのは、活性化関数で用いられるステップ関数の挙動と似ている。
		* ステップ関数の微分はほとんどの場所(0以外の場所)で0となる。
		* 同様にニューラルネットワークの学習においても、微分が0にならない(傾きが0にならない)という性質は重要である。
		* 微分が0にならない、ということは、常に連続的に出力値が変化することを示しており、学習がしやすいと言える。


	* ミニバッチ学習

		* 訓練データを使って学習を行うということは、厳密には訓練データに対する損失関数を求め、その値をできるだけ小さくするパラメータを探し出すことである。
		* よって、損失関数は全訓練データを対象に求める必要があり、例えば訓練データが100個ある場合、その100個の損失関数の和を指標とする必要がある。
		* これをもとに(4.2)の交差エントロピー誤差の式は以下の(4.3)のように書くことができる。

			* E = - (1/N) Σ Σ(t_nk * log y_nk)　(4.3) 　※Σ：n,k
			* N：訓練データの個数
			* y_nk：n番目の訓練データのk番目のニューロンの出力
			* t_nk：n番目の訓練データのk番目のラベル情報(one-hot表現)
			* k：訓練データの次元数 (ここでは0～9の分類なので、次元は10)
		* (4.3)は訓練データの個数Nで割ることで、1データ当たりの平均損失関数値として正規化している。
		* これによって得られた損失関数の値は、訓練データ数に依存しない指標となる。
		* 一方、MNISTの訓練データは60000枚であり、一般的な訓練データとなりうるビックデータであれば、さらに多くの訓練データが存在する可能性がある。
		* 訓練データ数が膨大な場合、全データに対する損失関数の和を求めるのには時間がかかるため、データの一部を選び、その一部のデータにおける損失関数の平均値を求め、それを全体のデータにおける損失関数の平均値として近似することを考える。
		* ニューラルネットワークの学習においても、訓練データが膨大な場合は、同様に一部のデータのみを選び出してそのデータの塊ごとに学習を行う。
		* このように訓練データをデータの塊に分け、その塊ごとに学習を行う手法をミニバッチ学習と呼ぶ。
		* ミニバッチ学習を行うために訓練データから指定された個数分のデータをランダムに選ぶ実装は以下の通り。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from dataset.mnist import load_mnist
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
			* print(x_train.shape)  # (60000, 784)
			* print(t_train.shape)   # (60000, 10)
			* 
			* train_size = x_train.shape[0]
			* batch_size = 10
			* batch_mask = np.random.choice(train_size, batch_size)
			* x_batch = x_train[batch_mask]
			* t_batch = t_train[batch_mask]
		* load_mnist()関数ではデータセットをダウンロードした後、訓練データの各要素の値を0.0～1.0に正規化し、ラベル情報をone-hot表現に変更している。
		* そのため、訓練データは784要素を持つ1次元配列が60000個、ラベル情報は10要素(=one-hot表現)を持つ1次元配列が60000個となる。
		* これらのデータから指定数分のデータをランダムに取り出すためには、np.random.choice()関数を使用する
		* この関数にランダムに取り出す数値の最大値とランダムに取り出す数値の数を指定することができる。
		* ここでは、x_train.shape[0]=60000を ランダムに取り出す数値の最大値とし、取り出す数をbatch_size=10としている。
		* np.random.choice()の返り値はランダムに取り出した値のarrayとなっているので、その配列をx_train, t_trainのインデックスとすることで、60000のデータからランダムなデータ10個をミニバッチとして取得できる。
		* ミニバッチ学習を前提とすると、損失関数である交差エントロピー誤差の実装は以下の通りとなる。

			* def cross_entropy_error(y, t):
			*     delta = 1e-7
			*     if y.ndim == 1:
			*         t = t.reshape(1, t.size)
			*         y = y.reshape(1, y.size)
			* 
			*     batch_size = y.shape[0]
			*     return -np.sum(t * np.log(y + delta)) / batch_size
		* 前述の1つのデータを対象とした交差エントロピー誤差の実装を拡張する。
		* yはニューラルネットワークの出力、tはラベル情報(one-hot表現)である。
		* yの次元が1、つまりミニバッチではなく、データ1つに対する交差エントロピー誤差を求める場合、reshape()関数で1次元配列に変換して、ミニバッチの処理と同等に扱えるように変更する。
		* 以降は交差エントロピー誤差を求め、バッチサイズで割って正規化し、訓練データ1枚当たりの平均値を計算する。


	* 数値微分

		* 勾配法では勾配の情報を使って進む方向が決められるが、そのベースには微分が使われている。
		* ランナーが10分間で2km走ったとすると、その速さは2/10 = 0.2km/分であり、1分間に0.2km進むスピード(変化)と言える。
		* これは走った距離が時間に対してどれだけ変化したか、ということになるが、10分間に2km走ったということから、厳密には10分間の平均速度、ということができる。
		* 微分とはある瞬間の変化量を表したもので、10分という時間を、1分、1秒、0.1秒...と限りなく小さくし、「ある瞬間」の変化量を得る。
		* 微分は数式では以下のように表される。

			* df(x)/dx = lim ((f(x+h) - f(x)) / h)　※h→0　(4.4)
		* 左辺df(x)/dxはf(x)のxについての微分を表す記号でxに対するf(x)の変化の度合い(=xの小さな変化に対し、f(x)がどれだけ変化するか)を意味する。
		* xの小さな変化はhで表し、hを限りなく0に近づける。これをlim(h→0)で表す。
		* この(4.4)の微分をそのまま実装すると以下のようになる。

			* def numerical_diff(f, x):
			*     h = 10e-50
			*     return (f(x+h) - f(x)) / h
		* numerical_diff(=数値微分)関数は引数として、関数fとfの引数xを取る。
		* 上記の実装ではhで定義している値の丸め誤差(rounding error)が問題になる。
		* h=10e-50は丸め誤差として小数の小さな範囲の数値が省略されてしまう。微小な値hとしては10e-4程度の値を用いるのが良いとされている。
		* また、上記の実装ではf(x+h)とf(x)の差分を計算して微分を計算しており、この計算と真の微分の値に誤差が生じる可能性がある。
		* 真の微分はxの位置での傾きであり、この実装ではxとx+hの間の傾きを計算していることになる。
		* この誤差はhを無限に0に近づけられないことによって生じるものである。
		* この誤差を減らす工夫として、xとx+hの間の差分(前方差分)を計算するのではなく、(x-h)と(x+h)の間の差分(中心差分)を計算することで実現できる。
		* これらの改善を行なったnumerical_diff関数は以下の通りとなる。

			* def numerical_diff(f, x):
			*     h = 10e-4
			*     return (f(x+h) - f(x-h)) / (2*h)
		* このような微小な差分を使って微分を求めることを数値微分と呼び、一方、数式の展開によって微分を求めることを解析的に微分を求める、と呼ぶ。
		* 解析的に求める微分はy = x^2に対し、dy/dx = 2*xとして計算するので、誤差が含まれない「真の微分」を求めることができる。


	* 偏微分

		* 以下のような変数を2つ持つ関数の微分について考える

			* f(x_0, x_1) = x_0^2 + x_1^2　(4.6)
		* この式自体は以下のような実装となる

			* def function_2(x):
			*     return x[0]**2 + x[1]**2
		* 関数に対し、変数が複数ある場合は、どの変数に対する微分かを区別する必要がある。
		* このように複数の変数から成る関数の微分を偏微分と呼び、df/dx_0, df/dx_1のように記述する。
		* 偏微分も1変数の場合と同様、ある場所における傾きを求めるが、複数ある変数の中で微分する変数を1つ選び、他の変数はある値に固定して、微分の値を求める。
		* (4.6)におけるx_0 = 3, x_1 = 4における偏微分df/dx_0, df/dx_1を求める。

			* df/dx_0を求める場合、x_1 = 4を(4.6)に代入する
			* その場合、f(x_0, x_1) = x_0^2 + x_1^2 = x_0^2 + 4*4 = x_0^2 + 16となる。
			* 以上より、df/dx_0 = 2*x_0 = 2*3 = 6となる。
			* 次にdf/dx_1を求める場合、x_0 = 3を(4.6)に代入する
			* その場合、f(x_0, x_1) = x_0^2 + x_1^2 = 3*3 + x_1^2 = 9 + x_1^2となる。
			* 以上より、df/dx_1 = 2*x_1 = 2*4 = 8となる。


	* 勾配

		* 偏微分の計算を変数ごとではなく、まとめて計算することを考える。
		* つまり、(x_0, x_1)の両変数による偏微分を(df/dx_0, df/dx_1)として計算することを考える。
		* ここで、前変数の偏微分をベクトルとしてまとめた(df/dx_0, df/dx_1)を勾配(grandient)と呼ぶ。
		* 勾配の実装は以下のようになる。

			* def numerical_gradient(f, x):
			*     h = 1e-4 # 0.0001
			*     grad = np.zeros_like(x)
			* 
			*     for idx in range(x.size):
			*         tmp_val = x[idx]
			*         # Caliculate f(x+h)
			*         x[idx] = tmp_val + h
			*         fxh1 = f(x)
			* 
			*         # Caliculate f(x-h)
			*         x[idx] = tmp_val - h
			*         fxh2 = f(x)
			* 
			*         grad[idx] = (fxh1 - fxh2) / (2*h)
			*         x[idx] = tmp_val
			* 
			*     return grad
		* np.zeros_like(x)でxと同じshapeでかつ要素の値が0のNumPy配列を生成し、gradに格納する。
		* (4.6)の場合、xは2変数なので、(0, 0)が生成される。
		* 以降はfor文で各変数ごとに数値微分を行い、gradを計算する。
		* numerical_gradient()関数に損失関数fとxをarrayで渡すことにより、損失関数fのxにおける勾配を求めることができる。
		* 勾配の意味を(4.6)を用いて考える。
		* 横軸・縦軸にそれぞれx_0、x_1をとり、-2.0～2.0までの範囲を考え、各点における勾配をベクトルとして表示する。
		* 各点の勾配はある1点(最小値：x_0=0, x_1=0)に向かうようになり、かつ最小値から離れるほどベクトルの大きさも大きくなる。
		* 実際、各ベクトルの向く向きは最小値とは限らず、周辺の極小値となることもあるが、基本的に各地点において、関数の値を最も減らす方向を向いていると言える。


	* 勾配法

		* ニューラルネットワークの学習では最適なパラメータ(重みとバイアス)を見つける。
		* 最適なパラメータとは損失関数が最小値を取るときのパラメータ値を意味する。
		* 一般に損失関数は複雑なため、最小値がどこかを見つけるのは難しい。
		* そこで勾配を利用し、関数の最小値を見つけることを考える。
		* 勾配は各地点において関数の値を最も減らす方向を表すものであり、その指す先が本当に関数の最小値かどうか、その先が進むべき方向なのかを保証することはできない。
		* 実際、損失関数が複雑になると、勾配が指し示す方向が最小値ではないことがほとんどとなる。
		* 勾配が0になる地点は最小値とは限らず、極小値(ある範囲に限定した場合の最小値)や鞍点(見る方向によって、極小値・極大値になりうる地点)の可能性がある。
		* 他にも損失関数が複雑で歪な形をしていると、ほとんど勾配がなく、学習が進まない停滞期に陥ることもある。
		* 勾配の方向が必ずしも最小値を指すわけではないが、その時点でその方向に進むことで関数の値を最も減らすことができる。
		* よって、関数の最小値を取る場所を探す問題では、勾配の情報を手掛かりに進むべき方向を決めるのがよい。
		* 勾配を使って最小値を取る場所を見つける手法は勾配法と呼ばれる。
		* 勾配法では現在の位置から勾配方向に一定距離だけ進み、移動した先で勾配を計算し、またその地点での勾配方向に一定距離だけ進む、を繰り返し、関数の値を徐々に減らしていく。
		* 勾配法はニューラルネットワークの学習でよく使われる手法となっている。
		* 特に関数の最小値を探す場合は勾配降下法(gradient descent method)と呼ばれる。
		* 勾配法を数式で表すと、以下の(4.7)のようになる。

			* x0 = x0 - η * df/dx0
			* x1 = x1 - η * df/dx1
		* ηはニューラルネットワークの学習における学習率(learning rate)と呼ばれ、1回の学習でどれだけ学習すべきか、どれだけパラメータを更新するかを決めている。
		* (4.7)は1ステップにおけるパラメータの更新式であり、何度かステップを繰り返して、各ステップごとに(4.7)を使って変数の値を更新していく。
		* また、(4.7)は変数が2つであるが、変数の数が増えても、各変数ごとに(4.7)に相当する偏微分の式を更新する。
		* 学習率の値は、大きすぎても小さすぎても最小値にたどり着けない可能性があるため、ニューラルネットワークの学習では学習率の値を変更しながら、正しく学習ができているか確認していくのが一般的である。
		* 勾配降下法の実装は以下の通りとなる。

			* def gradient_descent(f, init_x, lr=0.01, step_num=100):
			*     x = init_x
			* 
			*     for i in range(step_num):
			*         grad = numerical_gradient(f, x)
			*         x -= lr * grad
			* 
			*     return x
		* gradient_descent()関数の引数は、f(最適化したい関数：損失関数)、init_x(xの初期値)、lr(learning rate)、step_sum(勾配法による繰り返し回数)を取る。
		* 関数内部では、numerical_gradient()関数で勾配を求め、その値に学習率を掛けた値でxを更新し、その処理をstep_num分繰り返す。
		* この関数により、関数の極小値を求めることが可能であり、うまくいけば最小値を求めることができる。
		* ここで、f(x0, x1) = x0^2 + x1^2の最小値を勾配法で求める(gradient_descentを使って最小値を求める)と以下のようになる。

			* def function_2(x):
			*     return x[0]**2 + x[1]**2
			* 
			* init_x = np.array([-3.0, 4.0])
			* grandient_decent(f=function_2, init_x=init_x, lr=0.1, step_sum=100)
			* # array([-6.11110793e-10,  8.14814391e-10])
		* 上記では、勾配降下法を用い、初期値を(-3.0, 4.0)として最小値の探索を行なっている。結果は限りなく(0, 0)となっており、ほぼ正しい結果が取得できている。
		* 次に学習率を大きくした場合(lr=10.0)は以下のようになる。

			* def function_2(x):
			*     return x[0]**2 + x[1]**2
			* 
			* init_x = np.array([-3.0, 4.0])
			* grandient_decent(f=function_2, init_x=init_x, lr=10.0, step_sum=100)
			* # array([-2.58983747e+13,  -1.29524862e+12])
		* 同様に学習率を小さくした場合(lr=1e-10)は以下のようになる。

			* def function_2(x):
			*     return x[0]**2 + x[1]**2
			*  
			* init_x = np.array([-3.0, 4.0])
			* grandient_decent(f=function_2, init_x=init_x, lr=1e-10, step_sum=100)
			* # array([-2.99999994,  3.99999992])
		* 上記から学習率が大きすぎるとxは大きな値へと発散してしまい、学習率が小さすぎるとxはあまり更新されずに学習が完了していることが分かる。
		* 学習率はハイパーパラメータと呼ばれ、ニューラルネットワークの重みやバイアスのようなパラメータとは異なり、人の手で設定されるパラメータである。
		* ハイパーパラメータを決めるためには一般にいろんな値で試しながらうまく学習できる設定値を探す作業が必要となる。


	* ニューラルネットワークにおける勾配

		* ニューラルネットワークにおける勾配(損失関数の各重みに関する勾配)を考える。
		* 2×3の重みWを持つニューラルネットワークと損失関数Lを考えると、W及び損失関数の各重みに関数勾配dL/dWは以下のようになる。

			* W =(w11, w12, w13)
			*        (w21, w22, w23)
			* dL/dW = (dL/dw11,  dL/dw12,  dL/dw13)　(4.8)
			*                 (dL/dw21,  dL/dw22,  dL/dw33)
		* dL/dWの各要素はそれぞれの要素に関する偏微分で構成されており、1×1番目の要素dL/dw11はw11を少し変化させると損失関数Lがどう変化するかを表している。
		* また、dL/dWとWは同じ形状となり、上記の例ではいずれも2×3になっている。
		* ニューラルネットワークにおいて勾配を求める実装は以下の通り。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from common.functions import softmax, cross_entropy_error
			* from common.gradient import numerical_gradient
			* 
			* class simpleNet:
			*     def __init__(self):
			*         self.W = np.random.randn(2, 3)
			* 
			*     def predict(self x):
			*         return np.dot(x, self.W)
			* 
			*     def loss(self, x, t):
			*         z = self.predict(x)
			*         y = softmax(z)
			*         loss = cross_entropy_error(y, t)
			* 
			*         return loss
		* まず、上記のsimpleNetクラスを実装する。
		* simpleNetクラスではインスタンス変数Wとして2×3の重みを持ち、予測を行うpredict()関数、損失関数の値を求めるloss()関数を持つ。
		* xは入力データ、tは正解ラベルが渡されてくるものとしている。
		* このsimpleNetを使う実装は以下のようになる。

			* net = simpleNet()
			* print(net.W)　#重みの初期値を生成
			*     #[[0.47355232  0.9977393  0.84668094]
			*     # [0.85557411  0.03563661  0.69422093]
			* x = np.array([0.6,  0.9])　#xの初期値を生成
			* p = net.predict(x)　#予測を計算
			* print(p)
			*     # [1.05414809  0.63071653  1.1328074]
			* np.argmax(p)　#予測の最大値を持つインデックスを算出
			*     # 2
			* t = np.array([0,  0,  1])
			* net.loss(x, t)
			*     # 0.92806853663411326
		* 上記までで、x=(0.6, 0.9)における損失関数の値を算出できたため、次に勾配を求める。

			* def f(W):
			*     return net.loss(x, t)
			* 
			* dw = numerical_gradient(f, net.W)
			* print(dw)
			*     # [[ 0.21924763  0.14356247  -0.36281009]
			*     #  [ 0.32887144  0.2153437    -0.54421514]]
		* 勾配は上述のnumerical_gradient()関数を使って算出する。numerical_gradient()関数では損失関数の値を使用するため、ここではf(W)としてnet.loss(x, t)を返す関数を定義している。
		* numerical_gradient()関数の結果はdL/dWであり、2×3の2次元配列となる。
		* ここで例えば、dL/dw11は約0.2であり、これはw11をhだけ増やすと、損失関数の値は0.2h分増加することを意味する。
		* 損失関数の値をより小さくするためには、勾配がプラスの要素はマイナスの方向に更新し、勾配がマイナスの要素はプラスの方向に更新するのが良いと言える。
		* また、勾配の値の絶対値が大きいほど、同じ微小距離hに対する減少量が大きく、損失関数の減少に大きく貢献する。


	* ニューラルネットワークに対する学習アルゴリズム

		* これまでの内容をまとめ、ニューラルネットワークにおける学習手順をまとめると以下のようになる。

			* 前提：

				* ニューラルネットワークには重みとバイアスがあり、これらを訓練データに適用するように調整することを学習と呼ぶ。
			* ステップ1：ミニバッチ

				* 訓練データからランダムに一部のデータを選び出す。
				* 選ばれたデータの集まりをミニバッチと呼び、ステップ1ではミニバッチの損失関数の値を減らすことを目指す。
			* ステップ2：勾配の算出

				* ミニバッチによる損失関数の値を減らすために各パラメータの勾配を求める。
				* 勾配の値により、損失関数を最も減らす方向を知ることができる。
			* ステップ3：パラメータの更新

				* パラメータを勾配方向に微小量分だけ更新する。
			* ステップ4：繰り返す

				* ステップ1～3を繰り返す。
		* 上記は勾配降下法によって、パラメータを更新する方法を使用しているが、学習に使用するデータはミニバッチとして無作為に選ばれたデータを使用しているため、確率的勾配降下法(stochastics gradient decent)と呼ばれる。
		* つまり確率的勾配降下法とは「無作為に選んだデータに対して行う勾配降下法」という意味であり、略してSGDと呼ばれる。
		* 以下では2層のニューラルネットワーク(隠れ層が1層)を実装し、MNISTデータセットを使った学習を行う。
		* まずはじめに以下の通り、2層ニューラルネットワークを1つのクラスTwoLayerNetとして実装する。

			* import sys, os
			* sys.path.append(os.pardir)
			* from common.functions import *
			* from common.gradient import numerical_gradient
			* 
			* class TwoLayerNet:
			*     def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
			*         # Initialize network weights
			*         self.params = {}
			*         self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
			*         self.params['b1'] = np.zeros(hidden_size)
			*         self.params['W2] = weight_init_std * np.random.randn(hidden_size, output_size)
			*         self.params['b2'] = np.zeros(output_size)
			* 
			*     def predict(self, x):
			*         W1, W2 = self.params['W1'], self.params['W2']
			*         b1, b2 = self.params['b1'], self.params['b2']
			* 
			*         a1 = np.dot(x, W1) + b1
			*         z1 = sigmoid(a1)
			*         a2 = np.dot(z1, W2) + b2
			*         y = softmax(a2)
			* 
			*         return y
			* 
			*     def loss(self, x, t):
			*         y = self.predict(x)
			* 
			*         return cross_entropy_error(y, t)
			* 
			*     def accuracy(self, x, t):
			*         y = self.predict(x)
			*         y = np.argmax(y, axis=1)
			*         t = np.argmax(t, axis=1)
			*         accuracy = np.sum(y==t) / float(x.shape[0])
			* 
			*         return accuracy
			* 
			*     def numerical_gradient(self, x, t):
			*         loss_W = lambda W: self.loss(x, t)
			* 
			*         grads = {}
			*         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
			*         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
			*         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
			*         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
			* 
			*         return grads
		* まずTwoLayerNetクラスのインスタンス変数を確認する。
		* インスタンス変数として、params、gradsというディクショナリ型の変数を持ち、それぞれ重みとパラメータごとの勾配が格納されている。
		* 例えば、prams['W1']、prams['b1']でそれぞれ、第1層の重み、バイアスにそれぞれアクセスできる。
		* 次にTwoLayerNetクラスのメソッドを確認する。
		* __init__()はクラスの初期化メソッドでTwoLayerNetクラスのインスタンスを生成した際に呼ばれる。
		* 入力層のニューロン数、隠れ層のニューロン数、出力層の数などの値を受け取り、またパラメータの初期化処理を行う。
		* パラメータの初期化はここでは重みはガウス分布による乱数で初期化しており、バイアスは0で初期化している。
		* predict()、accuracy()関数はこれまでの推論処理を関数化したもの。accuracy()関数内で、predict()関数を使用している。
		* loss()関数では損失関数の値を計算しており、内部でpredict()関数を使用している。
		* numerical_gradient()関数では各パラメータごとの勾配を数値微分によって計算している。
		* 次にこのTwoLayerNetクラスを使用し、MNISTデータセットを使ってミニバッチ学習を実装する。
		* 実装は以下の通り。

			* import numpy as np
			* from dataset.mnist import load_mnist
			* from two_layer_net import TwoLayerNet
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
			* train_loss_list = []
			* 
			* # Define hyper-parameter
			* iters_num = 10000
			* train_size = x_train.shape[0]
			* batch_size = 100
			* learning_rate = 0.1
			* 
			* # Create TwoLayerNet class instance
			* network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
			* 
			* for i in range(iters_num):
			*     # Get mini-batch
			*     batch_mask = np.random.choice(train_size, batch_size)
			*     x_batch = x_train[batch_mask]
			*     t_batch = t_train[batch_mask]
			* 
			*     # Calculate gradient
			*     grad = netowork.numerical_gradient(x_batch, t_batch)
			* 
			*     # Update parameters
			*     for key in ('W1', 'b1', 'W2', 'b2'):
			*         network.params[key] -= learn_rate * grad[key]
			* 
			*     # Record learning progress
			*     loss = network.loss(x_batch, t_batch)
			*     train_loss_list.append(loss)
		* 上記の実装では、ミニバッチのサイズを100とし、60000個の訓練データからランダムに100個のデータ(画像データとそれに対応する正解データ)を抽出している。
		* 抽出した100個のミニバッチを対象に勾配を求め、確率的勾配降下法(SGD)によって、パラメータを更新する。
		* ここではSGDによる更新回数を10000回とし、更新ごとに訓練データに対する損失関数の値を計算し、その値を配列train_loss_listに格納している。
		* 通常、学習が進むにつれて、損失関数の値が減少していく。
		* これは学習がうまく進んでいるサインであり、ニューラルネットワークのパラメータが徐々に訓練データに適用していることを示している。


	* テストデータによるニューラルネットワークの評価

		* 学習が上手く進んでいる場合、損失関数の値は徐々に減少していくが、この損失関数の値は訓練データのミニバッチに対する損失関数の値であると言える。
		* よって、訓練データによって得られる損失関数の値が減少していても、他のデータセットで同じ実力を発揮できるネットワークとは限らないとも言える。
		* ニューラルネットワークの学習では訓練データ以外のデータでも正しく認識できるかを確認する必要がある。
		* これは過学習を起こしていないかの確認になる。
		* 「過学習を起こす」とは訓練データに含まれる画像は正しく見分けられるが、訓練データに含まれない画像は識別できない状態を呼ぶ。
		* ニューラルネットワークの学習の目標は汎化能力を身につけることである。
		* よって、ニューラルネットワークの汎化能力を評価するには訓練データに含まれないデータを使って評価する必要がある。
		* 通常、学習を行う過程で定期的に(1エポックごとに)訓点で0たとテストデータを対象に認識精度を記録する。

			* エポックとは学習において訓練データをすべて使い切る回数のこと。
			* 10000個の訓練データに対し、100個のミニバッチで学習する場合、確率的勾配降下法を100回繰り返すことで1エポックとなる。
		* 上記のニューラルネットワークの学習の実装に1エポックごとに認識精度を計算する処理を追加すると、以下のようになる(※が追加部分)。

			* import numpy as np
			* from dataset.mnist import load_mnist
			* from two_layer_net import TwoLayerNet
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
			* train_loss_list = []
			* train_acc_list = []　※
			* test_acc_list = []　※
			* // The number of iteration per epoch　※
			* iter_per_epoch = max(train_size / batch_size, 1)　※
			* 
			* # Define hyper-parameter
			* iters_num = 10000
			* train_size = x_train.shape[0]
			* batch_size = 100
			* learning_rate = 0.1
			* 
			* # Create TwoLayerNet class instance
			* network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
			* 
			* for i in range(iters_num):
			*     # Get mini-batch
			*     batch_mask = np.random.choice(train_size, batch_size)
			*     x_batch = x_train[batch_mask]
			*     t_batch = t_train[batch_mask]
			* 
			*     # Calculate gradient
			*     grad = netowork.numerical_gradient(x_batch, t_batch)
			* 
			*     # Update parameters
			*     for key in ('W1', 'b1', 'W2', 'b2'):
			*         network.params[key] -= learn_rate * grad[key]
			* 
			*     # Record learning progress
			*     loss = network.loss(x_batch, t_batch)
			*     train_loss_list.append(loss)
			* 
			*     # Calculate the accuracy in each epoch
			*     if i % iter_per_epoch == 0:　※
			*         train_acc = network.accuracy(x_train, t_train)　※
			*         test_acc = network.accuracy(x_test, t_test)　※
			*         train_acc_list.append(train_acc)　※
			*         test_acc_list.append(test_acc)　※
			*         print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))　※
		* 上記の結果得られる訓練データ、テストデータそれぞれのaccuracyをepochごとのグラフで表す(横軸：エポック数、縦軸：accuracy)。
		* 両グラフ共にエポックが進むにつれて認識精度は向上していくことがわかる。
		* 過学習が起きていなければ、両グラフはほぼ同じで認識精度に差がない形となる。


	* 誤差逆伝播法

		* これまでニューラルネットワークの学習ではニューラルネットワークの重みに対する損失関数の勾配を数値微分で計算している。
		* 数値微分はシンプルで実装が容易な一方、計算に時間がかかる、という問題がある。
		* 誤差逆伝搬法は重みパラメータに関する損失関数の勾配の計算を効率よく行なう方法として知られている。
		* 誤差逆伝搬法は数式によって理解する方法と計算グラフ(computational graph)によって理解する方法がある。
		* 前者の方法が一般的で数式を中心に展開され、厳密かつ簡潔である。
		* 一方、後者は視覚的に誤差逆伝搬法を理解することができる。


	* 計算グラフ

		* 計算グラフは計算の過程をグラフで表したものである。グラフは複数のノードとエッジによって表現される。
		* 計算グラフで以下のような簡単な問題を解いてみる。

			* 100円のリンゴ2個を購入する場合の支払金額を求めよ。ただし消費税が10％適用されるものとする。
		* ああ
		* 計算グラフのうち、ノードは○で表記し、○の中に演算内容を記述する。
		* エッジは→で表記し、→の上に計算の途中結果を記述し、計算結果がノードからノードへ伝わるように表現する。
		* 上記の問題を計算グラフで解くと下記のようになる。

			*       100      x    200     x     220
			* ○ ーーー→ ○ ーーー→ ○ ーーー→ ○
			* 　        2 ー↑     1.1 ー↑  
		* 計算グラフを使って問題を解く場合、以下の流れで行なう。

			* ① 計算グラフを構築する
			* ② 計算グラフ上で左から右へ計算を進める
		* ②の「左から右に計算を進める」というステップは順方向の伝播のため、順伝播(forward propagation)と呼ぶ。
		* つまり順伝播は計算グラフの出発点から終着点への伝播を表す。
		* 一方、「右から左に計算を進める」ステップを逆伝播(backward propagation)と呼ぶ。
		* 計算グラフの特徴として、「局所的な計算」を伝搬させることで最終的な結果を得られる点が挙げられる。
		* 「局所的な計算」とは全体でどのような計算が行われていても、自分に関係する情報のみで結果を出すことができることを意味する。
		* 上記の計算グラフの例(100円のリンゴ2個を消費税10%を適用して購入する)に加え、リンゴ以外に他のものも購入し、その分の支払金額が4000円になったとする。
		* その場合でも、各ノードにおける計算は局所的であるため、4000円、200円という支払金額がどういう計算に基づいて算出されたかは考えずに4000円+200円=4200円と計算するだけで、支払金額の合計が算出できる。
		* これは各ノードでは自分に関係する計算のみを行なえばよく、全体のことは考える必要がないことを意味する。
		* これにより、計算グラフでは局所的な計算に集中することができ、全体として複雑な計算であっても、各ステップごとに単純な計算を行ない、その結果を伝播させることで全体として複雑な計算を行うことができる。
		* 計算グラフを使用する場合の利点の1つは上記の通り、単純な計算を局所的に行い、それらを伝播させることで複雑な計算を解くことにある。
		* もう1つの利点は計算グラフによって途中計算の結果を全て保持することが可能な点にある。

			* リンゴ2個の金額200円、消費税を適用した金額220円など
		* このことにより、逆方向に結果を伝播することで微分計算を効率よく行うことができる。
		* ここで前述の問題(100円のリンゴ2個を消費税10%を適用して購入する)でリンゴの値段が値上がりした場合を考える。
		* リンゴの値上がりに対して支払金額がどう変化するかを知ることは、リンゴの値段に関する支払金額の微分を求めることと同じである。
		* つまり支払金額をL、リンゴの値段をxとすると、リンゴの値段変化に対して支払金額がどう変化するかはdL/dxを求めることになる。
		* リンゴの値段に関する支払金額の微分の値は計算グラフで逆伝播を行うことで求めることができる。
		* 前述の順伝播は以下のようになる。

			*       100      x    200     x     220
			* ○ ーーー→ ○ ーーー→ ○ ーーー→ ○
			* 　        2 ー↑     1.1 ー↑  
		* 逆伝播は以下のようになる。

			*       2.2      x     1.1      x      1
			* ○ ←ーーー ○ ←ーーー ○ ←ーーー ○
			* 　        2 ー↑     1.1 ー↑  
		* 逆伝播では順伝播の場合と逆向きの矢印で表現する。
		* 逆伝播では「局所的な微分」を伝達し、その微分の値を←の上に記述する。
		* この例では、1→1.1→2.2と微分の値が伝達されている。
		* この結果からリンゴの値段に関する支払金額の微分の値は2.2であると言える。つまりリンゴの値段が1円(=微小金額)値上がりしたら、支払金額が2.2円増えることを意味している。
		* また、リンゴの値段に関する支払金額の微分だけではなく、消費税に関する支払金額の微分、リンゴの個数に関する支払関数の微分も計算することが可能。
		* この逆伝播の過程で求めた微分の値は共有することができ、効率よく複数の微分を計算することができる。


	* 連鎖律

		* 逆伝播は局所的な微分を順伝播とは逆方向に伝達していく。
		* この局所的な微分を伝達する原理は連鎖律(chain rule)と呼ばれる。
		* 計算グラフを使った逆伝播の例として、y = f(x)を考える。
		* y = f(x)の計算の逆伝播を図示すると以下のようになる。

			*        x            f         y
			* ーーーーー→     ーーーーー→
			*                    ○
			* ←ーーーーー     ←ーーーーー
			*    E・dy/dx               E
		* 逆伝播は信号Eに対して、ノードの局所的な微分dy/dxを掛け、次のノードに結果を伝播させている。
		* ここでの局所的な部分は順伝播でのy = f(x)の微分を求めることを意味しており、xに関するyの微分(dy/dx)ことである。
		* その微分の値を伝達された値(上記の場合はE)に掛けて、次のノードに伝播させる。
		* このような計算により、微分の値を効率よく求めることができるが、その理由は連鎖律の原理から説明することができる。
		* 連鎖律とは以下のような性質を指す。

			* ある関数が合成関数で表現できる場合、その合成関数の微分は合成関数を構成するそれぞれの関数の微分の積で表現できる。
		* 例えば、z = (x+y)^2は以下のように複数の関数によって構成される関数(合成関数)の2式で表現できる。

			* z = t^2　(5.1)
			* t = x + y
		* z = (x+y)^2の微分dz/dxは連鎖律の原理をあてはめると、dz/dtとdt/dxの積によって以下のように表すことができる。

			* dz/dx = dz/dt・dt/dx　(5.2)
		* 上式はdtが互いに打ち消しあう形で表現されていることがわかる。
		* z = (x+y)^2に(5.2)を適用してdz/dxを求めると以下のようになる。

			* dz/dx = dz/dt・dt/dx = 2t・1 = 2t =2(x+y)　(5.4)
		* (5.4)の連鎖律の計算を計算グラフで表現すると以下のようになる。

			*                              y ＿
			*                                  ↓
			*                  x               +                  t               **2               z
			* ーーーーーーーーーー→     ーーーーーーーーーー→     ーーーーーーーーーー→
			*                                   ○                                  ○
			* ←ーーーーーーーーーー     ←ーーーーーーーーーー     ←ーーーーーーーーーー
			*   dz/dz・dz/dt・dt/dx              dz/dz・dz/dt                       dz/dz
		* 計算グラフの逆伝播は右から左へ信号を伝播していく。
		* つまりノードへの入力信号に対してノードの局所的な微分を掛けて次のノードへ伝播させる。
		* 逆伝播の最初の信号はdz/dz=1で開始する。
		* 逆伝播の結果は、dz/dz・dz/dt・dt/dx = dz/dt・dt/dx = dz/dxとなり、xに関するzの微分が求められることになる。
		* これにより、計算グラフの逆伝播が連鎖律によって成り立っていることがわかる。


	* 逆伝播の仕組み - 加算ノード・乗算ノードの逆伝播

		* まず、加算ノードの逆伝播について考える。
		* z = x + yを対象にすると、x, yに関するzの偏微分はそれぞれ以下のようになる。

			* dz/dx = 1　(5.5)
			* dz/dy = 1
		* この式を順伝播の計算グラフで表すと以下のようになる。

			* x ーーーーー
			*                ↓
			*             + ○ ーーー→ z
			*                ↑
			* y ーーーーー
		* 同様に逆伝播の計算グラフは以下のようになる。

			*  dL/dz・dz/dx = dL/dz・1 = dL/dz
			* ←ーーーー
			*                |
			*             + ○ ←ーーー z
			*                |   dL/dz
			* ←ーーーー
			*  dL/dz・dz/dy = dL/dz・1 = dL/dz
		* (5.5)を踏まえると、加算ノードの逆伝播は上流から伝わった微分がそのまま下流に伝わることになると言える。
		* ここでは上流から伝わった微分をdL/dzとしており、Lは大きな計算グラフの最終的な出力を表している。
		* つまり、z = x + yは大きな計算グラフのどこかの計算にあたり、上流からdL/dzが伝わってくるものとしている。
		* 上記を踏まえ、加算の逆伝播を具体的な数値で考えると、10 + 5 =15という計算はx=10、y=5、z=15となる。
		* この計算グラフに逆伝播の際に上流から1.3が伝わった場合は下流に1.3がx、yの方向にそれぞれ伝播していくことになる。
		* 次に乗算ノードの逆伝播について考える。
		* z = xyを対象にすると、x, yに関するzの偏微分はそれぞれ以下のようになる。

			* dz/dx = y　(5.6)
			* dz/dy = x
		* この式を順伝播の計算グラフで表すと以下のようになる。

			* x ーーーーー
			*                ↓
			*             × ○ ーーー→ z
			*                ↑
			* y ーーーーー
		* 同様に逆伝播の計算グラフは以下のようになる。

			*  dL/dz・dz/dx = dL/dz・y
			* ←ーーーー
			*                |
			*             + ○ ←ーーー z
			*                |   dL/dz
			* ←ーーーー
			*  dL/dz・dz/dy = dL/dz・x
		* (5.6)を踏まえると、乗算ノードの逆伝播は上流から伝わった微分に入力信号を逆転させた値を乗算して下流に伝わることになると言える。
		* 上記を踏まえ、乗算の逆伝播を具体的な数値で考えると、10 × 5 =50という計算はx=10、y=5、z=50となる。
		* この計算グラフに逆伝播の際に上流から1.3が伝わった場合は下流に1.3×5=6.5がx方向の下流に、1.3×10=13がy方向の下流にそれぞれ伝播していくことになる。
		* 乗算ノードの逆伝播は加算ノードの逆伝播と異なり、入力信号の値が必要となるため、乗算ノードの実装時に順伝播の入力信号を保持する必要がある。
		* 加算ノード・乗算ノードの逆伝播の詳細を踏まえ、改めて、1個100円のリンゴを2個、10％の消費税を適用して購入することを考える。
		* さらにリンゴの値段、リンゴの個数、消費税の3変数がそれぞれ最終的な支払金額にどのように影響するかを解く。
		* これはそれぞれの変数に関する支払金額の微分を求めることに相当する。
		* この問題を計算グラフの逆伝播を使って解くことを考える。
		* 順伝播は以下のようになる。

			*       100      x    200     x     220
			* ○ ーーー→ ○ ーーー→ ○ ーーー→ ○
			* 　        2 ー↑     1.1 ー↑  
		* 逆伝播は上述の通り、上流から伝わる信号に入力信号を逆転させて乗算し、下流に伝わるので以下のようになる。

			*       2.2      x     1.1      x      1
			* ○ ←ーーー ○ ←ーーー ○ ←ーーー ○
			* 　               |                |
			*          110 ←        200 ←
		* これにより、リンゴの値段、リンゴの個数、消費税に関する支払金額の微分はそれぞれ、2.2、110、200となる。


	* 乗算レイヤの実装

		* 上述の乗算ノード・加算ノードをそれぞれ乗算レイヤ(MulLayer)、加算レイヤ(AddLayer)として実装し、リンゴの買い物の例を実装する。
		* 両レイヤはforward()、backward()という順伝播、逆伝播に相当する共通のインターフェースを持つように実装する。
		* 乗算レイヤの実装は以下の通り。

			* class MulLayer:
			*     def __init__(self):
			*         self.x = None
			*         self.y = None
			* 
			*     def forward(self, x, y):
			*         self.x = x
			*         self.y = y
			*         out = x * y
			* 
			*         return out
			* 
			*     def backward(self, dout):
			*         dx = dout * self.y
			*         dy = dout * self.x
			* 
			*         return dx, dy
		* __init()__ではインスタンス変数x,yの初期化を行なう。これは順伝播時の入力を保持するために使用する。
		* forward()ではx, yを引数として受け取り、それらを乗算して返しており、backward()では上流から伝わってきた微分doutを順伝播時の入力信号を逆転させて乗算し、下流に伝える値を返している。
		* このMulLayerクラスを使ったリンゴの買い物の例を実装する。
		* 順伝播は以下の通り。

			* apple = 100
			* apple_num = 2
			* tax = 1.1
			* 
			* # layer
			* mul_apple_layer = MulLayer()
			* mul_tax_layer = MulLayer()
			* 
			* # forward
			* apple_price = mul_apple_layer.forward(apple, apple_num)
			* price = mul_tax_layer.forward(apple_price, tax)
			* print(price)　#220
		* 逆伝播で各変数に関する微分を求める実装は以下の通り。

			* # backward
			* dprice = 1
			* dapple_price, dtax = mul_tax_layer.backward(dprice)
			* dapple, dapple_num = mul_apple_layer.backward(dapple_price)
			* print(dapple, dapple_num, dtax)　#2.2  110  200
		* backward()はforward()の時と逆の順番で呼び出し、またbackward()の引数は順伝播時の出力変数に対する微分を設定する。

			* 順伝播時、mul_apple_layerはapple_priceを出力する。
			* 逆伝播時はapple_priceの微分値であるdapple_priceを引数に設定する。


	* 加算レイヤの実装

		* 同様に加算レイヤを実装すると以下の通り。

			* class AddLayer:
			*     def __init__(self):
			*         pass
			* 
			*     def forward():
			*         out = x + y
			*         return out
			* 
			*     def backward(self, dout):
			*         dx = dout * 1
			*         dy = dout * 1
			* 
			*         return dx, dy
		* 加算レイヤではインスタンス変数の初期化は不要なため、何も行わない。

			* passは何も行わないことを表す
		* forward()ではx, yを引数として受け取り、それらを加算して返しており、backward()では上流から伝わってきた微分doutをそのまま値を返している。
		* 乗算レイヤと加算レイヤを用いて、1個100円のリンゴ2個、1個150円のミカン3個を購入(消費税10％)することを実装すると以下の通り。

			* apple = 100
			* apple_num = 2
			* orange = 150
			* orange_num = 3
			* tax = 1.1
			* 
			* # layer
			* mul_apple_layer = MulLayer()
			* mul_orange_layer = MulLayer()
			* add_apple_orange_layer = AddLayer()
			* mul_tax_layer = MulLayer()
			* 
			* # forward
			* apple_price = mul_apple_layer.forward(apple, apple_num)
			* orange_price = mul_orange_layer.forward(orange, orange_num)
			* all_price = add_apple_orange_layer.forward(apple_price, orange_price)
			* price = mul_tax_layer.forward(all_price, tax)
			* 
			* # backward
			* dprice = 1
			* dall_price, dtax = mul_tax_layer.backward(dprice)
			* dapple, dorange_price = add_apple_orange_layer.backward(dall_price)
			* dorange, daorange_num = mul_orange_layer.backword(dorange_price)
			* dapple, dapple_num = mul_apple_layer.backword(dapple_price)
			* 
			* print(price)　#715
			* print(dapple_num, dapple, dorange_num, dorange, dtax)　#110  2.2  3.3  165  650
		* 上記の通り、計算グラフにおけるレイヤの実装は簡単に行うことができ、それらを使えば複雑な微分計算を行うことができる。


	* 活性化関数レイヤの実装

		* 乗算レイヤ、加算レイヤ以外でニューラルネットワークで使われるレイヤの実装を行う。
		* 以降ではニューラルネットワークを構成するレイヤを1つのクラスとして実装していく。
		* まずは活性化関数であるReLUとSigmoidレイヤを実装する。
		* 活性化関数として使用されるReLU(Rectified Linear Unit)は以下の式で表される。

			* y = x　(x>0)　(5.7)
			* 　= 0　(x≦0)
		* 上記より、xに関するyの微分は以下のようになる。

			* dy/dx = 1　(x>0)　(5.8)
			*          = 0　(x≦0)
		* これより、ReLUレイヤは順伝播時の入力xが0より大きければ、逆伝播時は上流から伝わる値をそのまま下流に伝え、順伝播時の入力xが0以下であれば、逆伝播時は下流に値を伝えない、ということがわかる。
		* 上記を踏まえ、ReLUレイヤの実装は以下のようになる。

			* class Relu:
			*     def __init__(self):
			*         self.mask = None
			* 
			*     def forward(self, x):
			*         self.mask = (x<=0)
			*         out = x.copy()
			*         out[self.mask] = 0
			* 
			*         return out
			* 
			*     def backward(self, dout):
			*         dout[self.mask] = 0
			*         dx = dout
			* 
			*         return dx
		* Reluクラスはインスタンス変数として、True/FalseからなるNumPy配列であるmaskを持つ。
		* self.mask = (x<=0)により、xが0以下の場合はTrueを格納し、xと同じ形のTrue/Falseからなる配列を生成する。
		* 上述の通り、Reluでは順伝播時の入力が0以下の場合、逆伝播時は0が下流に伝わる。
		* よって、逆伝播時に保持していたmaskを使用して、上流から伝播された値とmaskの値を比較し、maskでTrueになっている要素を0として、下流に伝える処理を行なっている。
		* 一方、forward()にはxが0以下の場合、出力は0になるため、maskの要素がTrueのときは該当の要素を0にする処理を行っている。
		* 次に活性化関数をSigmoidレイヤとして実装する。
		* シグモイド関数は以下の式で表される。

			* y = 1 / (1 + exp(-x))　(5.9)
		* また、このシグモイド関数を計算グラフで表すと以下のようになる。

			*       x     ×           -x          exp       exp(-x)        +     1+exp(-x)      /       y = 1 / (1 + exp(-x))
			*   ーーー ○ ーーーーーーーー ○ ーーーーーーーー ○ーーーーーーーー ○ ーーーーー
			* 　          |                                                        |
			* -1 ーーー                                               1 ーーー
		* 上記の計算グラフでは加算ノード、乗算ノードに加え、"exp"と"/"のノードを新たに使用している。
		* 以下では逆伝播の流れを順を追ってみていく。
		* ステップ1：

			* 最初のノードは"/"のノード、つまり、y= 1/xを表す。この微分は解析的に次の式で表される。

				* dy/dx = -(1/x^2) = -(y^2)　(5.10)
			* 上式より、/ノードは逆伝播の際、上流からの値に対して-y^2(順伝播時の出力の2乗に-をつけた値)を乗算して下流に伝えると言える。
			* これを踏まえると計算グラフは以下のようになる。

				*     x    ×              -x             exp       exp(-x)       +     1+exp(-x)      /       y = 1 / (1 + exp(-x))
				*   ーー ○ ーーーーーーーーーー ○ ーーーーーーーー ○ーーーーーーーー ○ ーーーーー
				* 　       |                                                             |  -(dL/dy)・y^2        dL/dy
				* -1 ーー                                                    1 ーーー
		* ステップ2：

			* 次の+ノードは上流の値をそのまま下流に流すだけなので、計算グラフは下記のようになる。

				*     x    ×              -x             exp       exp(-x)       +     1+exp(-x)      /       y = 1 / (1 + exp(-x))
				*   ーー ○ ーーーーーーーーーー ○ ーーーーーーーー ○ーーーーーーーー ○ ーーーーー
				* 　       |                                      -(dL/dy)・y^2   |  -(dL/dy)・y^2        dL/dy
				* -1 ーー                                                     1 ーーー
		* ステップ3：

			* expノードはy = exp(x)を表し、この微分は解析的に次の式で表される。

				* dy/dx = exp(x)　(5.11)
			* 上式より、expノードは逆伝播の際、上流からの値に対して順伝播時の出力(上記ではexp(-x))を乗算して下流に伝えると言える。
			* これを踏まえると計算グラフは以下のようになる。

				*     x    ×              -x             exp       exp(-x)       +     1+exp(-x)      /       y = 1 / (1 + exp(-x))
				*   ーー ○ ーーーーーーーーーー ○ ーーーーーーーー ○ーーーーーーーー ○ ーーーーー
				* 　       |  -(dL/dy)・y^2・exp(-x)  -(dL/dy)・y^2   |  -(dL/dy)・y^2        dL/dy
				* -1 ーー                                                     1 ーーー
		* ステップ4：

			* 最後の×ノードは順伝播時の入力を逆にして乗算し、下流に値を流すので、この場合、上流からの値-(dL/dy)・y^2・exp(-x)に-1をかけ、(dL/dy)・y^2・exp(-x)が下流に伝達される。
		* ステップ1～ステップ4でSigmoidレイヤの逆伝播を行なうことができたが、これをひとつのSigmoidノードして考えると、逆伝播の際のSigmoidレイヤの出力は(dL/dy)・y^2・exp(-x)になると言え、計算グラフで書くと以下のようになる。

			*                   x               sigmoid              y               
			* ーーーーーーーーーーーー→     ーーーーーーーーーーーー→ 
			*                                        ○
			* ←ーーーーーーーーーーーー     ←ーーーーーーーーーーーー
			*    (dL/dy)・y^2・exp(-x)                     dL/dy
		* 上述のステップ4までに計算グラフと上記のsigmoidノードとして記述した計算グラフは計算結果は同じになる。
		* sigmoidノードとして記述された計算グラフの方が逆伝播時の途中計算を省略出来たり、Sigmoidレイヤの詳細を知ることなく、入力と出力だけを考えることができる、と言ったメリットがある。
		* さらに上記のsigmoidノードの逆伝播時の出力は以下の通り、より簡略化した形かつ順伝播時の出力のみで記述することができる。

			* (dL/dy)・y^2・exp(-x) = (dL/dy)・(1/(1+exp(-x))^2)・exp(-x)
			*                                   = (dL/dy)・(1/(1+exp(-x))・(exp(-x)/(1+exp(-x))
			*                                   = (dL/dy)・y・(1-y)　　　　　　　　　　　　　　　　(5.12)
		* 以上より、sigmoidノードの計算グラフは以下のように記述できる。

			*                   x               sigmoid              y               
			* ーーーーーーーーーーーー→     ーーーーーーーーーーーー→ 
			*                                        ○
			* ←ーーーーーーーーーーーー     ←ーーーーーーーーーーーー
			*        (dL/dy)・y・(1-y)                        dL/dy
		* これらを踏まえ、Sigmoidレイヤを実装すると以下の通りとなる。

			* class Sigmoid:
			*     def __init___(self):
			*         self.out = None
			* 
			*     def forward(self, x):
			*         out = 1 / (1+np.exp(-x))
			*         self.out = out
			* 
			*         return out
			* 
			*     def backward(self, dout):
			*         dx = dout * (1.0 - self.out) * self.out
			* 
			*         return dx
		* 上記の実装では順伝播時の出力をインスタンス変数outに保持して起き、逆伝播時にoutを使用して計算を行えるようにしている。


	* Affineレイヤの実装

		* ニューラルネットワークの順伝播では重み付き信号の総和を計算するために行列の積を計算する。
		* この行列の積を求める計算はアフィン変換と呼ばれる。
		* ここではアフィン変換を行う処理をAffineレイヤとして実装する。
		* ニューロンの重み付き和はY = np.dot(X, W) + Bで計算され、Yが活性化関数によって変換され、次の層に値が伝播される。
		* この計算(行列の積とバイアスの和)を計算グラフで表すと、以下のようになる。

			*    X
			* ーーーーー
			*              ↓    X・W     +          Y
			*        dot ○ ーーーー→ ○ ーーーー→
			*    W       ↑                 ↑
			* ーーーーー          B       |
			*                     ーーーーー
		* これまでと異なり、計算グラフを流れるのはスカラ値ではなく、行列がノード間を伝播する。
		* 次にこの計算グラフの逆伝播について考える。
		* 行列を対象とする逆伝播を求める場合、その要素ごとに書き下すことでスカラ値を対象にした計算グラフと同じ手順で考えることができる。
		* 逆伝播を計算グラフで記述すると以下のようになる。

			* ←ーーーー
			*      ①      |                 +
			*        dot ○ ←ーーーー ○ ←ーーーー
			*               |     dL/dY     |        dL/dY
			* ←ーーーー                   |
			*      ②            ←ーーーー
			*                          ③
			* 
			* ① dL/dX = dL/dy・W^T　(5.13)
			* ② dL/dW = X^T・dL/dY
			* ③ dL/dB = dL/dy
		* dotノードの逆伝播は×ノードの行列への拡張となる。
		* つまりdotノードの逆伝播は上流から伝わった微分に入力信号(行列)の転置を逆転させて乗算し、下流に伝わることになると言える。
		* ここでdL/dXとX、dL/dWとWは同じ形状である。

			* 以下の式からXとdL/dXの形状が同じであることは明らかである。

				* X = (x0, x1, ..., xn)
				* dL/dX = (dL/dx0, dL/x1, ..., dL/dxn)　(5.15)
		* 次に入力がバッチの場合のAffineレイヤを考える。
		* 上述のAffineレイヤは入力Xがひとつのデータの場合を対象としていた。
		* N個のデータをまとめて順伝播させる場合のバッチ版Affineレイヤを考える。
		* 前述の3層ニューラルネットワーク(以下)においてバッチによる学習を行うとする。

			* 入力層(第0層)：ニューロン2つ
			* 隠れ層第1層(第1層)：ニューロン3つ
			* 隠れ層第2層(第2層)：ニューロン2つ
			* 出力層(第3層)：ニューロン2つ
		* バッチ学習ではない場合、Xは上記より(2, )と言えるが、バッチ学習の場合、Xは(N, 2)になる。
		* つまり、バッチ学習の場合でも、順伝播・逆伝播は同じ式で表すことができ、逆伝播は行列の形状が異なるだけで、(5.13)で表すことができる。
		* 一方、バイアスの順伝播時の加算はX・Wに対し、各要素に加算する必要する必要がある。
		* 例で示すと以下の通り。

			* X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])
			* B = np.array([1, 2, 3])
			* 
			* print(X_dot_W)　# array([[0, 0, 0], [10, 10, 10]])
			* print(X_dot_W+B)　# array([[1, 2, 3], [11, 12, 13]])
		* よって、逆伝播時は各要素の値がバイアスの要素に集約される。
		* 例で示すと以下の通り。

			* dY = np.array([[1, 2, 3], [4, 5, 6]])
			* print(dY)　# array([[1, 2, 3], [4, 5, 6]])
			* dB = np.sum(dY, axis=0)
			* print(dB)　#array([5, 7, 9])
		* 以上より、　Affineレイヤの実装は以下の通りとなる。
		* ただし、実装は入力データがテンソルの場合も考慮した実装としている。

			* class Affine:
			*     def __init__(self, W, b):
			*         self.W = W
			*         self.b = b
			*         self.x = None
			*         self.dW = None
			*         self.db = None
			* 
			*     def forward(self, x):
			*         self.x = x
			*         out = np.dot(x, self.W) + self.b
			* 
			*         return out
			* 
			*     def backward(self, dout):
			*         dx = np.dot(dout, self.W.T)
			*         self.dW = np.dot(self.x.T, dout)
			*         self.db = np.sum(dout, axis=0)
			* 
			*         return dx


	* Softmax-with-Lossレイヤの実装

		* 次に出力層で用いるSoftmax関数をレイヤとして実装する。
		* Softmax関数は入力された値を正規化(出力の和が1になるように変形)して出力する働きがある。
		* 例えば、MNISTデータセットによる手書き数字認識は10クラス分類なので、Softmaxレイヤへの入力は10となる。
		* ニューラルネットワークにおける推論では、Softmaxレイヤは用いず、通常最後のAffineレイヤの出力を認識結果として用いる。
		* 推論で1つの結果を出す場合はスコアの最大値だけが分かればよいので、Softmaxレイヤは不要なためである。
		* また、この場合のようにニューラルネットワークの正規化しない出力結果のことをスコアと呼ぶ。
		* 一方、ニューラルネットワークの学習にはSoftmaxレイヤが必要となる。
		* 以下でSoftmaxレイヤを実装するが、ここでは損失関数である交差エントロピー誤差(cross entropy error)を含めて、Softmax-with-loss レイヤとして実装する。
		* Softmax-with-lossレイヤ(Softmax関数・交差エントロピー誤差)をそれぞれ計算グラフで表して接続すると、やや複雑な構造となる。
		* これを変形し、簡略化して書くと3クラス分類を行なう Softmax-with-lossレイヤ の計算グラフは以下のようになる。

			* 　　　　　　-----------　　　　t1　   -----------
			* 　a1　　　　| 　　　　| 　y1　￣ ￣| | 　　　   |
			* ーーーーーー| 　　　　| ーーーーーー | 　　　   |
			* 　　　 y1-t1 | 　　　　| 　　　t2　   | Cross 　 |
			* 　a2　　　　| Softmax |   y2   ￣ ￣| | Entropy |　　　L
			* ーーーーーー| 　　　　| ーーーーーー | Error　  | ーーーーー
			* 　　　 y2-t2 | 　　　　| 　　　t3　   |  　　　  |　　　1
			* 　a3　　　　| 　　　　| 　y3　￣ ￣| |  　　　  |
			* ーーーーーー| 　　　　| ーーーーーー |  　　　  |
			* 　　　y3-t3  -----------　　　　　　  -----------
		* エッジの上段の数値は順伝播、下段の数値は逆伝播時の数値を表す。
		* まず順伝播の流れを確認する。
		* Softmaxレイヤは3クラス分類なので、3つの入力(a1, a2, a3)を受け取り、それらを正規化した(y1, y2, y3)を出力する。
		* Cross Entropy Errorレイヤでは入力を(y1, y2, y3)と、教師ラベル(t1, t2, t3)とし、損失Lを出力する。
		* 次に逆伝播の流れを確認する。
		* Softmaxレイヤの逆伝播はSoftmaxレイヤの出力、教師データを使って表すことができ、(y1-t1, y2-t2, y3-t3)となる。
		* さらに言えば、これはSoftmaxレイヤの出力と教師ラベルの差分(誤差)であり、これが逆伝播により、ニューラルネットワークの前のレイヤに伝わっていくと言える。
		* ニューラルネットワークの学習の目的はニューラルネットワークの出力(=Softmaxの出力)を教師ラベルに近づけるようにパラメータを調整することである。
		* そのためにはニューラルネットワークの出力と教師ラベルとの差(誤差)を効率的に前のレイヤに伝える必要がある。
		* 上述のSoftmaxレイヤの逆伝播(y1-t1, y2-t2, y3-t3)はこれに該当する。
		* 上記の通り、ソフトマックス関数の損失関数として交差エントロピー誤差を用いると、逆伝播が(y1-t1, y2-t2, y3-t3)というきれいな形になるのは、交差エントロピー誤差がそうなるように設計されているからである。
		* 同様に回帰問題では、出力層に「恒等関数」を用い、損失関数として「2乗和誤差」を用いる場合も同様で、逆伝播が(y1-t1, y2-t2, y3-t3)となる。
		* 例として、教師ラベル(t1, t2, t3)=(0, 1, 0)でSoftmaxレイヤの出力が(y1, y2, y3)=(0.3, 0.2, 0.5)の場合を考える。
		* この場合、正解ラベルに対するSoftmaxの出力は0.2=20%なので、ニューラルネットワークはまだ正しい認識ができていないと言える。
		* よって、Softmaxレイヤからの逆伝播は(y1-t1, y2-t2, y3-t3)=(0.3, -0.8, 0.5)となり、大きな誤差を前のレイヤに伝播する。
		* 前のレイヤはその大きな誤差から大きな学習を行うことになる。
		* 一方、教師ラベル(t1, t2, t3)=(0, 1, 0)でSoftmaxレイヤの出力が(y1, y2, y3)=(0.01, 0.99, 0)の場合を考える。
		* この場合、逆伝播の誤差は(y1-t1, y2-t2, y3-t3)=(0.01, -0.01, 0)となり、小さな誤差が前のレイヤに伝播していく。
		* 前のレイヤは伝播されてくる誤差が小さいので、学習できる内容も小さくなる。
		* これらをもとにSoftmax-with-Lossレイヤの実装を行うと以下の通り。

			* class SoftmaxWithLoss:
			*     def __init__(self):
			*         self.loss = None
			*         self.y = None
			*         self.t = None
			* 
			*     def forward(self, x, t):
			*         self.t = t
			*         self.y = softmax(x)
			*         self.loss = cross_entropy_error(self.y, self.t)
			* 
			*         return self.loss
			* 
			*     def backward(self, dout=1):
			*         batch_size = self.t.shape[0]
			*         dx = (self.y - self.t) / batch_size
			* 
			*         return dx
		* 上記のsoftmax()関数、cross_entropy_error()関数は以前に定義したものを用いる。
		* また逆伝播の実装時はbatch_sizeを求め、伝播させる誤差を1個あたりにするためにbatch_sizeで割っている。


	* 誤差逆伝播法の実装

		* これまで実装してきたレイヤを組み合わせることでニューラルネットワークを構築する。
		* ニューラルネットワークの学習手順は以下のようになる(再掲)。

			* 前提：

				* ニューラルネットワークには重みとバイアスがあり、これらを訓練データに適用するように調整することを学習と呼ぶ。
			* ステップ1：ミニバッチ

				* 訓練データからランダムに一部のデータを選び出す。
			* ステップ2：勾配の算出

				* 各パラメータに関する損失関数の勾配を求める、損失関数を最も減らす方向を知る。
			* ステップ3：パラメータの更新

				* パラメータを勾配方向に微小量分だけ更新する。
			* ステップ4：繰り返す

				* ステップ1～3を繰り返す。
		* 上記の学習手順で誤差逆伝播法が使われるのは、ステップ2の勾配の算出時である。
		* これまでは勾配の算出には数値微分を使用していたが、誤差逆伝播法を用いることにより、高速で効率よく勾配を求めることができる。
		* まずは以下で2層のニューラルネットワークをTwoLayerNetとして実装する。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from common.layers import *
			* from common.gradient import numerical_gradient
			* from collections import OrderedDict
			* 
			* class TwoLayerNet:
			*     def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
			*         # Initialize network weights
			*         self.params = {}
			*         self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
			*         self.params['b1'] = np.zeros(hidden_size)
			*         self.params['W2] = weight_init_std * np.random.randn(hidden_size, output_size)
			*         self.params['b2'] = np.zeros(output_size)
			* 
			*         # Generate layers
			*         self.layers = OrderedDict()
			*         self.layers['Affine1'] = Affine(self.params['W1'], self.param['b1'])
			*         self.layers['Relu1'] = Relu()
			*         self.layers['Affine2'] = Affine(self.params['W2'], self.param['b2'])
			*         self.lastLayer = SoftmaxWithLoss()
			* 
			*     def predict(self, x):
			*         for layer in self.layers.values():
			*             x = layer.forward(x)
			*         return x
			* 
			*     def loss(self, x, t):
			*         y = self.predict(x)
			*         return self.lastLayer.forward(y, t)
			* 
			*     def accuracy(self, x, t):
			*         y = self.predict(x)
			*         y = np.argmax(y, axis=1)
			*         if t.ndim != 1 : t = np.argmax(t, axis=1)
			*         accuracy = np.sum(y==t) / float(x.shape[0])
			*         return accuracy
			* 
			*     def numerical_gradient(self, x, t):
			*         loss_W = lambda W: self.loss(x, t)
			*         grads = {}
			*         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
			*         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
			*         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
			*         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
			*         return grads
			* 
			*     def gradient(self, x, t):
			*         # forward
			*         self.loss(x, t)
			*         # backward
			*         dout = 1
			*         dout = self.lastLayer.backend(dout)
			*         layers = list(self.layers.values())
			*         layers.reverse()
			*         for layer in layers:
			*             dout = layer.backward(dout)
			*         # Settings
			*         grads = {}
			*         grads['W1'] = self.layers['Affine1'].dW
			*         grads['b1'] = self.layers['Affine1'].db
			*         grads['W2'] = self.layers['Affine2'].dW
			*         grads['b2'] = self.layers['Affine2'].db
			*         return grads
		* まずコンストラクタではパラメータ(重みとバイアス)の初期化とレイヤの生成を行う。
		* 重みは乱数、バイアスは0で初期化する。
		* 次にレイヤの生成ではOrderedDict()クラスからインスタンスを生成し、保持する。
		* OrderedDictは順番付きディクショナリでディクショナリに追加した要素の順番を覚えておくことができる。
		* よって、ニューラルネットワークの順伝播の順序でレイヤを生成して追加することで順伝播時は単にforward()関数を呼ぶだけで処理を実行できる。
		* また、逆伝播時はディクショナリに保持されている順番の逆順でレイヤを呼び出すだけで処理を実行できる。

			* 各レイヤはそれぞれレイヤの内部の実装で順伝播と逆伝播の処理が実装されているため。
		* このように実装しておくことで、ニューラルネットワークを簡単に構築することができる。
		* 仮に5層、10層と大きなネットワークを作りたい場合でも、必要なレイヤを単純に追加することでニューラルネットワークを構成することができる。
		* 次に誤差逆伝播法の勾配確認について考える。
		* 勾配を求める方法は、数値微分によって求める方法と、解析的に数式を解いて求める方法の2種類がある。
		* 後者の方法は上述の通り、誤差逆伝播法を用いることで勾配が効率的に計算することも可能であった。
		* よって、今後は数値微分よりも解析的に数式を解く方法を誤差逆伝播法を用いて勾配を求めることにする。
		* 一方、数値微分を用いた勾配の計算は誤差逆伝播法による勾配計算の実装が正しいかどうかを確認するために用いる。
		* これは数値微分による勾配計算の実装が簡単であるためである。
		* 両者の計算結果を比較し、誤差逆伝播法による勾配計算が正しいか(一致するか)どうかを確認することを勾配確認(gradient check)と呼ぶ。
		* 勾配確認の実装は以下のようになる。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from dataset.mnist import load_mnist
			* from two_layer_net import TwoLayerNet
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
			* network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
			* 
			* x_batch = x_train[:3]
			* t_batch = t_train[:3]
			* 
			* grad_numerical = network.numerical_gradient(x_batch, t_batch)
			* grad_backprop = network.gradient(x_batch, t_batch)
			* 
			* for key in grad_numerical.keys():
			*     diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
			*     print(key + ":" + str(diff))
		* MNISTデータセットを読み込み、その一部を使って、数値微分で勾配を求め、同時に誤差逆伝播法でも勾配を求める。
		* 両者の差を各パラメータにおける要素の差を絶対値で求め、その平均を出力している。
		* この実装の出力例は以下のようになる。

			* b1:9.70418809871e-13
			* W2:8.41139039497e-13
			* b2:1.1945999745e-10
			* W1:2.232446644e-13
		* 上記のような場合、両者の差はかなり小さいと言え、誤差逆伝播法でも求めた結果が正しいと言える。
		* 数値微分と誤差逆伝播法によって求めた微分の計算結果が0になることはまれであり、上記のように比較対象の両者の差の絶対値を求めて確認する必要がある。
		* 次にTwoLayerNetを用い、誤差逆伝播法を使ったニューラルネットワークの学習を実装する。
		* 実装は以下の通り。

			* import sys, os
			* sys.path.append(os.pardir)
			* import numpy as np
			* from dataset.mnist import load_mnist
			* from two_layer_net import TwoLayerNet
			* 
			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
			* network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)
			* 
			* iters_num = 10000
			* train_size = x_train.shape[0]
			* batch_size = 100
			* learning_rate = 0.1
			* 
			* train_loss_list = []
			* train_acc_list = []
			* test_acc_list = []
			* iter_per_epoch = max(train_size / batch_size, 1)
			* 
			* for i in range(iters_num):
			*     batch_mask = np.random.choice(train_size, batch_size)
			*     x_batch = x_train[batch_mask]
			*     t_batch = t_train[batch_mask]
			* 
			*     # Calculate the gradient by error back-propagation
			*     grad = network.gradient(x_batch, t_batch)
			*     # Update parametors
			*     for key in ('W1', 'b1', 'W2', 'b2'):
			*         network.para,s[key] -= leraning_rate * grad[key]
			*     # Calculate the value of loss function
			*     loss = network.loss(x_batch, t_batch)
			*     train_loss_list.append(loss)
			*     # Calculate the accuracies per epoch
			*     if i % iter_list_epoch == 0:
			*         train_acc = network.accuracy(x_train, t_train)
			*         test_acc = network.accuracy(x_test, t_test)
			*         train_acc_list.append(train_acc)
			*         test_acc_list.append(test_acc)
			*         print(train_acc, test_acc)


	* 学習に関するテクニック

		* これまでニューラルネットワークの学習の基本の仕組みや実装を見てきたが、それに加えて学習において重要となるアイデアや手法を見ていく。
		* 具体的には以下となる。

			* 最適な重みパラメータを探索する最適化手法
			* 重みパラメータの初期値設定
			* ハイパーパラメータの設定方法
			* 過学習対策としての正則化手法
			* Batch Normalization
		* これらの手法を用いることでニューラルネットワークの学習を効率的に進めることができ、また認識精度を高めることができる。


	* 最適なパラメータ探索のための最適化手法

		* ニューラルネットワークの学習の目的は損失関数の値をできるだけ小さくするパラメータを見つけることである。
		* これは最適なパラメータを見つけることであり、そのような問題を最適化問題と呼ぶ。
		* ニューラルネットワークの最適化はパラメータ空間が非常に複雑なため、とても難しい問題である。
		* さらにディープニューラルネットワークではパラメータ数が膨大となり、さらに複雑となる。
		* これまでは最適なパラメータを見つけるためにパラメータの勾配(パラメータに対する損失関数の微分)を手掛かりとし、勾配方向にパラメータを更新するステップを繰り返して、徐々に最適なパラメータに近づけていく、という手法を適用してきた。
		* この方法は確率的勾配降下法(stochastic gradient descent=SGD)と呼ばれ、単純だがパラメータ空間を闇雲に探し回って最適解を見つけるよりも賢い方法である。
		* ただSGDは単純な方法であるかわりに欠点も存在する。そのような場合、SGDよりもスマートな手法を適用し、より効率的な学習ができる可能性がある。
		* ニューラルネットワークの学習で最適なパラメータを探索するということは、広大で複雑な地形を地図もなく目隠しをして最も深い地点を見つけることと同じであると言える。
		* そのような状況で重要になってくるのは地面の傾斜であり、勾配である。今いる場所で一番傾斜がきつい方向に進む、という方針を取るのがSGDである。
		* これを繰り返すことでいつか最も深い地点にたどり着けるかもしれない、という手法である。
		* SGDは前述の通り、以下の式で書くことができる。

			* W ← W - η・dL/dW　(6.1)

				* W：更新する重みパラメータ
				* dL/dW：Wに関する損失関数の勾配
				* η：学習係数
		* ηは学習係数で0.01や0.001といった値を前もって決めて使用する。
		* ←は右辺の値で左辺の値を更新することを表している。
		* この式でSGDは勾配方向へある一定の距離だけ進むという手法であることを表している。
		* SGDをPythonのクラスとして実装すると以下の通りとなる。

			* class SGD:
			*     def __init___(self, lr=0.01):
			*         self.lr = lr
			* 
			*     def update(self, params, grads):
			*         for key in params.keys():
			*             params[key] -= self.lr * grads[key]
		* 初期化時の変数lrは学習係数であり、インスタンス変数として保持される。
		* update()関数はSGDでは繰り返し呼ばれることになり、引数のparamsとgradsはそれぞれディクショナリ変数であり、params['W1']、grads['W1']のとして重みや勾配が格納される。
		* SGDクラスを使うことでニューラルネットワークのパラメータの更新は以下のように行なうことができる。

			* network = TwoLayerNet(…)
			* optimizer = SGD()
			* 
			* for i in range(10000):
			*     …
			*     x_batch, t_batch = get_mini_batch(…)
			*     grads = network.gradient(x_batch, t_batch)
			*     params = network.params
			*     optimizer.update(params, grads)
			*     …
		* optimizer(=最適化を行なう者)はSGDクラスのインスタンスとして生成されており、SGDが最適化を行う役割を担う。
		* 実装する際はこのoptimizerにパラメータと勾配を渡すだけで最適化が行えると言える。
		* SGDクラスのように最適化を行なうクラスを分離して実装しておくことで、機能のモジュール化が容易となる。
		* 例えば、SGDとは別の最適化手法を実装する場合、そのクラスにもupdate(params, grads)という共通のメソッドを持つように実装する。
		* これにより、optimizer = SGD()を別機能のクラスからインスタンスを生成するように差し替えるだけで最適化手法を切り替えることができる。
		* SGDは単純で実装も簡単であるが、解く問題によっては非効率な場合がある。
		* そのようなSGDの欠点を理解するにあたり、以下の関数の最小値を求める問題を考える。

			* f(x, y) = 1/20・x^2 + y^2　(6.2)
		* (6.2)はxを横軸、f(x,y)を縦軸に取ると、お椀型の形状になる。
		* さらに第3軸としてy軸を取ると、お椀型の形状をy軸方向に伸ばした形状となる。
		* 次にx, yに関するそれぞれの勾配を考えると、x軸方向の勾配は小さく、y軸方向の勾配は大きいと言える。

			* df/dx = 1/10・x、df/dy = 2・yから明らかと言える。
		* これはy軸方向は急な傾斜であり、x軸方向は緩やかな傾斜であることを意味する。
		* また、この(6.2)の最小値を取る(x, y)は(x, y) = (0, 0)であるが、求められる勾配は多くの場所で(0, 0)を指していないことがわかる。
		* ここで、(6.2)に対し、SGDを適用する。探索の開始は(x, y) = (-7.0, 2.0)とする。
		* この場合、SGDはx, y軸方向にジグザグな動きをしながら、最小値を得るパラメータ(0, 0)に到達する。
		* この点からSGDは関数の形状が等方的でない場合、非効率な経路を通り、パラメータの探索を行ってしまうと言える。

			* 等方的とは方向によって性質が変わらないことを意味する。(6.2)はx軸方向とy軸方向で形状が大きく異なる。
		* このような場合、SGDのように単に勾配方向に進むよりもよりスマートな方法で進む方向を決める手法が求められる。
		* SGDの非効率な探索の原因は勾配の方向が本来の最小値を取り得る位置の方向を向いていないことだと言える。
		* この欠点を改善する手法として、Momentum、AdaGrad、Adamを考え、実装を行う。
		* まず、Momentum(モーメンタム)を考える。
		* モーメンタムは運動量という意味で以下の数式で表される。

			* v = α・v - η・dL/dW　(6.3)
			* W ← W + v　(6.4)

				* W：更新する重みパラメータ
				* dL/dW：Wに関する損失関数の勾配
				* η：学習係数
		* 一方、vを新しい変数として導入する。vは物理における「速度」に相当する。
		* (6.3)を物理法則として考えると、物体が勾配方向に力を受け、その力によって物体の速度が加算されることを意味する。
		* Momentumはボールが地面を転がるようなな動きで最適解を見つけるイメージである。
		* (6.3)のα・vという項は物体が力を受けていないときに徐々に減速する役割を担う。よってαは0.9などの値が設定される。
		* これらを踏まえたMomentumの実装は以下の通りとなる。

			* class Momentum:
			*     def __init__(self, lr=0.01, momentum=0.9):
			*         self.lr = lr
			*         self.momentum = momentum
			*         self.v = None
			* 
			*     def update(self, params, grads):
			*         if self.v is None:
			*             self.v = {}
			*             for key, val in params.items():
			*                 self.v[key] = np.zeros_like(val)
			* 
			*         for key in params.keys():
			*             self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
			*             params[key] += self.v[key]
		* インスタンス変数としてv(物体の速度)を保持する。vは初期化時には何も保持せず、update()関数が初めて呼ばれるときにパラメータと同じ構造のデータをディクショナリ変数として初期化する。
		* 上記のMomentumクラスを使って、(6.2)の最適化問題を解いてみると、更新経路はボールがお椀を転がるような動きをし、SGDに比べるとジグザグ度合いが軽減されると言える。
		* この要因はx軸方向、y軸方向それぞれで以下となる。

			* x軸方向に受ける力は小さいが、常に同じ方向の力を受けるため、同じ方向へ一定して加速している。
			* y軸方向に受ける力は大きいが、正と負の方向の力を交互に受けるため、それらが互いに打ち消し合い、y軸方向の速度は安定しない。
		* これらにより、SGDに比べてx軸方向に速く近づきジグザグの動きを軽減することができる。
		* 次にAdaGradを考える。
		* ニューラルネットワークの学習では学習係数(learning rate)の値が重要であり、小さすぎると学習に時間がかかりすぎてしまい、大きすぎると発散して学習が正しく行えない。
		* 学習係数に関するテクニックとして、学習係数の減衰(learning rate decay)があり、学習が進むにつれて学習係数を小さくしていくという手法である。
		* 最初は大きく学習し、次第に小さく学習するという手法はパラメータ「全体」の学習係数の値を一括して下げることに相当する。
		* これをさらに発展させたのがAdaGradであり、1つ1つのパラメータに合わせた学習係数の値を生成する。
		* AdaGradはパラメータごとに適応的に学習係数を調整しながら学習を行う手法である。
		* AdaGradの更新方法の数式は以下のように表される。

			* h ← h + dL/dW ○ dL/dW　(6.5)
			* W ← W - η・1/√h・dL/dW　(6.6)

				* W：更新する重みパラメータ
				* dL/dW：Wに関する損失関数の勾配
				* η：学習係数
		* 一方、hを新しい変数として導入する。hは(6.5)で示す通り、勾配の値の2乗和(アダマール積：各行列の対応する要素同士を乗算)を計算し、hを更新する。
		* また、Wの更新の際はこのhを用い、1/√hを乗算して学習のスケールを調整する。

			* hは勾配のアダマール積のため、要素の値は全て0以上の値となるため、 √h の計算は可能
			* またhは勾配の大きさが大きいほど大きな値となっているため、その場合、逆に 1/√hは小さい値になる。
			* これにより、1つ前の更新で勾配の値が大きく、更新が大きくされた要素は学習率が小さくなることを意味する。
			* 更新量の大きかった要素の学習係数の減衰を要素ごとに1つ前の更新の大きさに合わせて行うことができる。
		* さらにAdaGradの改善版としてRMSPropという手法がある。
		* AdaGradは上記の通り、1つ前の勾配のアダマール積を使って2乗和を求め、それらをWの更新に使用している。
		* 学習が進むと徐々に更新量が小さくなり、無限に学習を行なうと更新量が0となり、Wが全く更新されなくなる。
		* RMSPropはこの現象を改善するために過去のすべての勾配をWの更新時のhに 均一に加算せず、過去の勾配を徐々に忘れ、逆に新しい勾配の情報がhに大きく寄与するように加算している。
		* これらを踏まえたAdaGradの実装は以下の通りとなる。

			* class AdaGrad:
			*     def __init__(self, lr=0.01):
			*         self.lr = lr
			*         self.h = None
			* 
			*     def update(self, params, grads):
			*         if self.h is None:
			*             self.h = {}
			*             for key, val in params.items():
			*                 self.h[key] = np.zeros_like(val)
			*         for key in params.keys():
			*             self.h[key] += grads[key] * grads[key]
			*             params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
		* 上記では 1/√hの計算時に分母に微小な値として1e-7を加えて、0割り計算を回避している。
		* このAdaGradクラスを使って、(6.2)の最適化問題を解いてみると、更新経路はSGDやMomentumに比べ、効率的に最小値に向かって動くようになる。
		* この要因はy軸方向は勾配が大きいため、始めは大きく動くが、それに比例して更新ステップが小さくなっており、これによって、y軸方向の更新量は小さくなり、ジグザグな動きが軽減されている。
		* 次にAdamを考える。
		* Momentumではボールが地面を転がる物理法則のような動きをし、AdaGradはパラメータの要素ごとに更新ステップを調整するアルゴリズムであった。
		* これら2つの手法を融合した手法がAdamのベースとなる。
		* Adamは2015年に提案された手法で、その理論は複雑だが、この2つの手法の利点を組み合わせることで効率的にパラメータの空間を探索することがで期待できる。
		* 加えてAdamはハイパーパラメータのバイアスを補正(バイアスの偏りの補正)機能も含まれている。
		* SGD、Momentum、AdaGradと同様、Adamクラスを用いて、(6.2)の最適化問題を解いてみると、探索経路はMomentumと似て地面をボールが転がるような動きをする。
		* ただMomentumと比較して左右への揺れが軽減される。これは学習の更新量を適宜調整しているためであると言える。
		* Adamには3つのハイパーパラメータが存在し、学習係数に加え、一次モーメント用の係数β1、二次モーメント用の係数β2がある。
		* 通常、β1=0.9、β2=0.999がデフォルト値のことが多く、多くの場合はデフォルト値で学習が効率的に進む。
		* ここまでSGD、Momentum、AdaGrad、Adamの4つのパラメータ更新方法を見てきたが、手法により、パラメータは異なる探索経路を辿り更新される。
		* この探索経路は解くべき問題によって結果が変わり、またハイパーパラメータの設定値によっても結果が変わる。
		* よって、結局どの手法を使えばよいか？に対する解はなく、全ての問題で優れた更新手法はない。
		* 各手法に特徴があり、得意不得意がある。
		* 現状、SGDが今でも多く使われているが、最近はAdamがよく使われている。
		* 一般にSGDよりも他の3手法の方が早く学習ができ、また最終的な認識性能も高くなることが知られている。


	* 重みパラメータの初期値設定

		* ニューラルネットワークの学習においてパラメータの初期値も重要になってくる。
		* 初期値をどう設定するかによって、学習の成否が分かれることもある。
		* 過学習を抑え、汎化性能を高めるテクニックとしてWeight decay(荷重減衰)という手法がある。
		* Weight decayは重みのパラメータの値が小さくなるように学習を行なうことを目的とした手法である。
		* 重みの値を小さくすることで過学習が起きにくくなる。
		* 重みを小さい値にしたい場合、初期値をできるだけ小さい値に設定して学習を始めるのが良い。
		* 実際にこれまでは0.01 * np.random.randn(10, 100)のようにガウス分布から生成される値に0.01を乗算した小さな値を用いてきた。
		* ただし、重みの初期値を全て0にしてしまうのはよくなく、正しい学習を行なうことはできない。
		* 重みの初期値を全て0にしてしまうこと(=重みを均一な初期値としてしまうこと)は誤差逆伝播法による学習において、全ての重みの値が均一に更新されてしまうからである。
		* 例えば、2層のニューラルネットワークにおいて、1層目と2層目の間の重みの初期値が0とする。
		* その場合、順伝播では2層目のニューロンには全て同じ値が伝わることになる。またこの場合、逆伝播時に2層目の重みは全て同じように更新されてしまうことになる。
		* この結果、ネットワークの重みが均一な値で更新され、重複した値を持ってしまうことになる。これは多くの重みがあっても意味がないことになってしまう。
		* この重みの均一性を崩すためにはランダムな初期値が必要となる。
		* 次に隠れ層のアクティベーション分布(活性化関数の後の出力データが重みの初期値によってどう変化するか)を考える。
		* ここでは5層のニューラルネットワーク(活性化関数はシグモイド関数を使用)を用意し、ランダムに生成した入力データを流したときにアクティベーション分布がどうなるかをヒストグラムで描画する。
		* その実装の一部は以下の通りである。

			* import numpy as np
			* import matplotlib.pyplot as plt
			* 
			* def sigmoid(x):
			*     return 1 / (1 + np.exp(-x))
			* 
			* x = np.random.randn(1000, 100)　# Generate 1000 data
			* node_num = 100　# The number of neuron in hidden layer
			* hidden_layer_size = 5　#5 hidden layer
			* activations = {}　# Store the result of activation
			* 
			* # Calculate & store the activation value
			* for i in range(hidden_layer_size):
			*     if i != 0:
			*         x = activations[i-1]
			* 
			*     w = np.random.randn(node_num, node_num) * 1
			*     z = np.dot(x, w)
			*     a = sigmoid(z)
			*     activations[i] = a
			* 
			* # Draw the histgram of activation value
			* for i, a in activation.items():
			*     plt.subplot(1, len(activations), i+1)
			*     plt.title(str(i+1) + "-layer")
			*     plt.hist(a.flatten(), 30, range=(0, 1))
			* plt.show()
		* ここでは、5つの層それぞれに100個のニューロンを持たせる。
		* 入力データとして、1000個のデータをガウス分布でランダムに生成し、ニューラルネットワークに流す。
		* 隠れ層の重みは100×100の初期値をガウス分布でランダムに生成する。
		* 活性化関数としてシグモイド関数を使用し、各層のアクティベーションの結果をactivationsに格納する。
		* 上記の実装では、重みの初期値は標準偏差が1のガウス分布を用いている。
		* 標準偏差(スケール)を変えることでアクティベーションの分布がどう変わるかを確認する。
		* ヒストグラム表示部分では、a.flatten()で各層ごとのニューロン100個のアクティベーションの値を横軸に30分割して取り、縦軸にその個数を表示する。
		* 上記の実装による各層ごとのアクティベーション分布はいずれの層も、アクティベーションの値が0もしくは1に大きく偏ってしまうことがわかる。
		* 活性化関数として用いているシグモイド関数は出力が0もしくは1に近づくにつれて、平らとなり、その微分の値が0に近づいていく。
		* つまり0と1に偏ったアクティベーション分布では、逆伝播での勾配の値がどんどん小さくなり、最終的には勾配の値が0になり、消失してしまう。
		* この問題は勾配消失(gradient vanishing)と呼ばれる。勾配消失は層が深くなるディープラーニングではより深刻な問題となり得る。
		* 次に重みの初期値の標準偏差を0.01としてアクティベーション分布を確認する。
		* これは上記の実装のうち、以下の1行のみを修正するだけで標準偏差を変更し、確認を行うことができる。

			* (Before)　w = np.random.randn(node_num, node_num) * 1
			* (After)　　w = np.random.randn(node_num, node_num) * 0.01
		* 標準偏差を0.01とした場合のアクティベーション分布は、いずれも0.5付近に偏る分布となってしまうことがわかる。
		* 標準偏差が1の場合とは異なり、0と1への偏りはないため、勾配消失は起こらない。
		* ただ、アクティベーションの値に偏りがあるのはネットワークの表現力の観点で問題があると言える。
		* これは複数のニューロンがほとんど同じ値を出力していることを意味しており、ニューラルネットワークが多数のニューロンを持っている意味がなくなってしまうためである。
		* つまり、アクティベーションの値の偏りはニューラルネットワークの表現力が制限されるために問題と言える。
		* 次にXavier Glorotの論文で推奨されている重みの初期値「Xavierの初期値」を使用してみる。
		* 現状、Xavierの初期値は一般的なディープラーニングフレームワークで標準的に用いられている。
		* Xaivierの論文では、各層のニューロンのアクティベーション分布を均一にすることを目的としており、そのための適切な重みのスケール(標準偏差)を規定している。
		* この論文では、前層のノードの個数をnとした場合、1/√nの標準偏差を持つ分布を使うのがよいとされている。
		* 上記より、Xavierの初期値を用いると、前層のノード数が多いほど対象のノードの初期値として設定される重みのスケールは小さくなる。
		* ここで上記の実装を以下のように変更し、初期値をXavierの初期値とする。

			* (Before)　w = np.random.randn(node_num, node_num) * 1
			* (After)　w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
		* このニューラルネットワークを学習し、各層のアクティベーション分布を見てみると、深い層に行くにつれて、いびつな形にはなるが、これまでよりも広がりを持ったアクティベーション分布になることがわかる。
		* これにより、各層を流れるデータに適度な広がりがあるので、シグモイド関数の表現力の制限を受けることもなく、効率的に学習ができる。
		* さらに活性化関数をシグモイド関数からtanh関数(双曲線関数)に変更すると、層が深くなるにつれていびつになる減少も改善することが知られている。
		* tanh関数を用いると、分布が釣鐘型となる。
		* このようにtanh関数を用いることで分布のいびつさが改善するのは、tanh関数が原点(0, 0)に対して対称なカーブを描くのに対し、シグモイド関数は(0, 0.5)に対して対称なカーブを描くことに起因する。
		* 活性化関数は原点対称で阿呆が良いアクティベーション分布を得られることで知られている。
		* ただし、Xavierの初期値は活性化関数が線形の場合にアクティベーション分布が均一になる初期値を設定する手法である。
		* sigmoid関数やtanh関数は非線形関数であるが、左右対称で中央付近が線形関数とみなせるため、Xavierの初期値が有効となっている。
		* 一方、活性化関数としてReLUを用いる場合は、Kaiming Heらが推奨するHeの初期値を用いる方がよい。
		* Heの初期値では、前層のノードの個数をnとした場合、√(2/n)の標準偏差を持つ分布を使うのがよいとされている。
		* ここで活性化関数としてReLUを用い、初期値として標準偏差0.01のガウス分布からの初期値、Xavierの初期値、Heの初期値を用いた場合のアクティベーション分布を比較してみる。
		* まず、標準偏差0.01のガウス分布からの初期値を用いた場合、各層のアクティベーションの値はとても小さくなってしまう。
		* つまりヒストグラムの左端のbarに値の分布が集中してしまい、層が深くなるにつれ、その偏りがさらに顕著になる。
		* アクティベーションの値が小さいと逆伝播時の重みの勾配も小さくなり、学習の進み具合が遅くなってしまう。
		* 次にXavierの初期値を用いた場合、最初の層は比較的アクティベーションの値の分布は均一であるものの、層が深くなるにつれて、値の偏りが大きくなっていく。
		* つまり、ヒストグラムでは浅い層ではbarの大きさが(0, 1)でほぼ同じ高さなのに対し、層が深くなるにつれ、小さい値の方に偏り、左端のbarほど高さが高い分布となる。
		* これは勾配消失問題を招き、学習に時間がかかる要因となる。
		* Heの初期値を用いた場合、各層のアクティベーションの値の分布はかなり均一となる。分布が均一のため、学習の過程で勾配消失が起きず、かつ逆伝播時も適切な値がネットワークを流れるので、学習も効率的に進むようになる。
		* 次に初期値として標準偏差0.01のガウス分布からの初期値、Xavierの初期値、Heの初期値を用いた場合のネットワークの学習の進み具合を比較する。
		* ここでは各層100個のニューロンを持つ5層のニューラルネットワークを活性化関数をReLUとする。
		* 損失関数の値を縦軸、iterationsを横軸に取り、損失関数の値の減少スピードを確認する。
		* まず、標準偏差0.01のガウス分布からの初期値の場合、損失関数の値が減少せず、ほぼ横ばいのグラフとなる。
		* これは上述の通り、順伝播で小さな値が流れるため、逆伝播時も勾配が小さくなり、重みの更新がほとんど行われないことが原因と言える。
		* 一方、Xavierの初期値、Heの初期値を用いた場合は順調に学習が進み、iterationsを重ねるについて損失関数の値が小さくなるグラフとなる。
		* また、Heの初期値を用いた方が学習の進みが速いと言える。


	* Batch Normalization

		* 重みパラメータの初期値設定では各層のアクティベーションの値の分布を確認したが、そこでは適切な初期値を設定することで分布が適度に広がり、学習が効率的に進むことがわかった。
		* ここでは逆に各層でアクティベーションの値の分布が適度な広がりを持つように強制的に分布を調整することを考える。
		* そのようなアイデアのベースがBatch Normalizationである。
		* Batch Normは2015年に提案された比較的新しい手法だが、既に広く使われている。
		* これにはBatch Normの以下のような利点があるからと言える。

			* 学習を速く進めることができる(学習係数を大きくとることができる)
			* 初期値に大きく依存しない(初期値の設定に神経質にならなくてもよい)
			* 過学習を抑制できる(Dropoutなどの必要性を減らすことができる)
		* これらの利点はディープラーニングでよく起きる問題を解消してくれるものである。
		* Batch Normはニューラルネットワークにデータ分布の正規化を行なうレイヤとして、Batch Normレイヤを挿入する形で使用される。
		* また、Batch Normalizationはその名の通り、学習を行なうミニバッチの単位で正規化を行なうレイヤである。
		* 具体的にはデータの分布が平均が0、分散が1になるように正規化を行なう。
		* 数式で表すと以下のようになる。

			* μB ← 1/m・Σxi (i=1→m)　(6.7)
			* σB^2 ← 1/m・Σ(xi - μB)^2 (i=1→m)
			* x_i ← (xi - μB) / √(σB^2 - ε)

				* B：{x1, x2, ..., xm}のm個の入力データ(ミニバッチ)
				* μB：入力データBの平均
				* σB^2：入力データBの分散
				* ε：0割り防止のための微小値(1e-7など)
		* 上式ではミニバッチの入力データ{x1, x2, ..., xm}を平均0分散1のデータ{x_1, x_2, ..., x_m}に変換している。
		* この処理は通常、Batch NormレイヤとしてAffineレイヤとReLUなどの活性化関数レイヤの間に挿入され、データの分布(アクティベーションの値の分布)の偏りを軽減している。
		* 加えて、Batch Normレイヤは(6.7)で正規化されたデータ{x_1, x_2, ..., x_m}に対し、以下の式で表されるような固有のスケールとシフトで変換を行っている。

			* yi ← ɤx_i + β　(6.8)

				* ɤ, β：最初はɤ=1, β=0とし、学習によって最適な値に調整されるパラメータ
		* 以上がBatch Normの順伝播におけるアルゴリズムとなる。
		* 逆伝播時の式の導出は複雑なため、省略するが、計算グラフを用いると、比較的簡単に導出することができる。
		* 次にMNISTデータセットを用い、Batch Normがある場合とない場合で学習の進みがどう変わるかを確認する。
		* 縦軸にaccuracy、横軸にepochsを取ってグラフを作成すると、Batch Normを用いる方がaccuracyの伸びが速いかつ大きくなり、学習の進みが速いと言える。
		* 次に様々な初期値のスケールを与えて学習した結果を同様のグラフを作成して比較してみる。
		* 結果、多くの場合でBatch Normを用いた方が学習の進みが速いことがわかる。
		* 逆にBatch Normを用いない場合は適切な初期値を与えないと学習が全く進まないこともある。
		* また、この結果は与える初期値に関わらず、Batch Normを用いることで学習を進めることができることを示しており、重みの初期値に対してロバスト性を与えることになる。


	* 正則化

		* 機械学習ではよく過学習(overfitting)が問題になる。
		* 過学習は訓練データだけに適応しすぎてしまい、訓練データに含まれない他のデータにうまく対応できない状態のことである。
		* 機械学習で目指すのは汎化性能であり、未知のデータであっても正しく識別できるモデルが望まれる。
		* 複雑かつ表現力の高いモデルを作ることは可能だが、その分過学習を抑制するテクニックが重要になってくる。
		* 過学習が起きる主な原因として以下が挙げられる。

			* パラメータを大量に持ち、表現力が高いモデルであること
			* 訓練データが少ないこと
		* まずはわざと上記を満たす状態を作り、過学習を発生させるような実装を行う。
		* この実装をMNISTデータセットに対して行うと以下の通り。

			* (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)
			* x_train = x_train[:300]
			* t_train = t_train[:300]
			* 
			* netowork = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)
			* optimizer = SGD(lr=0.01)
			* 
			* max_epochs = 201
			* train_size = x_train.shape[0]
			* batch_size = 100
			* 
			* train_loss_list = []
			* train_acc_list = []
			* test_acc_list = []
			* 
			* iter_per_epoch = max(train_size / batch_size, 1)
			* epoch_cnt = 0
			* 
			* for i in range(1000000000):
			*     batch_mask = np.random.choice(train_size, batch_size)
			*     x_batch = x_train[batch_mask]
			*     t_batch = t_train[batch_mask]
			* 
			*     grads = network.gradient(x_batch, t_batch)
			*     optimizer.update(network.params, grads)
			* 
			*     if i % iter_per_epoch == 0:
			*         train_aac = network.accuracy(x_train, t_train)
			*         test_aac = network.accuracy(x_test, t_test)
			*         train_aac_list.append(train_acc)
			*         test_aac_list.append(test_acc)
			* 
			*         epoch += 1
			*         if epoch_cnt >= max_epochs:
			*             break
		* まずはじめにMNISTデータセットの訓練データ60000個から300個のみを抽出する。
		* ネットワークは複雑性を高めるために7層のネットワーク(各層のニューロン：100、活性化関数：ReLU)を用いる。
		* optimizerはSGDとし、学習係数は0.01とする。
		* train_acc_list、test_acc_listにはエポック単位で認識精度を確認し、その値を保存する。
		* ここで取得した訓練データ、テストデータに対するネットワークの認識精度(accuracy)を縦軸に取り、エポックを横軸にとってグラフを作成する。
		* まず訓練データに対する認識精度をプロットしたグラフは、徐々に認識精度が向上し、100エポックを過ぎたあたりからは、認識精度がほぼ100%になる。
		* 一方、テストデータに対する認識精度をプロットしたグラフも徐々に認識精度が向上していくが、200エポックの学習を行っても、認識精度は70%程度までしか向上していない。
		* これは訓練データに対する認識精度とテストデータに対する認識精度に大きな差が出ていると言えるが、訓練データだけに対し、適応し過ぎてしまい、訓練に使用していないテストデータへ対応が上手くできていない結果と言える。
		* 上記のような過学習を抑制する手法として、Weight decay(荷重減衰)がある。
		* これは学習の過程で大きな重みをもつことに対してペナルティを課し、過学習を抑える手法である。
		* 過学習が発生する原因として、重みパラメータが大きな値を取ってしまうことが挙げられるためである。
		* ここで、ニューラルネットワークの学習は損失関数の値を小さくすることを目的としている。
		* よって、重みの2乗ノルム(L2ノルム)を損失関数に加算することを考える。
		* 損失関数を学習の過程で小さくしていくので、ここに重みの2乗ノルムが含まれることにより、重みが極端に大きくなることを防ぐことができる。
		* L2ノルムは各要素の2乗和のことで、W = (w1, w2, ..., wn)としたとき、WのL2ノルムは√(w1^2+w2^2+...+wn^2)となる。

			* またL1ノルムという各要素の絶対値の和を表す表現もあり、L1ノルムは|w1|+|w2|+...+|wn|となる。
		* これらを踏まえ、L2ノルムのWeight decayは以下のようになる。

			* 1/2・λ・W^2

				* W：重み
				* λ：正則化の強さを制御するパラメータ
		* λは大きいほど、大きな重みを取ることに対して、強いペナルティを課すことを意味する。
		* また、1/2は1/2・λ・W^2を微分した際にλ・Wとなるため、その調整用の定数である。
		* 全ての重みに対して、損失関数に1/2・λ・W^2を加算しているため、誤差逆伝播法で重みの勾配を求める際、これまでの勾配の計算結果に正則化項としてλ・W^2(=1/2・λ・W^2の微分)を加算することになる。
		* これをもとに先ほどの実装に対してλ=0.1として、Weight decayを導入し、エポックが進むにつれて、訓練データ・テストデータそれぞれに対する認識精度がどう変化するかを確認する。
		* 結果として、訓練データ・テストデータの間の認識精度の隔たりは依然としてあるものの、その違いの大きさは小さくなることがわかる。
		* また、訓練データの認識精度も100%には達せず、これは過学習が抑制され、訓練データに適用しすぎていないことを表している。
		* 上述の通り、Weight decayにより、ある程度の過学習は抑制できる。
		* ただ、ネットワークが複雑になると、Weight decayだけでは抑制が困難となってくる。
		* ここで別の手法としてDropoutがある。
		* Dropoutは訓練時にネットワーク中のニューロンをランダムに消去しながら学習する手法である。
		* 消去されたニューロンは信号の伝達が行われなくなり、その先に接続されているニューロンも含め、信号伝達がストップする。
		* 訓練時はデータが流れるたびにランダムに選んだニューロンを消去するが、テスト時は全ニューロンに信号を伝達する。
		* その際は各ニューロンの出力に対して訓練時に消去した割合を乗算して出力を行うようになる。
		* 上記のDropoutを実装すると以下の通りになる。

			* class Dropout:
			*     def __init__(self, dropout_ratio=0.5):
			*         self.dropout_ratio = dropout_ratio
			*         self.mask = None
			* 
			*     def forward(self, x, train_flg=True):
			*         if train_flg:
			*             self.mask = np.random.rand(*x.shape) > self.dropout_ratio
			*             return x * self.mask
			*         else:
			*             return x * (1.0 - self.dropout_ratio)
			* 
			*     def backward(self, dout):
			*         return dout * self.mask
		* 上記の実装では、順伝播でself.maskに消去するニューロンをFalseとして格納している。
		* まず、xと同じ形状でかつランダムな値(0~1の実数値)を要素として持つ配列を生成する。
		* 生成した配列の各要素の値に対し、self.dropout_ratioより大きな要素はTrue、小さい要素はFalseとしている。
		* これにより、self.maskにはxと同じ形状でランダムにTrue/Falseを要素として持つ配列を格納することになる。
		* forward()関数の返り値は生成したself.maskとxの積を返し、xはランダムに要素が0となる。
		* 逆伝播時の挙動はReLUと同じとなる。
		* 上記により、順伝播時に信号を通したニューロンは逆伝播時にも信号を通し、順伝播時に信号を通さなかったニューロンは逆伝播時も信号を通さず、そこで信号がストップする。
		* このDropoutを使用したMNISTデータセットを認識するニューラルネットワークに組み込み、Dropoutのありなしで認識精度や学習の過程がどう変化するか検証する。
		* ここでも過学習を起こすために作成した7層のネットワーク(各層のニューロン：100、活性化関数：ReLU)を用いる。
		* この結果から、Dropoutを組み込んでいる場合は、訓練データとテストデータによる認識精度の隔たりが小さくなることがわかる。
		* また、訓練データの認識精度が100%に達することもなくなっていることがわかる。
		* Dropoutを用いることで層が深く表現力の高いネットワークであっても過学習を抑制することが可能と言える。
		* Dropoutと近い関係の学習方法としてアンサンブル学習と呼ばれる手法がある。
		* アンサンブル学習とは、複数のモデルを個別に学習させ、推論時はその複数のモデルの出力を平均するものである。
		* ニューラルネットワークでは同じ構造もしくは似た構造を持ったネットワークを用意してそれぞれ学習し、推論時にそれぞれの出力の平均を結果として返すものとなる。
		* アンサンブル学習により、ニューラルネットワークの認識精度が数%向上することが実験的にわかっている。
		* Dropoutは学習時にニューロンをランダムに消去することで毎回異なるモデルを学習していると考えると、Dropoutとアンサンブル学習は近い概念と言える。
		* Dropoutはアンサンブル学習と同じ効果を疑似的に1つのネットワークで実現していると考えられる。


	* ハイパーパラメータの検証

		* ニューラルネットワークでは重みやバイアスといったパラメータとは別にハイパーパラメータ(hyper-parameter)が多く存在する。
		* 具体的には各層のニューロンの数やバッチサイズ、パラメータ更新時の学習係数やWeight decayなどが該当する。
		* これらのハイパーパラメータは適切に設定しないと、モデルの性能が悪くなってしまうため、その設定値は非常に重要である。
		* しかし、その値の決定には多くの試行錯誤が伴うため、より効率的にハイパーパラメータの値を探索する方法が必要となる。
		* まず適切なハイパーパラメータの設定値を決めるための検証データについて考える。
		* これまでネットワークの学習にはデータセットを訓練データとテストデータに分離して使用している。
		* これは訓練データでネットワークの学習を行い、テストデータを使って汎化性能を評価するためである。
		* これにより、訓練データだけに過度に適応しすぎていないか(過学習していないか)、汎化性能がどのくらいかを評価することができている。
		* ここで、ハイパーパラメータを決定するために使用するデータにテストデータを使用してはいけない。
		* これはハイパーパラメータの値がテストデータに対して過学習を起こす可能性があるためである。
		* つまり、設定されているハイパーパラメータの値の「良さ」をテストデータで確認することになるので、テストデータだけに適合するようにハイパーパラメータの値が調整されてしまうことを意味する。
		* これは他のデータに適用できない汎化性能の低いモデルになってしまう可能性がある。
		* 以上より、ハイパーパラメータを調整するためには、それ専用の確認データが必要となる。
		* ハイパーパラメータ調整用のデータは検証データ(validation datya)と呼び、このデータを使ってハイパーパラメータの良さを評価する。
		* これまでのデータセットとその役割をまとめると以下のようになる。

			* 訓練データ(training data)：ネットワークの重みやバイアスの学習に使用
			* テストデータ(test data)：ネットワークの汎化性能を確認するために使用
			* 検証データ(validation data)：ハイパーパラメータの性能を確認するために使用
		* データセットによっては予め、訓練データ、テストデータ、検証データに分離されているものもある。
		* 一方、分離されていないデータセットを使用する場合は、ユーザの手で分離する必要がある。
		* MNISTデータセットは訓練データとテストデータにのみ分割されている。
		* よって、検証データを得るために訓練データの中から20%程度を検証データとして先に分離するようにする。
		* 検証データを分離する実装は以下の通りとなる。

			* (x_train, t_train), (x_test, t_test) = load_mnist()
			* 
			* # Shuffle training data
			* x_train, t_train = shuffle_dataset(x_train, t_train)
			* 
			* # Devide validation data into training data
			* validation_rate = 0.20
			* validation_num = int(x_train.shape[0] * validation_rate)
			* x_val = x_train[:validation_num]
			* t_val = t_train[:validation_num]
			* x_train = x_train[validation_num:]
			* t_train = t_train[validation_num:]
		* 上記で得られた検証データを用い、ハイパーパラメータの最適化手法を適用し、よりよいハイパーパラメータの値を探索する。
		* ハイパーパラメータの最適化のポイントは各値の「良い値」が存在する範囲を徐々に絞り込んでいくところにある。
		* これはまず始めに大まかにハイパーパラメータの値を設定し、その中からランダムにハイパーパラメータを選び、その値で認識精度の評価を行なう。
		* それを複数回繰り返し、それぞれの認識精度の結果を確認し、その結果からハイパーパラメータの「良い値」の範囲を狭めていく。
		* これにより、適切なハイパーパラメータの範囲を徐々に限定していく。
		* ニューラルネットワークのハイパーパラメータの最適化ではグリッドサーチのような規則的な探索よりもランダムにハイパーパラメータの候補になる値をサンプリングして探索する方がよい結果になることが知られている。
		* これは複数のハイパーパラメータのうち、最終的な認識精度に与える影響度合いがハイパーパラメータごとに異なるからである。
		* ハイパーパラメータの範囲はおおまかに指定するのが有効で、0.001～1000というように10のべき乗スケジュールで指定する。
		* またディープラーニングの学習には多くの時間が必要となる。
		* よって、ハイパーパラメータの探索では筋が悪い設定値は早い段階で見切りをつけられるのが良い。
		* これらを踏まえ、ハイパーパラメータの最適化では、学習のためのエポックを小さくし、1回の評価に必要となる時間を短縮するのが有効とされる。
		* これまでのハイパーパラメータの最適化手順をまとめると、以下のようになる。

			* ステップ0：

				* ハイパーパラメータの大まかな範囲を設定する。
			* ステップ1：

				* 設定された範囲からランダムにハイパーパラメータの候補をサンプリングする。
			* ステップ2：

				* サンプリングしたハイパーパラメータの値を使用して学習を行う、検証データで認識精度を評価する。
				* 学習のエポックを小さく設定する。
			* ステップ3：

				* ステップ1、ステップ2をある程度の回数(100回程度)繰り返し、その認識精度の結果からハイパーパラメータの範囲を狭める。
		* この手順で絞られたハイパーパラメータの範囲からハイパーパラメータとして採用する値を1つ選ぶ。
		* この手法は最適なハイパーパラメータを知るためのどちらかと言えば実践的な手法と言える。
		* より科学的で洗練された手法としてはベイズ最適化(Bayesian optimization)が挙げられる。
		* ベイズ最適化ではベイズの定理を中心とした数学理論を駆使し、より厳密に効率よく最適化を行なうことができる。
		* 次にMNISTデータセットを使ってハイパーパラメータの最適化を行なう。
		* ここでは学習係数とWeight decay(1/2・λ・W^2)の強さλを探索するハイパーパラメータの対象とする。
		* まず、ハイパーパラメータの範囲をそれぞれ指定する。ここでは以下のように指定する。

			* lr = 10 ** np.random.uniform(-6, -2)
			* weight_decay = 10 ** np.random.uniform(-8, -4)
		* 上記の通り、学習係数の範囲を10^-6 ～ 10^-2、Weight decayの範囲を10^-8 ～ 10^-4に指定し、最適化をスタートさせる。
		* これを複数回繰り返して様々なハイパーパラメータの値で学習を行い、最適なハイパーパラメータの値がどこかを観察する。
		* この学習の結果の例をaccuracyが良い順に並べて示すと以下のようになる。

			* Best- 1 (val acc:0.83) | lr:0.0092, weight decay:3.86e-07
			* Best- 2 (val acc:0.78) | lr:0.00956, weight decay:6.04e-07
			* Best- 3 (val acc:0.77) | lr:0.00571, weight decay:1.27e-06
			* Best- 4 (val acc:0.74) | lr:0.00626, weight decay:1.43e-05
			* Best- 5 (val acc:0.73) | lr:0.0052, weight decay:8.97e-06
		* 上記の通り、探索しているハイパーパラメータ(lr, Weight decay)と検証データによる認識精度(val acc)を学習の結果として取得する。
		* これを踏まえると、 認識精度が高いのは、学習係数の範囲が10^-4 ～ 10^-2、Weight decayの範囲が10^-8 ～ 10^-6であると言える。
		* この結果が得られたら、さらに値の範囲を小さくし、同様の手順を繰り返すことでよりよいハイパーパラメータの値を決めることができる。


	* 畳み込みニューラルネットワークとは？

		* 畳み込みニューラルネットワーク(CNN:Convolutional neural network)は画像認識や音声認識などあらゆる認識問題で使用されている。
		* また画像認識のコンペティションではディープラーニングによる手法のほぼすべてがCNNをベースとしている。
		* まずCNNのネットワーク構造全体を確認する。CNNもこれまでのニューラルネットワークと同様、レイヤを組み合わせることで作ることができる。
		* CNNの場合、新たにConvolutionalレイヤ(畳み込み層)とPoolingレイヤ(プーリング層)を導入する必要がある。
		* これまでのニューラルネットワークでは、隣接する層の全てのニューロン間で結合をもっており、これを全結合(fully-connected)と呼ぶ。
		* これまで全結合層はAffineレイヤとして実装しており、Affineレイヤを使うことで例えば、5層の全結合ニューラルネットワークを以下のような構成で実現できる。

			* |Input| ---- |Affine|--|Relu| ---- |Affine|--|Relu| ---- |Affine|--|Relu| ---- |Affine|--|Relu| ---- |Affine|--|Softmax|
		* 全結合ニューラルネットワークではAffineレイヤの後に活性化関数のレイヤ(ReLU、Sigmoidなど)を配置する。
		* 最後の層はAffineレイヤの後にSoftmaxレイヤを配置し、最終的な結果(確率)を出力する。
		* 次にCNNはConvolutionalレイヤやPoolingレイヤを用いると、以下のような構成となる。

			* |Input| ---- |Conv|--|Relu|--|Pooling| ---- |Conv|--|Relu|--|Pooling|---- |Conv|--|Relu| ---- |Affine|--|Relu| ---- |Affine|--|Softmax|
		* CNNのレイヤのつながり順は上記の通り、「Convolution - ReLU - (Pooling)」となるが、Pooling層が省略されることもある。
		* これは全結合ニューラルネットワークにおける「Affine - ReLU」が置き換わったと考えることができる。
		* ただ出力層に近い層では、「Affine - ReLU」が用いられ、また出力層ではこれまでと同じ「Affine - Softmax」が用いられることが多い。
		* ここで全結合ニューラルネットワークの問題点を考える。
		* 全結合層(Affineレイヤ)では隣接する層のニューロンが全て連結されており、出力の数は任意に決めることが可能である。
		* 一方、データに形状がある場合、その形状が無視されてしまう。
		* 例えば、画像データは通常、縦・横・チャンネル方向の3次元の形状を持つ。
		* この画像データを全結合ネットワークに入力する場合、3次元データを1次元データ(flatten)にする必要がある。
		* これまでMNISTデータを扱う際、(1, 28, 28)の形状を1次元の784個のデータに変形し、Affineレイヤに入力してきた。
		* 画像は3次元形状であり、その形状には以下のような空間的情報が含まれている。

			* 空間的に近いピクセルは似た値である。
			* RGBの各チャンネル間にそれぞれ密接な関連性がある。
			* 距離が離れているピクセル同士はあまり関わりがない。
		* これらの空間的情報の中にはくみ取るべき本質的なパターンが含まれている可能性がある。
		* 全結合層ではこれらの形状を無視して全ての入力データを同等のニューロンとして扱ってしまうので、これらの情報を学習に生かすことができない。
		* 一方、畳み込み層は形状を維持して学習を行う。
		* 画像の場合、入力データを3次元データとして受け取り、3次元データのまま次の層にデータを出力する。そのため、CNNでは画像などの形状を有したデータを正しく理解することができる。
		* CNNでは畳み込み層の入出力データを特徴マップ(feature map)と呼び、特に畳み込み層の入力データを入力特徴マップ、出力データを出力特徴マップと呼ぶ。


	* 畳み込み層での畳み込み演算

		* 畳み込み層では畳み込み演算を行なう。畳み込み演算とは画像処理におけるフィルター演算に相当する。
		* 畳み込み演算処理は入力データに対し、フィルターを適用する。
		* 処理の例を以下の入力データ・フィルターを使って示す。

			* 入力データ

				* 1　2　3　0
				* 0　1　2　3
				* 3　0　1　2
				* 2　3　0　1
			* フィルター

				* 2　0　1
				* 0　1　2
				* 1　0　2
		* この例の入力データは縦・横方向に形状を持つデータで、フィルターも同様に縦・横に次元を持っている。
		* これら2次元データの形状は(height, width)として表現される。
		* この表現を用いると、入力データ、フィルターの形状はそれぞれ(4, 4)、(3, 3)と表現できる。
		* フィルターはカーネルとも呼ばれる。
		* 畳み込み演算では入力データに対して、フィルターのウィンドウを一定の間隔でスライドさせて適用する。
		* ウィンドウとはフィルターと同じサイズで入力データにフィルターを適用させる範囲のことを指す。
		* 畳み込み演算では各ウィンドウ位置ごとに入力データとフィルターの要素の値を乗算し、その和を求める。
		* この積和計算の結果を出力の対応する場所に格納する。
		* まず最初のウィンドウは入力の左上にあるため、以下の要素同士の積和計算を行う。

			* 1　2　3　　　2　0　1
			* 0　1　2　＊　0　1　2 = 1*2+2*0+3*1+0*0+1*1+2*2+3*1+0*0+1*2 = 2+0+3+0+1+4+3+0+2 = 15
			* 3　0　1　　　1　0　2
		* 次に右上のウィンドウにおける積和計算は以下のようになる。

			* 2　3　0　　　2　0　1
			* 1　2　3　＊　0　1　2 = 2*2+3*0+0*1+1*0+2*1+3*2+0*1+1*0+2*2 = 4+0+0+0+2+6+0+0+4 = 16
			* 0　1　2　　　1　0　2
		* 同様に左下、右下のウィンドウにおける積和計算は以下のようになる。

			* 0　1　2　　　2　0　1
			* 3　0　1　＊　0　1　2 = 0*2+1*0+2*1+3*0+0*1+1*2+2*1+3*0+0*2 = 0+0+2+0+0+2+2+0+0=6
			* 2　3　0　　　1　0　2
			* 
			* 1　2　3　　　2　0　1
			* 0　1　2　＊　0　1　2 = 1*2+2*0+3*1+0*0+1*1+2*2+3*1+0*0+1*2 = 2+0+3+0+1+4+3+0+2 = 15
			* 3　0　1　　　1　0　2
		* これらの積和計算の結果を(2, 2)のデータとして表すと以下のようになる。

			* 15　16
			*   6　15
		* 次に全結合層にも存在していたバイアスを考える。
		* 上記の畳み込み演算処理の内容からもわかる通り、CNNの場合、フィルターのパラメータが重みに対応する。
		* 一方、CNNにもバイアスは存在し、フィルター適用後のデータに対し、バイアス項の加算を行なう。
		* バイアスは常に(1, 1)であり、以下のようにフィルター適用後のデータの各要素に加算処理を行う。

			* 15　16　　　　　　　18　19
			* 　　　　　+　3　=
			*   6　15　　　　　　　  9　18
		* 上記のようにバイアス項の加算を行って得られたデータが畳み込み層の出力データとなる。
		* 次に3次元データの畳み込み演算を考える。
		* これまでは縦と横の2次元の形状のデータを対象としてきたが、多くの画像データの場合、縦・横に加え、チャンネル方向も合わせた3次元のデータとして扱う必要が出てくる。
		* ただ畳み込み演算の手順はこれまでの2次元の場合と基本的には同じとなる。
		* チャンネル方向に複数の特徴マップ(畳み込み層の入出力データ)がある場合、チャンネルごとに入力データとフィルターの畳み込み演算を行い、それらの結果を加算して１つの出力を得ることになる。
		* 以下のように(4, 4)の画像データかつ3チャンネルもつような特徴マップ、(3, 3)かつ3チャンネルを持つフィルターを考えると、出力は以下のようになる。

			* 1　2　3　0　　　2　0　1　　　15　14
			* 0　1　2　3　＊　0　1　2　＝　  6　13
			* 3　0　1　2　　　1　0　2
			* 2　3　0　1
			* 
			* 3　0　6　5　　　0　1　3　　　54　45　　　　　 15+54+58　14+45+44　　　127　103
			* 5　3　0　6　＊　3　0　1　＝　42　54　　　　＝　6+42+32　13+54+58　＝　  80　125
			* 6　5　3　0　　　1　3　0
			* 0　6　5　3
			* 
			* 4　2　1　2　　　4　0　2　　　58　44
			* 2　4　2　1　＊　2　4　0　＝　32　58
			* 1　2　4　2　　　0　2　4
			* 2　1　2　4
		* 上記の例から分かる通り、入力データとフィルターのチャンネル数は同じにする必要がある。
		* 多くの場合、入力データのチャンネル数が決まっているので、フィルターを入力データのチャンネル数に合わせて用紙する必要がある。
		* 一方、フィルターのサイズは任意に設定することができる。
		* ただし、上記では(3,4,4)の入力特徴マップが畳み込み層を通ることにより、(2,2)の出力特徴マップとなり、画像の空間的情報が失われていると言える。
		* 次に出力特徴マップをチャンネル方向に複数持たせることを考える。
		* この場合、上記では複数チャンネルかつ縦・横方向に次元を持つフィルターを複数持つことで実現できる。
		* ここで、入力特徴マップ、フィルターを(チャンネル数, 縦サイズ, 横サイズ)、出力特徴マップを(縦サイズ、横サイズ)として表すと、以下のようになる。

			* 入力特徴マップ：(C, H, W)
			* フィルター　　：(C, FH, FW)
			* 出力特徴マップ：(OH, OW)
		* 出力特徴マップをチャンネル方向にFNチャンネル持たせるためには、以下のようにフィルターをFN個用意する必要がある。

			* 入力特徴マップ：(C, H, W)
			* フィルター　　：(FN, C, FH, FW)
			* 出力特徴マップ：(FN, OH, OW)
		* 上記により、出力特徴マップの形状が(FN, OH, OW)の3次元となり、空間的情報が維持されたまま次の層に渡される。
		* 次に3次元データの畳み込み演算におけるバイアス加算処理について考える。
		* この場合、バイアスは1チャンネルごとに1データだけを持ち、データとしての形状は(FN, 1, 1)で表される。
		* このデータが各チャンネルごとに同じ値だけバイアスの加算処理が行われる。


	* 畳み込み層におけるパディング

		* 畳み込み層において畳み込み演算を行なう前に入力データの周りに固定の値(0など)のデータを埋めることがある。
		* この処理をパディング(padding)と呼び、畳み込み演算でよく用いられる。
		* 例えば、上記の入力データに対し、幅1のパディングを行ってから畳み込み演算を行なうと以下のようになる。

			* 入力データ(幅1のパディング処理後)

				* 0　0　0　0　0　0
				* 0　1　2　3　0　0
				* 0　0　1　2　3　0
				* 0　3　0　1　2　0
				* 0　2　3　0　1　0
				* 0　0　0　0　0　0
			* フィルター(カーネル)

				* 2　0　1
				* 0　1　2
				* 1　0　2
			* 出力データ

				*   7　12　10　  2
				*   4　15　16　10
				* 10　  6　15　  6
				*   8　10　  4　  3
		* 上記の通り、入力データに幅1のパディング処理を行うと、畳み込み層への入力データのサイズは(4, 4)から(6, 6)に形状が変わる。
		* そこに前述の例と同じ(3, 3)のフィルターを適用すると、(4, 4)の出力データが生成される。
		* パディング幅は1以外にも2や3など任意の整数を設定できる。
		* パディング処理は出力データの形状を調整するために行う。
		* 例えば、(4,4)の入力データに(3,3)のフィルターを適用すると、出力データは(2,2)になり、この場合、出力データは入力データの半分のサイズとなる。
		* これは畳み込み層を重ね、何度も畳み込み演算を行うディープニューラルネットワークでは問題となる。
		* 畳み込み演算を行うたびに空間的に縮小されてしまうため、ある時点で出力サイズが1となり、それ以上畳み込み演算が適用できなくなってしまうからである。
		* このような場合にパディング処理を適用する。入力データに幅1の0パディングを行なうことで入力データは(6,6)となり、その結果、出力データは(4,4)とすることができる。
		* このようにパディングを用いることにより、パディング前の入力データと出力データの形状を同じにすることもできる。


	* 畳み込み層におけるストライド

		* フィルターを適用する位置の間隔をストライド(stride)と呼ぶ。
		* これまでの例ではストライドが1だったが、例えば、ストライドを2にすると、フィルターを適用するウィンドウの間隔が2要素ごとになる。
		* 入力サイズが(7,7)のデータに対して、ストライドが2で(3,3)のフィルターを適用すると出力サイズは(3,3)となる。
		* ストライドは大きな値を指定すると出力サイズが小さくなり、一方、パディングは大きな値を設定すれば、出力サイズが大きくなる。
		* この関係を定式化すると、以下のようになる。

			* OH = (H + 2P - FH) / S + 1　(7.1)
			* OW = (W + 2P -FW) / S + 1

				* (OH, OW)：出力サイズ
				* (H, W)：入力サイズ
				* (FH, FW)：フィルターサイズ
				* P：パディング幅
				* S：ストライド幅
		* (7.1)に畳み込み層の各設定値を代入することでその畳み込み層の出力サイズ(OH, OW)が算出できる。
		* ただし、(OH, OW)はサイズのため、(7.1)が整数で割り切れる値になる必要がある。
		* もし、結果が小数になる場合は、エラーを出力するなどの対応が必要となる。
		* ただ多くのディープラーニングのフレームワークは結果が小数となった場合は、近い整数に丸めてエラーを出さずに実行が進むようになっている場合もある。


	* 畳み込み層におけるバッチ処理

		* ニューラルネットワークの処理では、入力データを１つのまとまりとして扱うバッチ処理を行なったが、畳み込み演算でも同様にバッチ処理に対応することを考える。
		* バッチ処理のためには、各層を流れるデータとして4次元のデータを考える。
		* 具体的には(batch_num, channel, height, width)という順でデータを格納する。
		* 例えば、N個のデータをミニバッチとする場合、データの形状は以下のようになる。

			* 入力特徴マップ：(N, C, H, W)
			* フィルター　　：(FN, C, FH, FW)
			* バイアス　　　：(FN, 1, 1)
			* 出力特徴マップ：(N, FN, OH, OW)
		* 上記の通り、各データの先頭にバッチ用の次元が追加される。
		* このN個のデータのミニバッチに対応することで、1回の処理でN個のデータに畳み込み演算を行なうことができる。
		* つまり、N回分の畳み込み演算処理を1回にまとめて行っていることになる。


	* プーリング層

		* プーリング層で行うプーリング処理は縦・横方向の空間を小さくする演算である。
		* 例えば、4×4データの2×2の領域を1つの要素に集約し、2×2にする処理は以下のようになる。

			* 1　2　1　0
			* 0　1　2　3　　→　　2　3
			* 3　0　1　2　　→　　4　2
			* 2　4　0　1
		* 上記は2×2のMaxプーリングをストライド2で行った場合の例となる。
		* Maxプーリングはその領域における最大値を取る演算である。
		* また、一般的にプーリング処理ではプーリングのウィンドウサイズとストライドを同じ値にする。
		* つまり、プーリング処理は要素を重ねないで処理することが一般的である。
		* プーリング処理にはMaxプーリングの他にウィンドウの要素の平均を取るAverangeプーリングなどがあるが、画像認識分野においてはMaxプーリングを用いることが多い。
		* プーリング層の特徴として、まず学習するパラメータがないことが挙げられる。
		* 上述の通り、プーリング層ではウィンドウ内の要素から最大値をとる、平均値をとる、といった処理を行うだけのため、学習するパラメータは存在しないと言える。
		* また、プーリング処理の前後でチャンネル数は変化しない。これはプーリング処理はチャンネルごと独立して行われるためである。
		* さらにプーリング層は微小な位置変化に対してロバスト性があると言える。
		* これは、入力データに小さなズレがあったとしても、プーリング層の出力は同じような結果になる性質があるためである。
		* 例えば、以下のような微小な位置のズレを持つ入力データ同士を考え、それぞれに対し、3×3のプーリング処理を行うことを考える。

			* 1　2　0　7　1　0　　　1　1　2　0　7　1
			* 0　9　2　3　2　3　　　3　0　9　2　3　2
			* 3　0　1　2　1　2　　　2　3　0　1　2　1
			* 2　4　0　1　0　1　　　3　2　4　0　1　0
			* 6　0　1　2　1　2　　　2　6　0　1　2　1
			* 2　4　0　1　8　1　　　1　2　4　0　1　8
		* 上記の入力データに3×3のMaxプーリング処理を行った結果はどちらも以下の通りになる。

			* 9　7
			* 6　8


	* Convolutional / Poolingレイヤの実装に用いるim2col

		* ここまでの畳み込み層とプーリング層を実装する。
		* それぞれのレイヤのクラスにはこれまでのレイヤの実装と同様、forward()とbackward()関数を持たせる。
		* 畳み込み層やプーリング層の実装は複雑になりそうだが、im2col関数を用いることで簡単に実装することができる。
		* まず上述の通り、CNNには各層を(batch_num, channel, height, width)の4次元データが流れる。
		* つまり、(10, 1, 28, 28)は縦28、横28で1チャンネルのデータ10個の入力データを表す。
		* このデータは以下のように実装し、生成することができる。

			* np.random.rand(10, 1, 28, 28)
			* x.shape　#(10, 1, 28, 28)
		* 上記の場合、バッチの1つ目のデータにアクセスする場合は、x[0]を指定することでアクセスできる。

			* x[0].shape　#(1, 28, 28)
		* 同様にバッチの1つ目のデータの1チャンネル目のデータにアクセスする場合は、x[0, 0]を指定することでアクセスできる。
		* これを前提にim2colという関数を使って、畳み込み演算を実装する。
		* 通常、畳み込み演算を実装しようとすると、for文を何層にも重ねた複雑な実装になってしまい、またNumPyでfor文を使うと処理が遅くなってしまう欠点がある。
		* 一方、im2col関数はフィルターにとって都合のいいように入力データを展開する関数である。
		* フィルターにとって都合がいいとは例えば、バッチ数も含めた4次元の入力データに対して、im2col関数を適用すると2次元の行列に変換される。
		* これは入力データに対してフィルタの適用する領域(=3次元ブロック)を横方向の1列に展開し、これをフィルターを適用する全ての場所に対して行ない、2次元行列を生成することを意味する。
		* 各フィルターをストライド幅ごとに適用し、それをim2col関数で1列に展開するため、フィルターを適用した回数分2次元配列の行が追加されていく。
		* これは、元の入力データの要素数よりもim2col関数で展開後の要素数の方が、設定されるストライド幅(フィルターを入力データに適用する回数)によっては、大きくなることを意味する。
		* つまりim2col関数を使った実装は通常の実装よりもメモリを多く消費すると言える。
		* ただ大きな行列の計算は行列計算ライブラリにより、高度な最適化がされており、大きな行列計算を高速に行うことができる。
		* つまり畳み込み演算を大規模な行列計算に帰着させることで、高速な行列計算ライブラリを有効活用することができる。
		* 入力データをim2colで展開した後は畳み込みフィルターを縦に展開する(1つのフィルタを列方向に1列に展開し、フィルターごとに列が追加されていく)。
		* 次にim2col関数で展開した入力データと列方向に展開したフィルターの2つの行列の積を計算する。これは全結合層のAffineレイヤで行ったことと同じである。
		* 最後に出力されたデータを次のレイヤに合わせた適切な形に整形(reshape)することで畳み込み層における処理が完了する。
		* im2col関数は以下のようなインターフェースを持つ関数として、Convolutional / Poolingレイヤを実装する。

			* im2col(input_data, filter_h, filter_w, stride=1, pad=0)

				* input_data：(batch_num, channel, height, width)の4次元配列からなる入力データ
				* filter_h：フィルターの高さ
				* filter_w：フィルターの幅
				* stride：ストライド幅
				* pad：パディング


	* Convolutionレイヤの実装

		* Convolutional レイヤを実装する。
		* まず入力データを生成し、im2col関数で展開する実装は以下の通りになる。

			* import sys, os
			* sys.path.append(os.pardir)
			* from common.util import im2col
			* 
			* x1 = np.random.rand(1, 3, 7, 7)　#batch size:1
			* col1 = im2col(x1, 5, 5, stride=1, pad=0)
			* print(col1.shape)　#(9, 75)
			* 
			* x2 = np.random.rand(10, 3, 7, 7)　#batch size:10
			* col2 = im2col(x2, 5, 5, stride=1, pad=0)
			* print(col2.shape)　#(90, 75)
		* 1つ目のデータはバッチサイズ1で7×7の3チャンネルデータであり、5×5のフィルターでストライド幅1、パディングなしでim2col関数に渡す。
		* 2つ目のデータはバッチサイズ10で7×7の3チャンネルデータであり、5×5のフィルターでストライド幅1、パディングなしでim2col関数に渡す。
		* im2col()関数で展開した後の出力データの形状はそれぞれ(9, 75)、(90, 75)となる。
		* ここで75は3チャンネル×５×５サイズのデータを意味している。
		* 次に畳み込み層をim2col()関数を用いて実装すると、以下のようになる。

			* class Convolution:
			*     def __init__(self, W, b, stride=1, pad=0):
			*         self.W=W
			*         self,b = b
			*         self.stride = stride
			*         self.pad = pad
			* 
			*     def forward(self, x):
			*         FN, C, FH, FW = self.W.shape
			*         N, C, H, W = x.shape
			*         out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
			*         out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
			* 
			*         col = im2col(x, FH, FW, self.stride, self.pad)
			*         col_W = self.W.reshape(FN, -1)　#Extract filters
			*         out = np.dot(col, col_W) + self.b
			* 
			*         out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
			* 
			*         return out
		* まず初期化メソッド(コンストラクタ)ではフィルター(重み)、バイアス、ストライド、パディングを引数として受け取る。
		* フィルターは(FN, C, FH, FW)の４次元の形状でそれぞれFilter Number、Channel、Filter Height、Filter Widthを表し、受け取ったWのshapeをそれぞれ格納する。
		* また、(7.1)を実装し、出力データのHeight/Widthをout_h、out_wとして計算する。
		* 次にim2col()関数で入力データを展開し、フィルタの数分の行数に整形するためにreshape()関数を合わせて用いる。
		* reshape(FN, -1)のように-1を指定することで多次元配列の要素数のつじつまが合うように整形することができる。
		* ここでは、行数がFNと指定されているため、要素をFN行の行列に整形される。
		* 例えば、(10, 3, 5, 5)に対し、reshape(10, -1)とすると、実際は(10, 75)に整形される。
		* 次に展開された行列に対して行列計算を行い、最後に出力サイズを整形する。
		* 行列計算後の出力をreshape()関数で成形し、transpose()関数で軸の順番を入れ替える処理を行い、次の層の入力データになるように処理する。
		* 次にConvolutionレイヤの逆伝播の実装はAffineレイヤの実装と共通する部分が多いため、省略。
		* ただし、Affineレイヤの逆伝播と異なる実装として、im2col()関数の逆の処理を行う必要がある。
		* ここでは、col2im()関数を使用することで簡単に実現することが可能。


	* Poolingレイヤの実装

		* Poolingレイヤの実装もConvolutionレイヤと同様、im2col()関数を使って入力データを展開する。
		* ただしPollingレイヤではチャンネル方向には独立して処理する点がConvolutionレイヤとは異なる。
		* つまりプーリングの適用領域はチャンネルごとに独立して展開する。
		* 例えば、以下の3チャンネルの入力データがあったとする。

			* 1　2　3　0　　　3　0　6　5　　　4　2　1　2
			* 0　1　2　4　　　4　2　4　3　　　0　1　0　4
			* 1　0　4　2　　　3　0　1　0　　　3　0　6　2
			* 3　2　0　1　　　2　3　3　1　　　4　2　4　5
		* この入力データを2×2のウィンドウでMaxプーリングを行なう場合、例えば1チャンネル目は以下のようになる。

			* 1　2　　　3　0　　　1　0　　　4　2
			* 0　1　　　2　4　　　3　2　　　0　1
		* この2×2の行列を1×4に変形し、4つ合わせて以下のような4×4の行列にする。

			* 1　2　0　1
			* 3　0　2　4
			* 1　0　3　2
			* 4　2　0　1
		* 同様に2チャンネル目、3チャンネル目に関しても4×4の行列を生成し、それらを全て行方向に接続して12×4の行列にすると以下のようになる。

			* 1　2　0　1　　→　2　　　2　4
			* 3　0　2　4　　→　4　　　3　4
			* 1　0　3　2　　→　3
			* 4　2　0　1　　→　4
			* 3　0　4　2　　→　4　　　4　6
			* 6　5　4　3　　→　6　　　3　3
			* 3　0　2　3　　→　3
			* 1　0　3　1　　→　3
			* 4　2　0　1　　→　4　　　4　4
			* 1　2　0　4　　→　4　　　4　6
			* 3　0　4　2　　→　4
			* 6　2　4　5　　→　6
		* 上記のような展開した行列に対して行ごとに最大値を求め、最後に適切な形状(3チャンネル2×2行列)に整形する処理を行う。
		* Poolingレイヤの実装は以下の通り。

			* class Pooling:
			*     def __init__(self, pool_h, pool_w, stride=1, pad=0):
			*         self.pool_h = pool_h
			*         self.pool_w = pool_w
			*         self.stride = stride
			*         self.pad = pad
			* 
			*     def forward(self, x):
			*         N, C, H, W = x.shape
			*         out_h = int(1 + (H - self.pool_h) / self.stride)
			*         out_w = int(1 + (W - 2*self.pool_w) / self.stride)
			*         # Extract input data
			*         col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
			*         col = col.reshape(-1, self.pool_h*self.pool_w)
			*         # Get maximum value
			*         out = np.max(col, axis=1)
			*         # Shape
			*         out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
			* 
			*         return out
		* Convolutionレイヤと同様、Poolingレイヤのforward()関数の処理もim2col()関数を用い、行列に展開することで簡単に実装することができる。
		* Poolingレイヤの逆伝播処理はこれまでと同じのため、省略。


	* CNN(畳み込みニューラルネットワーク)の実装

		* ここまでで実装したConvolutionレイヤとPoolingレイヤを用いて、手書き数字認識を行うCNNを実装する。
		* 実装するネットワークの構成は以下とし、SimpleConvNetという名前で実装する。

			* |Input| ---- |Conv|--|Relu|--|Pooling| ---- |Affine|--|Relu| ---- |Affine|--|Softmax|
		* まず、コンストラクタの実装は以下の通り。

			* class SimpleConvNet:
			*     def __init__(self, input_dim=(1, 28, 28),
			*                       conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},
			*                       hidden_size=100, output_size=10, weight_init_std=0.01):
			*         filter_num = conv_param['filter_num']
			*         filter_size =conv_param['filter_size']
			*         filter_pad =conv_param['pad']
			*         filter_stride =conv_param['stride']
			*         input_size = input_dim[1]
			*         conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride +1
			*         pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))
		* SimpleConvNetクラスは上記の通り、以下の引数を取る。

			* input_dim：入力データの(チャンネル数、高さ、幅)の次元数
			* conv_param：Convolutionレイヤのハイパーパラメータのディクショナリ
			*         filter_num：フィルター数
			*         filter_size：フィルターサイズ
			*         stride：ストライド幅
			*         pad：パディング幅
			* hidden_size：隠れ層(全結合)のニューロン数
			* output_size：出力層(全結合)のニューロン数
			* weight_init_std：初期化時の重みの標準偏差
		* コンストラクタでは、Convolutionレイヤのハイパーパラメータをディクショナリから取り出し、変数に格納する。
		* また、畳み込み層の出力サイズを(7.1)から算出する。
		* 続いて、パラメータの初期化を以下のとおり行う。

			*         self.params = {}
			*         self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
			*         self.params['b1'] = np.zeros(filter_num)
			*         self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)
			*         self.params['b2'] = np.zeros(hidden_size)
			*         self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)
			*         self.params['b3'] = np.zeros(output_size)
		* 上記の通り、パラメータは1層目の畳み込み層、残りの全結合層の重みとバイアスであり、それぞれの初期値をparamsディクショナリに格納する。
		* 1層目の畳み込み層の重みとバイアスをそれぞれW1、b1とし、2層目・3層目の全結合層の重みとバイアスをそれぞれ、W2、b2、W3、b3とし、初期化する。
		* 最後にレイヤの生成を以下の通り行なう。

			*         self.layers = OrderedDict()
			*         self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])
			*         self.layers['Relu1'] = Relu()
			*         self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
			*         self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
			*         self.layers['Relu2'] = Relu()
			*         self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])
			*         self.last_layer = SoftmaxWithLoss()
		* layersはOrderedDict()クラスのインスタンスとして生成し、追加した順にディクショナリにレイヤを追加できるようにし、最後のSoftmaxWithLossレイヤのみ別の変数last_layerに格納する。
		* ここまでがSimpleConvNetクラスのコンストラクタの実装となり、次にメソッドとしてpredict()、loss()を以下の通り実装する。

			*     def predict(self, x):
			*         for layer in self.layers.values():
			*             x = layer.forward(x)
			*         return x
			* 
			*     def loss(self, x, t):
			*         y = self.predict(x)
			*         return self.lastLayer.forward(y, t)
		* predictメソッドでは生成された各層を順番にforward処理を行ない、その結果を返す。
		* lossメソッドでは、最後のSoftmaxWithLossレイヤも含め、forward処理を行う。
		* 次に誤差逆伝播法による勾配を求める実装は以下のようになる。

			*     def gradient(self, x, t):
			*         # forward
			*         self.loss(x, t)
			* 
			*         # backward
			*         dout = 1
			*         dout = self.lastlayer.backward(dout)
			*         layers = list(self.layers.values())
			*         layers.reverse()
			*         for layer in layers:
			*             dout = layer.backward(dout)
			* 
			*         # Settings
			*         grads = {}
			*         grads['W1'] = self.layers['Conv1'].dW
			*         grads['b1'] = self.layers['Conv1'].db
			*         grads['W2'] = self.layers['Affine1'].dW
			*         grads['b2'] = self.layers['Affine1'].db
			*         grads['W3'] = self.layers['Affine2'].dW
			*         grads['b3'] = self.layers['Affine2'].db
			* 
			*         return grads
		* パラメータの勾配はこれまで通り、順伝播処理の後、誤差逆伝播法(逆伝播)で求める。
		* 最後にgradsというディクショナリに各パラメータの勾配を格納する。
		* これまで実装したSimpleConvNetを使って、MNISTデータセットを学習すると、訓練データの認識率は99.82％、テストデータの認識率は98.96％程度となる。
		* この値はSimpleConvNet程度の小さなネットワークではとても高い認識率と言える。
		* これにさらに層を重ねることで、テストデータの認識率は99％を超えるまでのネットワークを実現できる。
		* ここまでで、畳み込み層とプーリング層は画像認識において必須のレイヤと言える。
		* これは画像という空間的な形状のある特性をCNNがうまく読み取れるからである。


	* CNNの可視化

		* ここでは畳み込み層の可視化を行い、畳み込み層は入力データの何を見ているのかを探索する。
		* SimpleConvNetでは、1層目の畳み込み層の重み(フィルター)の形状は(30, 1, 5, 5)としていた。

			* サイズが5×5の1チャンネルフィルターが30枚
		* フィルターのサイズが5×5でチャンネル数が1の場合、1つのフィルターは1チャンネルのグレー画像として可視化できることを意味している。
		* このフィルターを画像として表示し、学習前と学習後のフィルター(重み)を比較してみる。
		* まず学習前のフィルターはランダムに初期化されているため、白黒の濃淡に規則性はない。
		* 一方、学習後のフィルターは規則性のある画像となっている。
		* 白から黒へグラデーションを伴って変化するフィルターや塊(blob)のある領域を持つフィルターなどが見られる。
		* フィルターは特にエッジ(色が変化する境目)やブロブ(局所的に塊のある領域)を見ており、それらと入力データと一致する箇所とを比較している。
		* 例えば、左半分が白、右半分が黒のフィルターがあった場合、このフィルターは入力データの縦方向のエッジに反応するフィルターと言える。
		* 上記により、畳み込み層のフィルターはエッジやブロブなど比較的プリミティブな情報を抽出していると言える。
		* このようなプリミティブな情報が後段の層に渡されていく、ということがCNNで行われている。
		* 次に何層にも重ねたCNNでは各層でどのような情報が抽出されるかを考える。
		* ディープラーニングの可視化に関する研究では、層が深くなるにつれて、抽出される情報(強く反応するニューロン)がより抽象化されていくということが示されている。
		* 最初の層は単純なエッジにだけ反応し、次の層ではテクスチャ(個体の集合に対する特徴)に反応し、さらに次の層ではより複雑な物体のパーツに反応するようになる。
		* これは層が深くなるにつれ、ニューロンは単純な形状から高度な情報へと反応するように変化していくことを意味している。


	* 代表的なCNN - LeNet

		* これまで様々なネットワーク構成のCNNが提案されてきており、その1つがLeNetである。
		* LeNetは1998年に提案されたCNNの元祖ともいえるモデルである。
		* LeNetは畳み込み層とプーリング層(正確には要素を間引くサブサンプリング層)を連続して行い、全結合層を経て結果が出力される。
		* LeNetは現在の最新のCNNと比較すると主に以下の2つの違いがある。
		* 1つは活性化関数が現在は主にLeLUが使われているのに対し、LeNetではシグモイド関数が使用されている。
		* もう1つはPooling層でsubsamplingによって、中間データのサイズ縮小を行っているが、現在のCNNではMax poolingが主流である。
		* このような違いはあるものの大きな違いはなく、20年近く前にCNNの原型が作られたと言える。


	* 代表的なCNN - AlexNet

		* AlexNetはLeNetから20年近く経ってから発表され、AlexNetがディープラーニングブームの火付け役となった。
		* ただそのネットワーク構成は基本的にLeNetと大きく変わっていない。
		* LeNetとの違いは以下の通り。

			* 活性化関数にLeLUを用いている。
			* LRN(Local Response Normalization)という局所的正規化を行う層を用いている。
			* Dropoutを用いている。


	* ディープラーニング

		* ディープラーニングは層を深くしたディープなニューラルネットワークでの学習のことである。
		* これまでのネットワークをベースに層を重ねるだけでもネットワークを作ることができるが、ディープなネットワークにすることにも課題がある。
		* ここではディープラーニングの性質と課題、その可能性について俯瞰していく。
		* 以下ではこれまでのニューラルネットワークの様々なテクニックを使い、よりディープなネットワークを構成し、精度の高いMNISTデータセットの手書き数字認識を行う。
		* 以下のようなVGGというモデルに似たネットワークを構成する。

			* |Input| ---- |Conv|--|Relu|--|Conv|--|Relu|--|Pooling| ---- |Conv|--|Relu|--|Conv|--|Relu|--|Pooling| ---- |Conv|--|Relu|--|Conv|--|Relu|--|Pooling| ---- |Affine|--|Relu|--|Dropout| ---- |Affine|--|Dropout|--|Softmax|
		* このネットワークの特徴は以下の通りとなる。

			* 畳み込み層では全て3×3のフィルター、活性化関数はReLUを用いる。
			* 層が深くなるにつれてチャンネル数が大きくなる(チャンネルは16, 16, 32, 32, 64, 64と増えていく)。
			* 全結合層の後にDropoutレイヤを用いる。
			* 重みの初期値として「Heの初期値」を用いる。
			* パラメータの更新にはAdamを用いる。
		* このネットワークを使って学習すると、認識精度は99.38％にもなる。
		* 逆に誤認識率はわずか0.62％であるが、認識を誤った画像を確認してみると、人間にとっても判断が難しい画像がほとんどであることがわかる。
		* よって、今回のCNNは高精度でありながら、認識を誤った画像を見てみると、人間と同じような認識ミスを犯すこともわかる。
		* "Waht is the class of this image?"というWebサイトでは様々なデータセットを対象にこれまで発表されてきた手法の認識精度がランキング形式で掲載されている。
		* この結果を見ると、ランキングの上位を占める多くの手法はCNNをベースとした手法であることがわかる。
		* ただし、MNISTデータセットに関しては、層をそこまで深くしなくても高い精度が得られており、現在の最高認識精度は99.79%で畳み込み層が2層、全結合層が2層のネットワークである。
		* これは、手書き数字認識という比較的単純な問題に関しては、ネットワークの表現力をそこまで高める必要がないためと言える。
		* 一方、大規模な一般物体認識では問題が複雑になるため、ネットワークの層を深くすることによる認識精度の向上が大きく貢献すると言える。
		* 上述のランキング上位の手法を参考にすると、認識精度を高める手法として、アンサンブル学習や学習係数の減衰(learning rate decay)、Data Augmentation(データ拡張)などがあることがわかる。
		* Data Augmentationは入力する訓練画像をアルゴリズムによって人工的に加工することで認識精度の向上を図る有効な手法である。
		* 具体的には入力画像を回転したり、縦方向・横方向に微小な移動変化を加え、訓練画像データを増やすことを行う。
		* これはデータセットの枚数が少ない場合には有効な手段と言える。
		* さらに画像の中から一部を切り出すcrop処理、左右を反転させるflip処理等を行うこともある。
		* flip処理は左右の対称性を考慮する必要のない場合に有効なデータ拡張手法となる。
		* さらに輝度などの見た目の変化や拡大・縮小などのスケールに変化をつけることも有効な手法となる。
		* これらのData Augmentation手法により、訓練画像をうまく増やすことでディープラーニングの認識精度を向上させることができる。
		* 一方、層を深くすることの重要性に関して、理論的にはそれほど多くのことがわかっていない。
		* よって、理論的側面からの裏付けは現状乏しいが、これまでの経験や実験からその重要性について説明できることはいくつかある。
		* まずはじめに層を深くすることの重要性はILSVRCなどの大規模画像認識コンペティションの結果から分かる。
		* このようなコンペティションでは上位を占める手法の多くがディープラーニングの手法であり、さらにネットワークの層を深くする方向に向かっている。
		* また、層を深くすることの利点の1つとして、ネットワークのパラメータを少なくできることが挙げられる。
		* これは層を深くしたネットワークは層を深くしなかった場合に比べ、少ないパラメータで同レベルの表現力を達成できることを意味する。
		* 例えば、5×5のフィルターからなる畳み込み層を考える。
		* この場合、出力データの各ノードは入力データの5×5の領域から計算される。
		* 次に3×3の畳み込み演算を2回繰り返す場合を考える。
		* この場合、出力データの各ノードは中間データの3×3の領域から計算され、さらにその中間データは3×3の入力データの領域から計算される。
		* この2つの結果を見ると、3×3の畳み込み演算を2回繰り返して得られた出力データのノードと、5×5の畳み込み演算を行って得られた出力データのノードの見ている入力データは同じである。
		* つまり、5×5の畳み込み演算1回の領域は3×3の畳み込み演算を2回行うことでカバーできる。
		* またパラメータは前者が5×5=25個であるのに対し、後者が3×3×2=18個であり、パラメータ数はフィルターが小さく、層を2つ重ねた方が少なくなることがわかる。
		* このようなパラメータ数の差は層が深くなるにつれて大きくなる。
		* 例えば、3×3の畳み込み演算を3回繰り返す場合、パラメータ数は3×3×3=27個になるが、同じ領域を畳み込み演算1回で見るためには7×7の大きさのフィルターが必要となり、その場合のパラメータ数は7×7=49個となる。
		* 加えて、小さなフィルターを重ねてネットワークを深くすることの利点として、受容野(receptive field)を広くカバーできる点が挙げられる。
		* 受容野とはニューロンに変化を生じさせる局所的な空間領域のことで、層を重ねることで活性化関数が畳み込み層の間に挟まれることでネットワークの表現力がさらに向上する。
		* これは活性化関数によって、ネットワークに非線形の力が加わり、非線形の関数が重なることで複雑な表現が可能となる。
		* 層を深くすることで学習の効率性も向上する。
		* これは層を深くしなかった場合に比べて、学習データを少なくでき、高速に学習ができることを意味する。
		* 直感的にはCNNの可視化で前述したとおり、畳み込み層が階層的に入力データの情報を抽出していることからもわかる。
		* 前層の畳み込み層ではエッジなどの単純な形状にニューロンが反応し、層が深くなるにつれ、テクスチャや物体のパーツのような複雑なものに反応するようになることがわかっている。
		* ここで、「犬」を認識する問題を考える。
		* その場合、上述のようにネットワークが階層的に犬を認識しようとするが、そのネットワークが浅い場合、犬の特徴を1度に理解する必要がある。
		* また、犬は様々な種類があり、撮影される環境によっても見え方が大きく異なるため、その特徴を理解するためには多くのバリエーション豊富な学習データが必要となり、また学習に多くの時間が必要になる。
		* 一方、ネットワークを深くすることで、学習すべき問題を階層的に分解することが可能となる。
		* 各層では学習すべき問題がより単純な問題として取り組むことができることになる。
		* つまり例えば、最初の層はエッジの学習に専念すればよく、少ない学習データで効率よく学習を行うことが可能である。
		* これは犬が写っている画像に比べ、エッジを含む画像は多く存在し、そのエッジのパターンは犬のパターンよりも簡単な構造であるからである。
		* 層を深くすることで階層的に情報を後段の層に渡すことができるのも重要な点である。
		* 例えば、エッジを抽出した層の次の層ではエッジの情報を使うことができる。それにより、その層ではより高度なパターンを効率的に学習することができる。
		* つまり、層を深くすることで各層が学習すべき問題を解きやすいシンプルな問題へと分解することができると言える。
		* 以上がネットワークの層を深くすることの重要性となるが、これらは層を深くしても正しく学習できる技術(ビックデータやコンピュータパワーなど)によって、もたらされているとも言える。


	* ディープラーニングの歴史

		* ディープラーニングが大きな注目を集めるきっかけになったのは、2012年に開催されたILSVRC(大規模画像認識コンペティション)と言われている。
		* その年のコンペティションでディープラーニングの手法、AlexNetが圧倒的な手法で優勝し、これまでの画像認識に対するアプローチを覆したからである。
		* それ以降、画像認識のコンペティションでは常にディープラーニングが主役となっている。
		* コンペティションで用いられる画像のデータセットとしてImageNetがある。
		* ImageNetには様々な画像が含まれており、各画像にはラベル(クラス名)が紐づけられている。
		* ILSVRCのコンペティションにはテスト項目がいくつかあり、そのうちの1つにクラス分類(Classification)がある。
		* クラス分類のコンペティションでは1000クラスの分類を行い、その正確さを競うものになっている。
		* 上位5クラスまでに正解が入っていることを正しいとみなしてその誤認識率の推移を2010年からの優勝チームの成績を確認すると、2012年以降、ディープラーニングを用いた手法が常にトップに立っている。
		* 特に2012年のAlexNetが誤認識率を大幅に下げており、特に2015年のResNet(150層を超えるディープなネットワーク)では誤認識率が3.5％まで下げることができている。
		* この誤認識率は一般的な人間の認識能力を上回っていると言われている。
		* 以降ではここ数年でよい成績を残した有名なネットワークを見ていく。
		* まず、VGGは畳み込み層とプーリング層から構成される基本的なCNNである。
		* 重みのある層(畳み込み層や全結合層)は16層 or 19層まで重ねていることが特徴である。
		* VGGは3×3の小さなフィルターによる畳み込み層を連続して行っている点が特徴的である。
		* 畳み込み層を2～4回連続して重ね、プーリング層でサイズを半分にする、という処理を繰り返し、最後は全結合層を経由して結果を出力している。
		* VGGは2014年のコンペティションで2位になり、性能面では1位となったGoogleNetに及ばなかった。
		* ただVGGはとてもシンプルな構成で応用性が高く、VGGベースのネットワークは好んで用いられている。
		* 次にGoogLeNetはVGGやこれまでのネットワークに比べ、少し複雑な構成になっている。
		* 特徴として、ネットワークが縦の深さだけではなく、横方向の深さ(広がり)を持っている点が挙げられる。
		* この横方向の幅のことをインセプション構造と呼び、以下のような構造となる。

			* previous layer ------> 1×1 convolutrions -------> Filter concatenation
			*                          |                                      ↑
			*                          ---> 3×3 convolutrions -----
			*                          |                                      ↑
			*                          ---> 5×5 convolutrions -----
			*                          |                                      ↑
			*                          ---> 3×3 max pooling ------
		* インセプション構造はサイズの異なるフィルター(とpooling)を複数適用し、それぞれの層の結果を後段の層で結合する構造を持っている。
		* GoogLeNetはこの構造を1つの構成ブロックとして使用している。
		* また、GoogLeNetでは、サイズが1×1のフィルターの畳み込み層を多くの場所で使用し、この演算でチャンネル方向にサイズを減らしている。
		* これにより、パラメータの削減や処理の高速化を図っている。
		* 最後にResNetはMicrosoftによって開発されたネットワークである。
		* ResNetではこれまで以上に層を深くできるような仕掛けがされている。
		* これまで層を深くすることで性能の向上が図れることはわかっていたが、ディープラーニングの学習において層を深くし過ぎると、学習が上手くいかず、最終的な性能が劣ることが多くあった。
		* ResNetではそのような問題を解決するために「スキップ構造」を導入している。
		* これにより、層を深くすることに比例して性能を向上させることが確認できている。
		* スキップ構造とは以下のように入力データの畳み込み層をまたいで出力に合算する構造のことである。

			* ---------> weight layer --------> weight layer ---> + ----------->
			*    x   ↓                           relu                              ↑     relu
			*          ---------------------------------------------------
		* 上記では2層連続する畳み込み層において、入力xを2層先の出力にスキップしてつなげている。
		* これにより、本来の出力(=畳み込み層2層を通った出力)F(x)に対し、スキップ構造により、出力がF(x)+xとなる。
		* このようなスキップ構造は層を深くしても効率的な学習を行うことを可能にしている。
		* これは学習の差異の逆伝播時にスキップ構造によって、信号が減衰することなく伝わるからである。
		* スキップ構造は入力データをそのまま流すので、逆伝播時も上流から勾配をそのまま下流に流す。
		* これにより、勾配が小さくなりすぎたり、大きくなりすぎたりすることがなく、前層のレイヤに意味のある勾配が伝わることが期待できる。
		* これは層を深くすることで勾配が小さくなりすぎる勾配消失問題を軽減することができると言える。
		* ResNetはVGGのネットワークをベースとし、それに畳み込み層を2層おきにスキップ構造を取り入れ、層を深くしている。
		* 実験により150層以上にしても認識精度が向上し続けることが判明している。
		* ImageNetのような巨大なデータセットを使って学習した重みデータを有効活用することが実践的に良く行われている。
		* これは転移学習と呼ばれ、学習済みの重みを別のニューラルネットワークにコピーして使用することをいう。
		* また、そのコピーしたニューラルネットワークで再学習を行うことをfine tuningと呼ぶ。
		* fine tuningでは学習済みの重みを初期値とし、新しいデータセットを対象に再学習し、重みを更新することをいう。
		* 転移学習やfine tuningは手元にデータセットが少ない場合に有効な手法となる。


	* ディープラーニングの高速化

		* ビッグデータとネットワークの大規模化により、ディープラーニングでは大量の演算を行なう必要が出てきている。
		* これらの演算にはCPUを使ってきたが、多くのディープラーニングフレームワークはGPUをサポートしており、GPUを使うことで大量の演算処理を可能としている。
		* さらに複数GPU、複数台のマシンでの分散学習にも対応され始めている。
		* まず、ディープラーニングのどのような処理に時間が費やされているかを確認する。
		* AlexNetを用いた推論時のforward処理を対象に考えると、多くの時間が畳み込み層に費やされることがわかる。
		* GPUでは全体の95％、CPUでも全体の89%が畳み込み層での演算に費やされている。
		* つまり畳み込み演算をどれだけ高速に処理できるかがディープラーニングの課題と言える。
		* 学習時の結果も上記の推論時の結果と同様で畳み込み層での演算に多くの時間が費やされている。
		* 畳み込み層で行う演算は前述の通り、積和演算に帰着される。つまり、積和演算をどう高速処理するかを考える必要がある。
		* その解決の１つとして、GPUを使った高速化がある。
		* GPUはもともとグラフィックのための専用ボードとして利用されてきたが、最近では汎用的な数値計算にも使用されるようになっている。
		* GPUは並列的な数値計算を高速に行うことができるため、大量の積和演算を高速に行うことができる。
		* 一方、CPUは連続的で複雑な計算を得意としている。
		* よって、ディープラーニングの演算ではGPUを使うことにより、CPU単体の場合に比べて大きな高速化を図ることができる。
		* 例えば、AlexNetの学習に要する時間を比較すると、CPUでは40日以上も要する時間をGPUによって6日にまで短縮することができる。
		* また、cuDNNというディープラーニングに最適化されたライブラリを用いることでさらなる高速化を達成できることがわかっている。
		* ここで、GPUはNVIDIA社とAMD社の2社によって主に提供されている。
		* ディープラーニングで用いられるのはNVIDIA社のものである。
		* これはNVIDIAが提供しているCUDAという統合開発環境がディープラーニングのフレームワークで使用されているからである。
		* 前述のcuDNNはCUDA上で動作するライブラリでディープラーニングように最適化された関数が実装されている。
		* 高速化を図る別の手段として、分散学習が挙げられる。
		* GPUによる高速化は有効だが、それでもディープなネットワークになると、学習に数日・数週間といった時間がかかることがある。
		* ディープラーニングでは多くの試行錯誤を伴うため、よいネットワークを作るためには多くのことを試す必要がある。
		* よって、1回の学習に要する時間をできるだけ小さくしたいことになる。
		* そこで分散学習、ディープラーニングの学習のスケールアウトが重要になってくる。
		* つまり、複数のGPUや複数台のマシンで分散して計算を行うことを考える。
		* 現行のディープラーニングのフレームワークではこれらの分散学習をサポートしているものが現れ始めている。
		* TensorFlowによる分散学習の効果を確認してみると、使用するGPUが増えるにつれ、学習速度も向上することがわかる。
		* 具体的には100個のGPU(複数マシンにセットされた100個のGPU)での学習はGPUが1つの場合に比べ、56倍程度の高速化が可能となっている。
		* これは例えば、7日かかっていた学習がわずか3時間で完了することを意味する。
		* ただ、どのように計算を分散させるかはマシン間での通信やデータの同期処理などの問題もあり、難しい問題である。
		* また演算精度のビット削減もディープラーニングの高速化に繋がる。
		* これは、計算量に加え、メモリ容量やバス帯域などが高速化におけるボトルネックになり得るためである。
		* つまり大量の重みやパラメータや中間データをメモリに収める必要があり、またGPU・CPUのバスを流れるデータの増加を抑える必要がある。
		* これらを踏まえると、ネットワークを流れるデータのビット数をできるだけ小さくできることが望まれるが、通常コンピュータでは64ビット/32ビットの浮動小数点数が使われている。
		* これにより、コンピュータは多くのビットを使って数を表現できるため、数値演算時の誤差の影響を抑えることができるが、その分計算処理コストやメモリ使用量が増大してバス帯域に負荷をかけてしまう。
		* ディープラーニングで必要となる数値精度(何ビットで数値を表現するか？)はそこまで高い必要はない。
		* これはニューラルネットワークの重要な性質であり、ニューラルネットワークのロバスト性によるものである。
		* つまり、ニューラルネットワークは入力画像に小さなノイズがのっていても出力結果は変わらない。
		* 同様にネットワークに流れるデータを劣化させても出力結果に与える影響は少ないと考えられる。
		* ディープラーニングではこれまでの実験により、16ビットの半精度浮動小数点数(half float)でも問題なく学習ができることがわかっている。
		* 実際にNVIDIAのPascalアーキテクチャでは半精度浮動小数点数の演算がサポートされており、半精度浮動小数点数が今後、標準的に用いられることが考えれる。
		* これまでの実装では数値精度を考えてこなかったが、Pythonでは一般的に64ビットの浮動小数点数が使われる。
		* 一方、NumPyには16ビットの半精度浮動小数点数の型が用意されているが、演算自体は16ビットでは行われない。
		* また、ディープラーニングのビット数を削減する研究は他にも行われており、最近では重みや中間データを1ビットで表現する「Binarized Neural Networks」という手法が提案されている。
		* ビット数削減は特に組み込み向けでディープラーニングを利用する際の重要なテーマになっている。


	* ディープラーニングの実用例

		* これまでは手書き数字認識のような画像のクラス分類(物体認識)を中心に見てきたが、ディープラーニングは物体認識だけではなく、様々な問題に適用でき、いずれも優れた性能を発揮している。
		* 物体検出

			* 画像中から物体の位置の特定を含めてクラス分類を行なう問題のこと。
			* 物体検出は物体認識よりも難しく、画像全体だけでなく、画像の一部からクラスの位置を特定する必要がある。
			* さらに1つの画像に複数の物体が存在する可能性もある。
			* 物体検出に対してはCNnベースの手法がいくつか提案されている。
			* その中でも有効なR-CNNと呼ばれる手法が知られている。
			* R-CNNでは始めにオブジェクトらしい領域を探し出し、その領域に対してCNNを適用してクラス分類を行なう。
			* さらに画像を正方形に変形したり、クラス分類の際にSVMを使ったりしており、全体の処理フローはやや複雑。
			* オブジェクト候補領域抽出処理ではコンピュータビジョンにおける様々な手法を用いている。
			* R-CNNの論文では、Selective Searchと呼ばれる手法が使われているが、最近ではこの候補領域抽出処理をFaster R-CNNと呼ばれるCNNをベースとした手法で行う方法も提案されている。
			* Faster R-CNNでは全ての処理を1つのCNNで行うので、高速な処理が可能となる。
		* セグメンテーション

			* 画像に対してピクセルレベルでクラス分類を行なう問題のこと。
			* ピクセル単位でオブジェクトごとに色付けされた教師データを使って学習する。
			* また、推論時は入力画像の全てのピクセルに対してクラス分類を行なう。
			* これまで実装したニューラルネットワークは画像全体に対してのクラス分類だったのに対し、セグメンテーションではピクセルレベルで分類できる必要がある。
			* その場合、全てのピクセルを対象地してピクセルごとに推論を行なうのが最も単純な方法である。
			* 例えば、ある矩形領域の中心のピクセルに対してクラス分類するネットワークを用意し、それをすべてのピクセルを対象に推論処理を実行することが考えられる。
			* ただこの方法だとピクセルの数だけforward処理をすることになり、多くの領域の畳み込み演算を再計算することになり、無駄な計算が発生してしまう。
			* このような無駄な計算を改善する方法としてFCN(Full Convolutional Network)という手法が提案されている。
			* FCNでは1回のforward処理で全てのピクセルに対してクラス分類を行なう。
			* 一般的なCNNが全結合層を含むのに対し、FCNでは全結合層を同じ働きをする畳み込み層に置き換える。
			* 物体認識で用いるネットワークの全結合層では、中間データの空間ボリュームを1列に並んだノードとして処理していたが、畳み込み層だけから構成されるネットワークでは、空間ボリュームを保ったまま最後の出力まで処理される。
			* また、FCNでは最後に空間サイズを拡大する処理を導入している。
			* これにより、小さくなった中間データを入力サイズと同じ大きさまで拡大することができる。
			* 加えてFCNの最後で行う拡大処理は、バイリニア補完による拡大を行っている。このバイリニア拡大をデコンボリューション(逆畳み込み演算)によって実現している。
		* 画像キャプション生成

			* 画像を与えると、その画像を説明する文章(画像キャプション)を自動生成すること。
			* ディープラーニングによる画像キャプションを生成する代表的な方法にNIC(Neural Image Caption)というモデルがある。
			* NICはディープなCNNと自然言語を扱うためのRNN(Recurrent Neural Netowork)から構成される。
			* RNNは再帰的なつながりを持つネットワークであり、自然言語や時系列データなどの連続性のあるデータに対して用いられている。
			* NICでは画像からCNNによって、特徴を抽出し、その特徴をRNNに渡している。
			* RNNはCNNが抽出した特徴を初期値として、テキストを再帰的に生成する。
			* つまり、NICはCNNとRNNという2つのネットワークを組み合わせたシンプルな構成をしている。
			* NICのように画像と自然言語といったような複数の種類の情報を組み合わせて処理することをマルチモーダルな処理と呼び、最近注目を集めている。
			* RNNのRはRecurrent(再帰的な)を表し、ニューラルネットワークの再帰的なネットワーク構造を指す。
			* この再帰的な構造は以前に生成した情報から影響を受ける、つまり過去の情報を記録する点がRNNの特徴である。
			* 例えば、「私」という言葉を生成した後に次に生成する言葉は、「私」という言葉を受けて「は」を生成する。
			* 次に「私は」というこれまでに生成した言葉を受けて、「寝る」という言葉を生成する、ということが行われる。
			* 自然言語や時系列データなどの連続性のあるデータに対して、RNNは過去の情報を記憶するように動作する。
		* 画像スタイル変換

			* 2つの画像を入力し、新しい画像を生成する。
			* 画像のうちの1つはコンテンツ画像、もう1つはスタイル画像として、その2枚を入力することで新しい画像を生成する。
			* これにより、コンテンツ画像にスタイル画像のスタイルを適用し、新しい絵画として出力される。
			* この手法は「A Neural Algorithm of Artistic Style」として発表され、注目を集めた。
			* この手法では、まずネットワークの中間データがコンテンツ画像の中間データに近づくように学習する。
			* これにより、入力画像をコンテンツ画像の形状に似せることができる。
			* 次にスタイル画像からスタイルを吸収するためにスタイル行列という概念を導入し、スタイル行列のずれを小さくするように学習することで、入力画像をスタイル画像のスタイルに近づけることができる。
		* 画像生成

			* 何の画像も使用せず、新しい画像を描き出す研究。
			* 過去にはベッドルームの画像をゼロから生成することを実現する手法DCGAN(Deep Convolutional Generative Adversarial Network)が提案されている。
			* DCGANが描く画像はまだ誰も見たことがなく、学習データに存在しない画像である。
			* DCGANは画像が生成される過程をモデル化し、そのモデルを大量の画像を使って学習を行なう。
			* 学習後はそのモデルを使い、新しい画像を生成することができる。
			* DCGANではディープラーニングが使われており、さらにGenerator(生成する人)とDiscriminator(識別する人)と呼ばれる2つのニューラルネットワークを使用している。

			* Generatorが本物に似た画像を生成し、Discriminatorがそれが本物かどうかを判定する役割を担う。
			* 両者を競わせるように学習することで、Generatorはより成功で本物に近い画像を生成し、Discriminatorは偽物の高い精度で見破ることができるようになる。
			* このように両者を切磋琢磨して競わせ、ネットワークを成長させていく方法をGAN(Generative Adversarial Network)と呼ぶ。
		* 自動運転

			* 人間の代わりにコンピュータが自動車を運転する技術は自動車メーカーだけでなく、IT企業や大学なども含め、実現に向けてしのぎを削っている。
			* 自動運転には様々な技術を組み合わせて実現されるが、その中でも周囲の環境を正しく認識する技術が特に重要だと言われている。
			* これは時々刻々と変わる環境を正しく認識することが非常に難しい問題だからだと言える。
			* 最近ではその周囲の環境を認識する技術にディープラーニングを用いられるようになっている。
			* 例えば、SegNetと呼ばれるCNNベースのネットワークは高精度に走路環境を認識することができる。
		* Deep Q-Network(強化学習)

			* 人が試行錯誤して学ぶようにコンピュータにも試行錯誤の家庭から自律的に学習させようとする分野があり、これを強化学習(reinforcement learning)と呼ぶ。
			* 強化学習はエージェントが環境の状況に応じて行動を選択し、その行動によって環境が変化する、そしてエージェントは環境の変化によって、何らかの報酬を得る、というのが基本的な仕組みである。
			* 強化学習の目的はより良い報酬が得られるようにエージェントの行動指針を決める点にある。
			* ここでの注意点は報酬は決められたものではなく、見込みの報酬である、という点である。
			* エージェントを右に動かすことでどれだけの報酬を得るかはその時点では明確ではない。
			* ゲームのスコア(得点を取る、敵を倒す、など)やゲームオーバーなどの明確な指標から逆算して見込みの報酬を決める。
			* ディープラーニングを使った強化学習の手法として、Deep Q-Learning(DQN)という手法がある。
			* これはQ学習と呼ばれる強化学習アルゴリズムをベースにしている。
			* Q学習では最適な行動を決定するために最適行動価値関数と呼ばれる関数を決定する。
			* その関数を近似するためにCNNを用いるのがDGNである。
			* DQNの研究ではテレビゲームを自動的に学習させ、人を超えるレベルの操作を実現した例が報告されている。
			* DQNで使われるCNNはゲーム画像のフレーム(4枚の連続したフレーム)を入力として、最終的にはゲームのコントローラの動き(ジョイスティックの移動量やボタン操作の有無)に対して、その動作の価値を出力する。
			* これまでテレビゲームを学習する場合、ゲームの状態(キャラクターの場所など)をあらかじめ抜き出して与えることが一般的であった。
			* ただDQNでは、入力データはテレビゲームの画像のみだけであり、DQNの応用性を大きく高めていると言える。
			* これにより、ゲームごとに設定を変える必要はなく、単にゲームの画像を与えれば学習ができるからである。
			* 実際、別の多くのゲームに関しても同じ構成で学習することができ、多くのゲームで人間を上回る成績を残すことができている。
			* AlphaGoという人工知能が囲碁のチャンピオンを破ったが、AlphaGoもディープラーニングと強化学習が用いられている。
			* AlphaGoは3000万のプロの棋譜を与えて学習させ、さらにAlphaGo自身が自分自身と対戦することを何度も繰り返しながら学習を行っている。

★P.73：sys.path.append()★P.73：os.pardir★P.74：PIL.Image.fromarray().show()★P.76：pickle.load(f)★P.76：np.argmax(y)★P.80：range(start, end, step)　startからend-1までの値をstepの値の間隔で値を生成★P.93：np.random.choice()★P.104, P.171：np.zeros_like(x)★P.110：np.random.randn()★P.110：np.dot()★P.159：t.ndim != 1★P.161：grad_numerical.keys()★P.171：self.v is None★P.171：params.items()★P.180：a.flatten()★P.196：np.random.rand(*x.shape) > self.dropout_ratio★P.201：np.random.uniform(-6, -2)★P.226：out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)★P.228：out = np.max(col, axis=1)★P.231：np.zeros(filter_num)
