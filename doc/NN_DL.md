## ニューラルネットワークと深層学習
https://nnadl-ja.github.io/nnadl_site_ja/

### 第1章：ニューラルネットワークを用いた手書き文字認識
* 多くの人にとって、手書き数列は簡単に読める。
* 脳内では複雑なことが起きているが、脳は数億年の人間の進化によって洗練され、驚くべき適応を遂げたスーパーコンピュータである。
* 手書き数字の認識が簡単なのではなく、人間は目に見えるものを解釈する作業を無意識に行い、またその認識作業をとても得意としている。
* 手書き数字認識プログラムを書くことは非常に困難である。
* 「数字の9は上に輪があって、右下から下に向かって線が生えている」といったものをアルゴリズムで表現するのは単純にはできない。
* このようなルールを正確にプログラムにしようとすると、膨大な例外や特殊ケースの考慮が必要となる。
* ニューラルネットワークはこの問題に違った角度から取り組む。
* ニューラルネットワークの発想は手書き数字データのたくさん用意し、それらから学習することのできるシステムを開発する、という考え方である。
* つまりニューラルネットワークではこれらのデータをもとに数字認識のルールを自動的に推論する。
* データを増やせば、ネットワークは手書き文字に関する知識をより多く獲得し、精度が向上する。
* 手書き文字認識はニューラルネットワークについて解説する上でよい題材である。
* 一筋縄ではいかず、莫大な計算資源が必要である一方、きわめて複雑な解法は不要で、非常に困難な課題というわけではない。
* 手書き文字認識を実装することでニューラルネットワークのカギとなるアイデアをいくつも開発できる。
* それらのアイデアには2種類の人工ニューロン(パーセプトロンやシグモイドニューロン)やニューラルネットワークの標準的な学習アルゴリズムである確率的勾配降下法などが含まれる。

##### パーセプトロン
* 1950～1960年代にWarren McCullochとWalter Pittsらの先行研究に触発されたFrank Rosenblattによって開発された。
* 現在ではパーセプトロン以外の人工ニューロン、特にニューラルネットワークの研究ではシグモイドニューロンを使うことが一般的。
* パーセプトロンは複数の二進数$x_{1}$, $x_{2}$, ...を入力、1つの二進数を出力する。
* Rosenblattは出力を計算するために重み$w_{1}$, $w_{2}$, ...という概念を導入。
* 重みとは各入力が出力に及ぼす影響の大きさを表す数値のこと。
* パーセプトロンの出力が0になるか1になるかは入力の重み付き和 $\sum_{j}^{}w_{j}x_{j}$ と閾値(threshold)の大小比較で決まる。
* 閾値(threshold)も同様、パーセプトロンの挙動を決める数値パラメータである。
* パーセプトロンを数値で表現すると以下の通り。
$$
output=\left \{\begin{array}{l}
0　(if \sum_{j}^{}w_{j}x_{j} \leq threshold) \\
1　(if \sum_{j}^{}w_{j}x_{j} > threshold)
\end{array}
\right. \tag{1}
$$
* 直感的にはパーセプトロンは複数の情報に重みをつけながら出力の決定をくだす機械である。
* ある判断事項に対して、その判断に影響を及ぼすfactorが3つあるとすると、入力は二進数値$x_{1}$, $x_{2}$, $x_{3}$ で表現できる。
* ここで、$x_{1}$ が判断に対して、最も強い影響を及ぼし、$x_{2}$, $x_{3}$ は判断にそれほど大きな影響を与えないものとする。
* パーセプトロンはこのような意思決定を表現することが可能。
* 例えば、各入力の重みを $w_{1}$=6, $w_{2}$=2, $w_{3}$=2、パーセプトロンの閾値(threshold)を5に設定する。
* この場合、$x_{1}$=1なら、パーセプトロンは必ず1を出力し、$x_{1}$=0なら、パーセプトロンは必ず0を出力する。
* つまりこの設定の場合、$x_{2}$, $x_{3}$ はパーセプトロンの出力に影響を与えない。
* また、$w_{1}$ の値が他の重みに比べて大きいのは、判断事項に対して$x_{1}$ が与える影響が大きく重要であることを表している。
* 重みと閾値(threshold)を変化させることで、異なる意思決定モデルを表現することも可能。
* 例えば、重みはそのままで閾値(threshold)を3に変更する。
* この場合、パーセプトロンが1を出力するのは、以下の2パターンとなる。
    * $x_{1}$=1の場合
    * $x_{2}$=1かつ$x_{3}$=1の場合
* 閾値(threshold)を小さくすることはその判断事項が正になりやすくなる(その判断をしたくなっている)こと、閾値(threshold)を大きくすることはその判断事項が負になりやすくなる(その判断をしたくなくなっている)ことを表している。
* パーセプトロンは人間の意思決定モデルを完全に表現できるわけではないが、これらを複雑に組み合わせることで微妙な判断も扱うことが可能となる。
* パーセプトロンで構成され、3つの層を持つネットワークを考える。また各層にはそれぞれ3, 4, 1のパーセプトロンを持っているとする。
* 第1層の3つのパーセプトロンは入力情報に重みをつけ、とても単純な判断を行う。
* 第2層の4つのパーセプトロンは第1層のパーセプトロンの下した判断に重みをつけることで判断を行う。一般に第1層よりも複雑で抽象的な判断を行なう。
* 第3層の1つのパーセプトロンはさらに複雑な判断を行う。
* このような多層のニューラルネットワークを構築することで高度な意思決定が可能となる。
* 1つのパーセプトロンは上述の通り、出力は1つだが、多層ニューラルネットワークにおいて、その出力は次の層の複数のパーセプトロンの入力として使われる可能性がある。
* 式(1)をより簡潔に表記し直すと以下の通り。
$$
output=\left \{\begin{array}{l}
0　(if \ \boldsymbol{w}・\boldsymbol{x}+\boldsymbol{b} \leq \boldsymbol{0}) \\
1　(if \ \boldsymbol{w}・\boldsymbol{x}+\boldsymbol{b} > \boldsymbol{0})
\end{array}
\right. \tag{2}
$$
* 入力の重み付き和は内積を用いて、$\boldsymbol{w}・\boldsymbol{x} \equiv \sum_{j}^{}w_{j}x_{j}$ と表記する。
* $\boldsymbol{w}$, $\boldsymbol{x}$ はそれぞれ重みと入力を要素に持つベクトルとする。
* 閾値(threshold)を左辺に移動し、バイアス $\boldsymbol{b} \equiv -threshold$ と表記する。
* バイアスはパーセプトロンが1を出力する傾向の高さを表す数値、もしくはパーセプトロンというニューロンが発火する傾向の高さを表す数値と言える。
    * バイアス値が小さいほど、パーセプトロンが1を出力するのは難しく、バイアス値が大きいほど、パーセプトロンが1を出力しやすくなる。
    * バイアス値は閾値(threshold)の値の正負が逆の値を持っているため、大きさの大小によるパーセプトロンの挙動の傾向も逆になる。
* 閾値(threshold)よりもバイアスを使って式を表記したほうがシンプルな表現になるため、多くの場合、パーセプトロンの挙動はバイアスを使って表現される。
* ここまで入力された情報に重みをつけ、それらをパーセプトロンを使って何かしらの判断を行う手続きを考えてきたが、他の用途にも使用することができる。
* 例えば、パーセプトロンを使って、AND, OR, NANDといった論理関数を計算することが可能。
    * ※詳細は省略
* パーセプトロンが様々なものに応用できるということは、ニューラルネットワークの重みとバイアスを自動的に最適化するような学習アルゴリズムを開発できる、ということも意味する。
* 「ニューラルネットワークの重みとバイアスの最適化」はプログラマが直接介入することなく、勝手に行われる。

##### シグモイドニューロン
* ニューラルネットワークに対して学習アルゴリズムを設計するにはどうすればよいか？
* 例えば、ある種の問題をパーセプトロンで構成されたネットワークを使って解こうとしているとする。
    * 入力は手書き文字の画像ピクセルデータ
    * ニューラルネットワークで数字を正しく分類するための重みとバイアスを学習する。
* 学習がどのように働くかを知るためには、いくつかの重みやバイアスを少しだけ変更するとする。
* このような小さな変更によって変化するニューラルネットワークからの出力の変化も小さいものであることが望ましい。
    * この性質が「学習」を可能にする。
    * これはニューラルネットワークがより自分の思った通りの挙動を示すように重みとバイアスを修正できることを意味する。
* あるニューラルネットワークが「9」であるべき手書き文字を「8」に分類しているとする。
* 「小さな変更によって変化するニューラルネットワークからの出力の変化も小さいものである」という性質があれば、正しく「9」を分類するために重みとバイアスに小さな変化を与え、正しく分類できる方向に近づくかを探ることができる。
* この過程を繰り返し、重みとバイアスを変化させ続けることで生成される結果を改善することができる。
* ただパーセプトロンで構成したニューラルネットワークでこのような学習はできない。
    * つまり「小さな変更によって変化するニューラルネットワークからの出力の変化も小さいものである」という性質を持っていない。
    * パーセプトロンで構成されたニューラルネットワークではどれから1つの重みやバイアスを少し変更すると、そのパーセプトロンの出力は全く変化しないか、0から1、もしくは1から0へなど反転し、大きく変化してしまう。
    * ネットワーク内のパーセプトロンの出力が反転してしまうと、ネットワーク内の他の部分の挙動も連動して複雑に変化してしまう。
    * 「9」を正しく分類できないニューラルネットワークの重みやバイアスを変化させ、正しく分類できるようになったとしても、他の数字の分類に関する挙動も大きく変わってしまっている可能性がある。
    * 現時点でパーセプトロンで構成されたニューラルネットワークを上手に学習する方法は明らかになっていない。
* このパーセプトロンの問題はシグモイドニューロンと呼ばれている別の人工ニューロンを使用することで解決できる。
* シグモイドニューロンは重みやバイアスに微小な変化を与えると、出力の変化も微小な変化になる性質を持っている。
* つまり、シグモイドニューロンで構成されたニューラルネットワークは最適な重みやバイアスの学習を可能にする性質を持っていると言える。
* パーセプトロンと同様、シグモイドニューロンも複数の入力$x_{1}$, $x_{2}$,...を取るが、これらの値は$0$ or $1$の二進数ではなく、0から1の間のあらゆる値を取ることができる。
* またシグモイドニューロンはパーセプトロンと同様、重み$w_{1}$, $w_{2}$, ...を持ち、バイアス$b$ も持つ。
* シグモイドニューロンの出力は、$0$ or $1$ の二進数ではなく、$\sigma(w・x+b)$　という値を取る。
* $\sigma$ はシグモイド関数(ロジスティック関数)と呼ばれ、以下の式で定義される。
 $$
\sigma(z) \equiv \frac{1}{1+\mathrm{e}^{-z}} \tag{3}
 $$
* シグモイドニューロンの出力を入力$x_{1}$, $x_{2}$,...、重み$w_{1}$, $w_{2}$, ...、バイアス$b$ を使って表記すると、以下のように表現できる。
$$
\sigma(z) \equiv \frac{1}{1+\mathrm{e}^{-z}} = \frac{1}{1+\exp({-\sum_{j}^{}w_{j}x_{j}}-b)} \tag{4}
$$
* シグモイドニューロンとパーセプトロンは式から見ると、大きく異なるように見えるが、多くの共通点がある。
* $z \equiv w・x+b$ を大きな正の数と考えると、$\mathrm{e}^{-z} \approx 0$ となり、$\sigma(z) \approx 1$となる。
* 逆に$z \equiv w・x+b$ を大きな負の数と考えると、$\mathrm{e}^{-z} \rightarrow \infty$ となり、$\sigma(z) \approx 0$となる。
* つまり、シグモイドニューロンへの重み付き入力和+バイアスが大きいとき、小さいときのシグモイドニューロンの出力はそれぞれ、1, 0となり、パーセプトロンの挙動と同じになる。
* 一方、重み付き入力和+バイアスがそれほど大きくなく、また小さくもない場合は、パーセプトロンとは同じ挙動にはならないといえる。
* シグモイド関数 $\sigma$ はステップ関数のなめらか版である。
* シグモイド関数が滑らかであることは重みとバイアスの微小変化 $\Delta w_{j}$ 、 $\Delta b$ はニューロンの出力の微小変化 $\Delta output$ を生み出すことを意味している。
* 式で表すと以下の通りとなる。
$$
\Delta output \approx \sum_{j}^{} \frac{\partial output}{\partial w_{j}} \Delta w_{j} + \frac{\partial output}{\partial b} \Delta b \tag{5}
$$
* この式は同時に出力の微小変化 $\Delta output$ が重みとバイアスの微小変化 $\Delta w_{j}$ 、 $\Delta b$ に対して線形であることを表している。
* さらに言えば、重みとバイアスを変化させることでどんな小さなoutputの微小変化も得ることができることを意味する。
* シグモイドニューロンの出力をどう扱い、変換するべきか？
* パーセプトロンの出力が0 or 1なのに対し、シグモイドニューロンは0～1のあらゆる実数が出力される可能性がある。
* シグモイドニューロンの出力が0～1のあらゆる実数をとる、ということは例えば、ニューラルネットワークの入力画像のピクセル平均色度合いを出力値として表したい場合などに有用となる。
* 一方、「入力画像が9」 or 「入力画像が9ではない」を示したい場合のように厄介なものにもなりえる。
* しかしこのような場合、例えば、出力が0.5より大きい場合は「入力画像が9」とみなし、出力が0.5より小さい場合は、「入力画像が9ではない」とみなすようなルールを設定することで解決できる。

##### ニューラルネットワークのアーキテクチャ
* ニューラルネットワークのそれぞれの部分に名前を付ける。
* 1番左の層は入力層(input layer)と呼び、1番右の層を出力層(output layer)と呼ぶ。
* ニューラルネットワークの中央の層で入力でも出力でもない層を隠れ層(hidden layer)と呼び、ネットワークによっては複数の隠れ層を持つことも可能。
* ニューラルネットワークの入出力層の設計は単純なことが多い。
    * 手書き画像が9かどうかを判断したいとする。
    * その画像が64×64=4096の白黒画像ならば、入力層のニューロンの数は4096となる。
    * 画像のピクセルあたりの色の度合いの明度を0～1の値で表し、入力ニューロンにエンコードする。
    * 出力層は1つのニューロンからなり、出力値が0.5以上なら、入力画像が9であり、0.5より小さければ、入力画像が9ではないと判断する。
* 隠れ層の設計は入出力層の設計に比べ難しく、単純で大まかなプロセスのみで設計することは不可能。
* ニューラルネットワークの研究者はヒューリスティクス(発見的、経験則の、試行錯誤的な)な設計方法を開発してきている。
* ある層の出力が次の層の入力になるようなニューラルネットワークをフィードフォワードニューラルネットワーク(feedforward neural networks)と呼ぶ。
* フィードフォワードニューラルネットワークはネットワーク内にループがなく、情報は常に前に伝わり、後ろに戻らないことを意味する。
* フィードバックループを用いることが可能な人工ニューラルネットワークモデルも存在し、再帰型ニューラルネットワーク(recurrent neural networks)と呼ぶ。
* 再帰型ニューラルネットワークはフィードフォワードニューラルネットワークに比べ、学習アルゴリズムが非力だったこともあり、あまり影響力がなかった。
* ただ再帰型ニューラルネットワークは人間の脳の働きに近く、フィードフォワードニューラルネットワークで解くには難しい問題を解くことができる可能性を秘めている。

##### 手書き数字を分類する単純なネットワーク
* 手書き数字認識問題は2つの下位問題に分割できる。その1つは複数桁の数字からなる画像をそれぞれの数字の画像列にすることである。
* つまり「504192」という1つの手書き数字の数字列画像を「5」、「0」、「4」、「1」、「9」、「2」という6つの画像にすることである。
* 人間がこの分割問題を解くことは簡単だが、プログラムが正確に画像を分割するのは難しい。
* 画像が分割されてしまえば、あとは個々の数字を認識するだけになるので、各数字の分類問題を解くことを考える(手書き数字認識問題から分割されるもう1つの問題)
* 各数字の分類問題を解く良い方法が見つかれば、画像分割問題もそれほど難しくなくなるはず。
* 手書き数字認識問題のために3層のニューラルネットワークを考える。
    * 入力層はピクセル値をエンコードするニューロンを持つ必要がある。手書き数字画像として28×28ピクセルの画像を扱うとすると、入力層のニューロンは28×28=784個となる。
    * 隠れ層はニューロンの数を $n$ とし、 $n$ の値を変えて実験する。最初は $n=15$ とする。
    * 出力層は10個のニューロンで構成する。最初のニューロンが発火(出力値 $\approx 1$ )したら、ネットワークがその入力画像を0だと認識し、2番目のニューロンが発火した場合は、ネットワークがその入力画像を1だと認識したことを表す。
    * 出力ニューロンのうち、どのニューロンが最も高く活性化するかで入力画像がどの手書き数字であるかを推測できる。
* 出力ニューロンを認識する数字の数(=10)だけ用意するのはなぜか？
    * 出力ニューロンとして4個のニューロンを準備し、それぞれのニューロンの出力(0 or 1)を組み合わせれば、10の手書き数字の判別は表現できる。
    * $2^4=16$ なので、10の分類を行うためには4つのニューロンで十分。
* 出力ニューロンを4個ではなく、10個用意する方がよい、ということは経験に基づく結果である。
* 経験的に出力ニューロンを10個にした方が、うまく学習できることが経験則的にわかっている。
* なぜその方がよいのかはニューラルネットワークが何をしているかという原理から考える必要がある。
* 10個のニューロンを用意する場合の最初の出力ニューロンは入力画像が0かどうかを決めようとしている。
* 0を決める出力ニューロンはその前段の隠れ層のニューロンからの情報を使って0かどうかを判断する。
* この場合の隠れ層のニューロンはどのような働きをしているか？
* 0を決める出力ニューロンに入力している隠れ層の各ニューロンは、0の画像の断片が自身の入力に存在するかを判定・検出している。
* 検出はその断片画像と入力画像の重なったピクセルに重く重み付けし、他の入力には軽い重み付けを行うことで実現している。
* 複数の隠れ層ニューロンの出力が0を決める出力ニューロンに入力している場合、隠れ層ニューロンが検出する画像の断片を合わせると、0の画像になる。
* つまり、0を決める出力ニューロンに入力している複数の隠れ層ニューロンが全て発火した場合、その時の入力画像は0と言える。
* 上述の方法で手書き文字の認識ができる場合、出力ニューロンが4個であるよりも、10個の方がよいことがわかる。
    * もし出力ニューロンが4個の場合、最初の出力ニューロンは数字を判断する数字列の最上位ビットを決める役割を果たすことになる。
    * しかし、上述の通り、最上位ビットを画像のような形状に関連付ける方法はなく、うまく学習できない。

##### 勾配降下法を用いた学習
* ニューラルネットワークはどのようにして手書き数字を認識する方法を学習するのか？
* まず学習するためのトレーニングデータセットが必要になる。
* ここでは数万件の手書き数字スキャン画像とその正しい分類からなるMNISTデータセットを使用する。
* MNISTの名称はアメリカ国立標準技術研究所(NIST)によって、収集及び修正(Modify)されたデータセットから成り立っていることに由来する。
* MNISTは2つの要素から成り立っている。
    * 1つは60,000個の訓練用画像データ。250人の手書き標本からスキャンされたもので、各画像は28×28ピクセルのグレースケールになっている。
    * もう1つは10,000個のテスト用画像データ。これらの画像も28×28ピクセルのグレースケールになっている。
    * テスト用画像データはニューラルネットワークが手書き数字の認識についてどれくらい学習できているかを評価するために用いられる。
    * テスト用画像データはテストの精度を上げるために訓練用画像データとは別の人から採取されている。
    * テスト用画像データと訓練用画像データを分けることで、システムが認識できる数字を訓練時に使用していないことを保証できる。
* ここで訓練入力を $\boldsymbol{x}$ と定義する。
* $\boldsymbol{x}$ はベクトルであり、MNISTの訓練画像データを入力画像とする場合、28×28=784次元ベクトルとなる。
* ベクトル $\boldsymbol{x}$ の各成分はピクセルごとの濃淡値を表している。
* 次に出力を $\boldsymbol{y} = \boldsymbol{y}(\boldsymbol{x})$ と定義し、 $\boldsymbol{y}$ を10次元ベクトルとする。
* 例えば、訓練用画像の $\boldsymbol{x}$ が6を示している場合、 $\boldsymbol{y}(\boldsymbol{x})=(0,0,0,0,0,0,1,0,0,0)^T$と表現される( $T$ は転置演算子)。
* よりよい手書き数字の認識ニューラルネットワークを得るためには、全訓練入力 $\boldsymbol{x}$ について、ネットワークの出力が $\boldsymbol{y}(\boldsymbol{x})$ になるべく近くなるような重みとバイアスを見つけるためのアルゴリズムを得る必要がある。
* このアルゴリズムを得ることに対し、どれだけそれを達成できたかを測るためのコスト関数(損失関数・目的関数)を以下の通り定義する。
$$
C(\boldsymbol{w},\boldsymbol{b}) \equiv \frac{1}{2n} \sum_{x}^{} || \boldsymbol{y}(\boldsymbol{x}) - \boldsymbol{a} ||^2 \tag{6}
$$
* $\boldsymbol{w}$ はネットワーク中の全ての重み、 $\boldsymbol{b}$ は全ニューロンのバイアス、 $n$ は訓練入力総数、 $a$ は入力が $x$ の時ににネットワークから実際に出力されるベクトル、 $||\boldsymbol{v}||$ はベクトル $\boldsymbol{v}$ の距離関数をそれぞれ表わす。
* $C$ は2次コスト関数と呼ばれ、平均二乗誤差もしくはMSE(mean squared error)としても知られている。
* 定義したコスト関数 $C(\boldsymbol{w},\boldsymbol{b})$ は総和の全項目が非負となるため、非負となる。
* また、 $C(\boldsymbol{w},\boldsymbol{b})$ が小さい(つまり $C(\boldsymbol{w},\boldsymbol{b}) \approx 0$ )場合、 $\boldsymbol{y}(\boldsymbol{x})$ と実際の出力 $\boldsymbol{a}$ がほぼ等しくなるため、これを満たす重みとバイアスを見つけられれば、よりよい手書き数字認識のニューラルネットワークを得ることができたと言える。
* 従って、 $C(\boldsymbol{w},\boldsymbol{b})$ を最小化する重みとバイアスのペアを見つけることがよりよい認識ニューラルネットワークを得ることと等価となる。
* コスト関数を最小化する重みとバイアスのペアを勾配降下法というアルゴリズムを使って見つける。
* ここでなぜ分類の正解数を最大化せずに2次コストを最小化することを目指すのか？
    * 分類の正解数がネットワーク中の重みとバイアスの滑らかな関数にならないため。
    * つまり、重みとバイアスに小さな変更を加えても正解数が変化することがほとんどないため、分類の正解数を最大化するために重みとバイアスをどう変更させたら良いか分からない。
* 上述の2次コスト関数のような滑らかな関数を用い、重みとバイアスに小さな変更を加えることでコストが変化し、どう変更を加えれば、コストが改善されるかが容易にわかる。
* ではなぜ上述の2次コスト関数を用いる必要があるのか？仮に異なるコスト関数を選べば、コストを最小化する重みとバイアスのペアは全く別のものになるのではないか？
    * 上述のコスト関数はニューラルネットワークを学ぶにあたり、学習の基礎を理解するにあたって最適であるため。コスト関数はほかにもある。
* ここからは勾配降下法を理解するために一旦、ニューラルネットワークについては忘れ、シンプルな関数が与えられ、その関数における最小化問題を考えることにする。
* 関数 $C(\boldsymbol{v})$ を最小化することを考える。変数は $\boldsymbol{v}=v_{1},v_{2},…$ を取り、 $C(\boldsymbol{v})$ は実数値を返す関数とする。
* 例えば、$\boldsymbol{v}=v_{1},v_{2}$ とし、 $C(\boldsymbol{v})$ が2つの変数のみを持つとすると、3次元グラフを描くことで容易に大域最小値を見つけることができる。
* ただし、変数を多く持つ一般的な関数は複雑となり、グラフを眺めるだけでは最小値を見つけるのは困難である。
* この問題の攻略法の１つとして、微積分を使って解析的に最小値を見つける(=近似式も含め、数式の変形だけで最小値を見つける)方法が挙げられる。
    * 導関数を計算することで、関数の極値を見つけることができる。
    * しかし、この手法も変数が多くなると、計算量が膨大となり、機能しない(巨大なニューラルネットワークになるとコスト関数は10億の重みとバイアスを持つこともある)。
* 変数が膨大にある関数の最小化に微積分を使用できないため、別の方法を考える必要がある。
* ここで極小値を求めようとしている関数が谷のようであり、その谷の斜面をボールが転がり落ちていくところを想像し、その考え方を関数の最小化に応用できないかを考える。
    * 斜面の任意の場所をボールのスタート地点としてランダムに選ぶ。
    * ボールが谷底に向かって転がっていき、その動きをシミュレーションする(二次導関数を求めることで傾きが0になる箇所を求められる)。
* $v_{1}$ 方向、$v_{2}$ 方向それぞれに微小な量 $\Delta v_{1}$ 、 $\Delta v_{2}$ だけボールを動かした場合、関数 $C$ の微小変化 $\Delta C$ は以下の通りとなる。  
$$
\Delta C \approx \frac{\partial C}{\partial v_{1}} \Delta v_{1} + \frac{\partial C}{\partial v_{2}} \Delta v_{2} \tag{7}
$$
* ここで $\Delta C$ が負の値(=ボールが谷を転がり落ちる方向)になるような $\Delta v_{1}$ 、 $\Delta v_{2}$ を選ぶ方法を見つける必要がある。
* $\boldsymbol{\Delta v}$ を $v$ の変化ベクトルとして、 $\boldsymbol{\Delta v} \equiv (\Delta v_{1}, \Delta v_{2})^T$ と定義する。
* また、 $C$ の勾配ベクトル $\boldsymbol{\nabla C}$ を以下のように定義する。勾配ベクトルとは多変数関数に対するその偏微分を並べた多次元ベクトルのことを表す。
$$
\boldsymbol{\nabla C} \equiv (\frac{\partial C}{\partial v_{1}}, \frac{\partial C}{\partial v_{2}})^T \tag{8}
$$
* これらの定義から式(7)は以下の通り、変形できる。
$$
\Delta C \approx \frac{\partial C}{\partial v_{1}} \Delta v_{1} + \frac{\partial C}{\partial v_{2}} \Delta v_{2} = \boldsymbol{\nabla C}・\boldsymbol{\Delta v} \tag{9}
$$
* この式より、 $\boldsymbol{\nabla C}$ は $C$ を変化させる $v$ の変化に関わっており、勾配にあたる。
* $\Delta C$ を負にする $\boldsymbol{\Delta v}$ は次のように仮定することで得ることができる。
$$
\boldsymbol{\Delta v} = - \eta \boldsymbol{\nabla C} \tag{10}
$$
* $\eta$ は小さい正のパラメータで学習率と呼ぶ。
* この式から式(9)は $\Delta C \approx \boldsymbol{\nabla C}・\boldsymbol{\Delta v} = - \eta \boldsymbol{\nabla C}・\boldsymbol{\nabla C} = - \eta || \boldsymbol{\nabla C} ||^2$ となり、 $|| \boldsymbol{\nabla C} ||^2 \geq 0$ かつ $\eta \geq 0$ なので、$\Delta C \leq 0$ が成り立つ。
* 式(10)を前提にすると、 $v$ を変更する限り、 $C$ は常に減少し、増加しないことが保証できる。
* 式(10)を使い、 $\boldsymbol{\Delta v}$ を以下のように計算し、ボールの位置を $v$ から $v'$ へ移動する。
$$
v \rightarrow v' = v - \eta \boldsymbol{\nabla C} \tag{11}
$$
* この規則を使い続けると、 $C$ は減少し続け、やがて大域最小値に到達することができる。
* まとめると、勾配降下法とは勾配 $\boldsymbol{\nabla C}$ を計算し、逆方向へ動かすことを繰り返し行うことで谷へと降下させる方法である。
* 勾配降下法を正しく動作させるためには十分小さな学習率 $\eta$ を選んで、式(9)をよく近似する必要がある。
* 一方、学習率 $\eta$ が小さすぎる場合は式(10)より、 $\boldsymbol{\Delta v}$ の変化がとても小さくなり、勾配降下法の動きが非常に遅くなってしまう。
* 実用的な実装では、式(9)の近似を維持しつつ、学習率 $\eta$ を頻繁に変更して勾配降下法の進みが遅くなり過ぎないようにしている。
* ここまでは $C$ は2つの変数 $v_{1},v_{2}$ の関数としてきたが、以下では $C$ は $v_{1},v_{2},...v_{m}$ の関数として、定義しなおす。
* 微小変化を $\boldsymbol{\Delta v} \equiv (\Delta v_{1}, \Delta v_{2}, ... \Delta v_{m})^T$ とし、それによってもたらされる $C$ の変化 $\Delta C$ は以下の通りとなる。
$$
\Delta C \approx \boldsymbol{\nabla C}・\boldsymbol{\Delta v} \tag{12}
$$
* 勾配ベクトル $\boldsymbol{\nabla C}$ は以下となる。
$$
\boldsymbol{\nabla C} \equiv (\frac{\partial C}{\partial v_{1}}, ..., \frac{\partial C}{\partial v_{m}})^T \tag{13}
$$
* 同様に $\boldsymbol{\Delta v}$ は以下のように設定する。
$$
\boldsymbol{\Delta v} = - \eta \boldsymbol{\nabla C} \tag{14}
$$
* この式から式(12)は $\Delta C \approx \boldsymbol{\nabla C}・\boldsymbol{\Delta v} = - \eta \boldsymbol{\nabla C}・\boldsymbol{\nabla C} = - \eta || \boldsymbol{\nabla C} ||^2$ となり、 $|| \boldsymbol{\nabla C} ||^2 \geq 0$ かつ $\eta \geq 0$ なので、$\Delta C \leq 0$ が成り立つ。
* 式(14)を前提にすると、 $v$ を変更する限り、 $C$ は常に減少し、増加しないことが保証できる。
* 式(14)を使い、 $\boldsymbol{\Delta v}$ を以下のように計算し、ボールの位置を $v$ から $v'$ へ移動する。
$$
v \rightarrow v' = v - \eta \boldsymbol{\nabla C} \tag{15}
$$
* この規則を使い続けることで、 $C$ の大域最小値に到達できる。
* 勾配降下法はどんな時でも機能するわけではなく、時には勾配降下法が大域最小値の発見を妨げることもある。
* ただ多くの場合、勾配降下法はよく機能し、ニューラルネットワークのコスト関数に対する強力な最小化手段であり、ネットワークの学習を助けてくれるものである。
* これまで様々な勾配降下法が研究されてきており、いくつか長所を持っているものの、一方で大きな欠点も持っている。
* 具体的な欠点として、勾配降下法は二階偏微分の計算が必要であり、計算量が膨大になることが挙げられる。
* ただ既にこれを回避する手段はいくつか存在しており、また勾配降下法の代替手段も盛んに研究されている。
* ニューラルネットワークの学習に勾配降下法を適用するということは、式(6)のコストを最小化する重みとバイアスを見つけるために勾配降下法を用いることと等価である。
* これまで式(12)～(15)で用いてきた $v_{j} (j=1,2, ...)$ を重みとバイアスに置き換えて、勾配降下法の各式を再定義する。
* つまり $v_{j}$ ではなく、$w_{k}$ , $b_{l}$ を用い、勾配ベクトル $\boldsymbol{\nabla C}$ は要素として、 $\partial C/\partial w_{k}$、 $\partial C/\partial b_{l}$ をもつものとして表現すると、式(15)は以下のように書き換えることができる。
$$
w_{k} \rightarrow w_{k}' = w_{k} - \eta \boldsymbol{\nabla C} = w_{k} - \eta \frac{\partial C}{\partial w_{k}} \tag{16}
$$
$$
b_{l} \rightarrow b_{l}' = b_{l} - \eta \boldsymbol{\nabla C} = w_{k} - \eta \frac{\partial C}{\partial b_{l}} \tag{17}
$$
* 上式に従って、更新を繰り返すことでボールが斜面を転がり降り、うまくいけばコスト関数の最小値を見つけることができる。つまり、ニューラルネットワークの学習にも勾配降下法を適用することができる。
* 勾配降下法を使った学習の高速化のアイデアとして、確率的勾配降下法という手法がある。
* 勾配降下法で式(6)のコスト関数を最小化し、その時の重みとバイアスを得ようとすると、学習時間の点で問題がある。
    * コスト関数の式(6)は $C = \frac{1}{n} \sum_{x}^{} C_{x}$ ,　　$C_{x} \equiv \frac{|| \boldsymbol{y}(\boldsymbol{x}) - \boldsymbol{a} ||^2}{2}$ と書き直すことができる。
    * 勾配降下法を適用するため、 上記の2式から $\boldsymbol{\nabla C}$ を計算すると、 $\boldsymbol{\nabla C} = \frac{1}{n} \sum_{x}^{} \boldsymbol{\nabla C_{x}}$ となり、$x$ の大きさ(=入力データの数)によって、偏微分の計算量が変わる $\boldsymbol{\nabla C_{x}}$ を計算する必要が出てくる。
    * よって、入力データ数が非常に大きい場合、勾配降下法を用いたニューラルネットワークの学習にとても時間がかかる。
* 確率的勾配降下法では、入力データから無作為に抽出したデータに対して、 $\boldsymbol{\nabla C_{x}}$ を計算し、その計算結果から $\boldsymbol{\nabla C}$ を推定する。
* つまり、無作為に抽出したデータ(小さな標本群)ごとに、 $\boldsymbol{\nabla C_{x}}$ を計算し、それらの平均を取って、 $\boldsymbol{\nabla C}$ を求めることで、勾配降下法及び学習自体の高速化を実現できる。
* 確率的勾配降下法の詳細は以下の通り。
    * 小さい数 $m$ を無作為抽出する。 $m$ は訓練入力データを無作為抽出する数(標本サイズ)として使用される。
    * ランダムに選んだ訓練入力データを $X_{1}, X_{2},...,X_{m}$ とし、これら入力データ群をミニバッチと呼ぶ。
    * 標本サイズ $m$ が十分大きい場合、 $\boldsymbol{\nabla C_{X_{j}}}$ の平均値は全ての訓練入力データに対する $\boldsymbol{\nabla C_{x}}$ の平均値とほぼ同じとなると言える。すなわち、以下の式が成り立つ。
$$
\frac{1}{m} \Sigma_{j=1}^{m} \boldsymbol{\nabla C_{X_{j}}} \approx \frac{1}{n} \Sigma_{x} \boldsymbol{\nabla C_{x}} = \nabla C \tag{18}
$$
* 式(18)から 訓練データ全体に対する勾配 $\boldsymbol{\nabla C}$ とミニバッチにおける勾配 $\boldsymbol{\nabla C_{X_{j}}}$ の関係のみを抽出すると、以下となる。
$$
\nabla C \approx \frac{1}{m} \Sigma_{j=1}^{m} \boldsymbol{\nabla C_{X_{j}}} \tag{19}
$$
* 上記の式より、ミニバッチを計算することにより、全体の勾配を推定できることが確認できることがわかる。
* 式(19)を使い、重み $w_{k}$, バイアス $b_{l}$ を用いたニューラルネットワークの形式として表記すると、式(16), 式(17)はそれぞれ以下のようになる。
$$
w_{k} \rightarrow w_{k}' =  w_{k} - \frac{\eta}{m} \Sigma_{j=1}^{m} \frac{\partial C_{X_{j}}}{\partial w_{k}} \tag{20}
$$
$$
b_{l} \rightarrow b_{l}' =  b_{l} - \frac{\eta}{m} \Sigma_{j=1}^{m} \frac{\partial C_{X_{j}}}{\partial b_{l}} \tag{21}
$$
* 確率的勾配降下法では、全訓練入力データから無作為に選んだミニバッチで訓練を行い、それを全訓練入力データがなくなるまで行う。
* 無作為抽出する訓練入力データがなくなった時点で、1回の訓練が完了したとみなし、その1巡をエポックという。これはつまり、1エポック中に全訓練入力データがニューラルネットワークの学習に必ず使用されることを意味する。
* ここで、式(6)で表されているコスト関数は $1/n$ をかけている(=訓練入力のデータ数で割っている)。これにより、コスト関数は訓練入力の平均誤差をコスト値として扱っている。
* 時にこの $1/n$ は省略されることがある。省略することでコスト関数は訓練入力の総和をコスト値として扱うことになる。
* これは特に訓練入力の総数 $n$ が事前にわからない場合に有効となる。例えば、リアルタイムに訓練データが生成される場合などがそれにあたる。
* また、式(20)、(21)における $1/m$ (=訓練データから無作為に抽出されるデータ数・バッチサイズ)も省略されることがあるが、これは $\eta$ の大きさを変更することに相当するので、あまり概念的な意味に変化はないと言える。
* 確率的勾配降下法は世論調査のように考えることができる。
    * 国民総選挙よりも、世論調査を実施する方が簡単なのと同様、全訓練データを一括処理で勾配降下法を適用するよりも、小さな標本を元にミニバッチを行う方が勾配降下法の適用が簡単である。
    * $n=60,000$ の訓練データに対し、ミニバッチの大きさが $m=10$ とすると、訓練データ全体に対して勾配降下法を適用するのに比べ、勾配の推定は6,000倍早くできることになる。
    * ミニバッチによる勾配の推定は完璧ではないが、あまり気にする必要はない。
    * 気にする必要があるのは、コスト値 $C$ が減少する大まかな移動方向であり、その際に正確な勾配の計算は必要ないためである。
    * 確率的勾配降下法はニューラルネットワークの学習によく用いられる強力な手法であり、学習テクニックの大部分の基礎になる。
* コスト関数は全ての重みとバイアスを変数にもつ高次元な関数であり、これらを可視化することはできない。
* 高次元な空間上の曲面を可視化する代わりに、 $C$ を減少させる方法を明らかにする $\Delta C$ の代数表現などを用いる。

#### 数字を分類するニューラルネットワークの実装
* 手書き数字を認識する方法を学ぶプログラムを作成するために、確率的勾配降下法とMNISTの訓練データを使用する。
* MNISTの訓練データは以下のリポジトリからgit cloneして取得することができる。

```
git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git
```

* MNISTのデータは60,000の訓練データと10,000のテストデータに分かれている。
* ここで訓練データ60,000を分割し、訓練とその検証にそれぞれ50,000データ、10,000データを使用するものとする。
* この検証に使用するデータはハイパーパラメータ(学習率など、学習アルゴリズムで直接選択できないパラメータ)を設定するのに役立つ。
* 検証データはMNISTの仕様に含まれていないが、現在はこのやり方でデータが使われていることが多い。
* 以下、MNISTの訓練データは上記の50,000データを指すことにする。
* データとは別に、高速に線形代数を解くためのNumpyというPythonライブラリも学習環境にインストールしておく必要がある。
* ニューラルネットワークのコードのコア部分はNetworkクラスが中心でニューラルネットワークを表現するために使用する。
* Networkクラスの初期化部分のコードは以下となる。

```
class Network():

    def __init__(self, sizes):
        self.num_layers = len(sizes)   # layers：
        self.sizes = sizes             # sizes ：各層のニューロンの数を要素として持つ配列
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
                                       # randn(a,b)：平均値0、標準偏差1のガウス分布の乱数を要素に持つa×bのarrayを返す。
                                       # yはfor文により、sizesの2番目の要素以降が順に代入されてarrayが生成される。
                                       # sizesが[2, 3, 1]の場合、3×1, 1×1のarrayがbiasesに格納される。
        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
                                       # sizesが[2, 3, 1]の場合、xは2,3,1、yは3,1と各for loopに入れられる。
                                       # よって、3×2, 1×3のarrayが生成され、weightに格納される。
```

* sizesは各層のニューロンの数を表しており、第1～3層までのニューロンの数がそれぞれ2, 3, 1とすると、以下のように定義する。

```
net = Network([2, 3, 1])
```

* バイアスと重みは初期化時にNumpyのnp.random.randnによって生成している。これは平均値0、標準偏差1のガウス分布の乱数で初期化されることを意味する。
* これらの初期化された値は確率的勾配降下法の開始点として使用される。
* このプログラムでは乱数で初期値を決めているが、実際には重みとバイアスにより良い初期値を与える方法もある。
* 上記のコードの第1層は入力層のため、バイアスの初期値生成時はsizes[1:]を指定することにより、2層目以降のニューロンに対してのみバイアスの初期値を生成している。
* バイアスと重みはNumpyの行列のリストとして保存される。よって、net.weight[1]には第2層と第3層の各ニューロンをつなぐ重みが保存される。
* net.weight[1]は記述として冗長なので、 $\boldsymbol{w}$ という行列表記を使用して表示する。
* 行列の各成分は $\boldsymbol{w_{jk}}$ で表され、第2層のk番目のニューロンと第3層のj番目のニューロンを結ぶ重みを表す。
* また、第3層目のニューロンの活性化ベクトルを $\boldsymbol{a'}$ は第2層目のニューロンの活性化ベクトル $\boldsymbol{a}$、第2層の各ニューロンと第3層の各ニューロンの重みを成分として持つ重み行列 $\boldsymbol{w}$、第2層目の各ニューロンのバイアスを成分として持つバイアスのベクトル $\boldsymbol{b}$、シグモイド関数 $\sigma$ を用いて、以下のように表すことができる。
$$
\boldsymbol{a'} = \sigma(\boldsymbol{wa+b}) \tag{22}
$$
* Networkから出力を計算するためのコードはシグモイド関数の定義を含め、以下のようになる。

```
def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))

sigmoid_vec = np.vectorize(sigmoid)
```

* sigmoidメソッドはスカラー実装のため、numpyのvectorizeメソッドでsigmoidメソッドをベクトル化(スカラー値に対する実装からベクトルに対する処理ができる実装への変換処理)し、sigmoid_vecを生成する。
* ネットワークの入力aが与えられたら、出力を返すfeedforwardメソッドをNetworkクラスに追加する。feedforwardは各層ごとに式(22)を適用する。

```
def feedforward(self, a):
        """Return the output of the network if "a" is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid_vec(np.dot(w, a)+b)
        return a
```

* Networkクラスで学習するために確率的勾配降下法(SGD)を使用するために以下のコードを追加する。

```
def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The "training_data" is a list of tuples
        "(x, y)" representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If "test_data" is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""

        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):                          # xrangeはpython2系、python3のrangeと同じ
            random.shuffle(training_data)                 # shuffleでtraining_dataのリストの要素をランダムソート
            mini_batches = [                              # training_dataの各要素をmini_batch_sizeごとに分割し、
                training_data[k:k+mini_batch_size]        # mini_batchesに分割したリストを要素とするリストを作成
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)
```

* training_dataは訓練入力とそれに対応したラベルデータの組(x,y)のリスト、epochsとmini_batch_size、etaはそれぞれ訓練の世代数とサンプリングの際のミニバッチの大きさ、学習率を表す。
* オプションの引数test_dataは各訓練エポックの後にネットワークを評価する場合に指定する。
* ネットワークを評価することで現在の学習の進行状況が出力される。学習モデルの性能改善の状況を確認する場合に役立つが計算処理により、学習全体に時間がかかるようになる。
* 各エポックではまず訓練データをシャッフルし、ミニバッチのサイズに分割する。
* 各ミニバッチでは、 self.update_mini_batch(mini_batch, eta) で勾配降下法を1ステップ実行する。
* これによりミニバッチの訓練データのみを使用して勾配降下法を実行してネットワークの重みとバイアスを更新する。
* update_mini_batchメソッドのコードは下記の通り。

```
def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]
```

* このメソッドの作業の多くは```delta_nabla_b, delta_nabla_w = self.backprop(x, y)```で行われている。
* backpropメソッドでコスト関数の勾配を高速で計算する誤差逆伝搬法(backpropagation)を起動・適用する。
* update_mini_batchはミニバッチ内の訓練データごとに勾配を計算し、self.weightsとself.biasesを更新する。
* これまで説明した完全なプログラムは以下の通り。

```
~~~~~~~~~~
network.py
~~~~~~~~~~
A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network():

    def __init__(self, sizes):
        """The list ``sizes`` contains the number of neurons in the
        respective layers of the network.  For example, if the list
        was [2, 3, 1] then it would be a three-layer network, with the
        first layer containing 2 neurons, the second layer 3 neurons,
        and the third layer 1 neuron.  The biases and weights for the
        network are initialized randomly, using a Gaussian
        distribution with mean 0, and variance 1.  Note that the first
        layer is assumed to be an input layer, and by convention we
        won't set any biases for those neurons, since biases are only
        ever used in computing the outputs from later layers."""
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid_vec(np.dot(w, a)+b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The ``training_data`` is a list of tuples
        ``(x, y)`` representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If ``test_data`` is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        """Return a tuple ``(nabla_b, nabla_w)`` representing the
        gradient for the cost function C_x.  ``nabla_b`` and
        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
        to ``self.biases`` and ``self.weights``."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid_vec(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime_vec(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            spv = sigmoid_prime_vec(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * spv
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation."""
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations-y)

#### Miscellaneous functions
def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

sigmoid_vec = np.vectorize(sigmoid)

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

sigmoid_prime_vec = np.vectorize(sigmoid_prime)
```

* ここからはこのコードがどれだけよく手書き文字を認識できるかを確認していく。
* まずはMNISTデータをダウンロードする。mnist_loader.pyを使用して、以下のコマンドを実行する。

```
>>> import mnist_loader
>>> training_data, validation_data, test_data = \
... mnist_loader.load_data_wrapper()
```

* データをダウンロード後、30個の隠れ層ニューロンを持つNetoworkを設定する。入力層/出力層のニューロン数はMNISTデータの仕様により、それぞれ、784個、10個に決まる。

```
>>> import network
>>> net = Network([784, 30, 10])
```

* 最後にエポック数(訓練の世代数)を30、ミニバッチサイズを10、学習率 $\eta$ を3.0とし、確率的勾配降下法を使用してMNISTデータを学習する。

```
>>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)
```

* これらの処理には2014年時点の一般的なスペックのPCだと多少の時間がかかる。訓練は1世代につき、数分程度かかる。
* 結果を急いで取得したい場合は、世代数を減らす、隠れニューロンの数を減らす、訓練データの一部のみを使用するといったことで可能になる。
* 一度、ニューラルネットワークを訓練すれば、それを他の多くのプラットフォームで非常に高速に実行することができる。
* ニューラルネットワークの重みとバイアスのセットがあれば、ブラウザのJavascriptやモバイルアプリに移植し、実行するのは容易である。
* 以下にニューラルネットワークの訓練プロセスの一部の結果を示す。
* この結果は、エポックごとにニューラルネットワークを使用して適切に訓練データを認識できた数を示している。

```
Epoch 0: 9129 / 10000
Epoch 1: 9295 / 10000
Epoch 2: 9348 / 10000
...
Epoch 27: 9528 / 10000
Epoch 28: 9542 / 10000
Epoch 29: 9534 / 10000
```

* 最初のエポックが終了した時点で10000個中9129個が正しく認識できたことを示しており、その後、28世代までは学習を行うにつれてその数が増加していっていることがわかる。
* また、このネットワークは95%の分類率を有していると言える。
* ただ重みとバイアスをランダム値で初期化しているため、常に全く同じ分類率になるとは限らない。
* 次に隠れニューロン数を100にして同じ計算を行う。
* 隠れニューロン数を多くすると、全体の学習時間が長くなる。

```
>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)
```

* この計算では96.59%に分類率が向上した。これは少なくともこのケースではより多くの隠れ層を使用することでよい結果が得られることを示している。
* 分類率の精度を獲得するためには隠れ層のニューロン数だけでなく、訓練のエポック数、ミニバッチサイズ、学習率 $\eta$ などを具体的に選択する必要がある。
* これらのパラメータは学習アルゴリズムによって学習するパラメータ(重みとバイアス)と区別して、ハイパーパラメータと呼ぶ。
* ハイパーパラメータを不適切に選択してしまうと、悪い結果を得ることになる。
* 例えば、学習率 $\eta = 0.001$を選択したとすると、結果の改善の進捗が遅くなってしまう。

```
>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)
```
```
Epoch 0: 1139 / 10000
Epoch 1: 1136 / 10000
Epoch 2: 1135 / 10000
...
Epoch 27: 2101 / 10000
Epoch 28: 2123 / 10000
Epoch 29: 2142 / 10000
```

* 改善の進捗は遅いものの、分類率は向上しているので、これは学習率を大きくすべきであることを意味していると言える。
* 例えばここで、学習率を $\eta = 0.01$ にして再度実行してみると、より良い結果を得ることができる。
* ハイパーパラメータはこのように値を変更して結果を確認することで性能を改善する適切なハイパーパラメータの値を得ることができる。
* 多くの場合、ニューラルネットワークをデバッグすることは困難であり、特にハイパーパラメータの選択が悪い場合は特に困難となる。
* ここで、学習率 $\eta = 100.0$ に変更した場合を考える。

```
>>> net = network.Network([784, 30, 10])
>>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)
```
* この結果を確認すると、やはり学習率の値が大きすぎることがわかる。
```
Epoch 0: 1009 / 10000
Epoch 1: 1009 / 10000
Epoch 2: 1009 / 10000
Epoch 3: 1009 / 10000
...
Epoch 27: 982 / 10000
Epoch 28: 982 / 10000
Epoch 29: 982 / 10000
```

* ただ、もし初めて学習を行い、上記のような結果が得られたとする。
* 前の実験の結果を知っていれば、学習率を下げることが正しいことがわかるが、初めてこの結果が得られた場合、出力結果から何をすべきかはわからないことがほとんどである。
* これは学習率だけではなく、ニューラルネットワークの他の全てのことに当てはまる。
* 重みとバイアスの初期値の選定がよくなく学習を難しくしたのではないか、十分な訓練データを持っていないのではないか、エポック数が十分ではないのではないか、と思うこともある。
* さらには手書き数字を認識するための学習はニューラルネットワークでは不可能だと思うこともある。
* こういった疑問を取り除くためのニューラルネットワークのデバッグは難しく、この方法を学ぶ必要がある。
* つまり良いハイパーパラメータと良いアーキテクチャを選択するための経験則的な技術を開発する必要がある。
* これらの技術は後述する。
* 上述のプログラムでよい学習結果が得られた、とはどういう意味で何と比較しているのか？
* 学習結果を理解するために性能基準を持つことには非常に意味がある。
* 例えば、数字をランダムに推測するとその性能は10％程度になる。人間が目で判断する方がより高い性能を持っている。
* 別の性能基準として、画像がどれだけ暗いか？について見てみると、例えば「2」画像の方が「1」より暗い部分が多いはずである。
* つまり0～9の各数字の黒色のピクセルの平均値を学習するために訓練データを使用することを意味する。
* 学習により、ネットワークはデータが入力されたとき、その画像がどれだけ黒いピクセルを持っているかによって、その画像のデータの数字を区別するようになる。
* この方法では10000枚の訓練データのうち、2225枚を正しく区別できた。性能は22.25％ということになり、ランダムな推測に比べて大きな改善ができたことがわかる。
* 一方、性能(予測精度)をさらに20～50％程度まで改善させるには、機械学習アルゴリズムを使用することで実現できる。
* よく知られている手法としてサポートベクターマシン(SVM)がある。scikit-learnというライブラリを使用することでSVMを簡単に試すことができる。
* scikit-learnのSVM分類器をデフォルト設定で使用したとすると、10000枚のうち、9435枚を適切に分類できる。
* 上述の黒色のピクセルの平均値に基づいた分類方法から比べると大きな改善となる。
* SVMはニューラルネットワークと同程度の性能を持っていることが知られている。
* SVMは調整可能なパラメータをいくつか持っており、さらに性能のよい判別を行うためにはこれらのパラメータを調整し、その値を探すことができる。
* 一部の研究ではSVMのパラメータを最適化することで、98.5％というかなり良い予測精度を達成している。
* ニューラルネットワークでもこれに匹敵する、もしくはこの性能を上回ることは可能である。
* MNISTの識字アルゴリズムの中で、よく設計されたニューラルネットワークが最も優れている。
* 2014年時点では10000枚の画像のうち、9979枚を適切に分類することができている。その性能は人間とほぼ同等かそれを上回っていると考えられる。
* それはMNIST画像の中には人間であっても自信をもって認識することが難しい画像も含まれているからである。
* そのようなデータが含まれているにも関わらず、ニューラルネットワークが21枚以外の画像を適切に分類できることは驚くべきことである。
* MNISTの識字を行うためには高度なアルゴリズムが必要と考えがちだが、ニューラルネットワークを使えば、かなりシンプルなアルゴリズムであっても複雑な問題を解決できることを意味している。

##### Deep Learningに向けて
* ニューラルネットワークは印象的な性能を提供する一方、その性能は神秘的である。
* ネットワークの重みやバイアスが自動的に発見される、ということはそのネットワークがどのように機能しているかについて説明できないことを意味する。
* ネットワークが手書き数字を分類する原理を理解する方法を見つけられるか？そのような原理を考え、さらに良くすることができるか？
* この問いに厳密に答えるために、まず直近20～30年でニューラルネットワークは人工知能(AI)になると考える必要がある。
* AIのような賢いネットワークがどのように働くかを理解することは可能だろうか？自分が把握していない重みとバイアスを使ったネットワークは不透明なものになるかもしれない。
* 初期のAI研究者はAIを構築できるようになれば、知能の原理を理解できるようになる、と考え、それを望んでいた。ただ人間は自分たちの脳はおろか、人工知能の働きさえ理解できないかもしれない。
* これらの問題に答えるために画像が人の顔を示しているかどうかを判断することを考える。
* 手書き文字認識と同じ方法で、ニューラルネットワークの入力として画像ピクセルを使い、1つの出力ニューロンで「顔である」か「顔でない」かを判断させることができる。
* ただし、学習アルゴリズムは使わず、ネットワークを手動で設計し、適切な重みとバイアスを設定していくとする。
* その場合、どうすればよいか？1つはヒューリスティクスによって問題を小さな問題に分割することを考える。
* 分割される小さな問題とは、「画像の左上に目はあるか」、「画像の右上に目はあるか」、「画像の中心に鼻はあるか」、「画像の中央下に口はあるか」、「画像の上の方に髪の毛はあるか」といったものである。
* これらの問題の答えのいくつかが、「Yes」もしくは「おそらくYes」ならその画像は顔だと言える。逆にほとんどが「No」であれば、おそらくその画像は顔ではないと言える。
* この荒いヒューリスティクスには多くの欠点がある。禿げた人には髪の毛がなく、また顔の一部のみが含まれていたり、顔に角度がついていたり、顔の一部が隠されていたりした場合、正しく顔を判断できない。
* ただ、このヒューリスティクスから小さな問題をニューラルネットワークを使って解くことができるなら、それらのネットワークを組み合わせることで顔判定のためのネットワークを構築できるということがわかる。
* ただし、このような問題の解き方が顔認識問題を解くための現実的なアプローチとはならない。
* 顔認識問題を解くための「小さな問題」はさらに小さく分解できる。
* 例えば、画像の左上に目はあるか」という問題は、「眉毛はあるか」、「まつ毛はあるか」、「眼球の光彩はあるか」などより小さな問題に分割できる。
* 加えてこれらの問題は位置情報を含み、「画像の右上に眉毛はあるか、それは光彩の上にあるか」という形になるべきである。
* このような問題は複数の層を介して、さらに分解していくことが可能であり、究極的には単一ピクセルレベルで簡単に答えられるシンプルな問題に答える程度まで小さく分解される。
* つまり、画像における顔認識問題のように非常に込み入った問題は単一のピクセルレベルで答えられる問題に分解したネットワークの集合である。
* そのためには入力画像についてのとても簡単な特定の質問に答えるための初期の層と、より複雑で抽象的な概念の階層を構築する後半の層を含む多くの層を通る必要がある。
* 多くの隠れ層(2つorそれ以上)を含む多層構造ネットワークはディープニューラルネットワークと呼ばれる。
* 上記では、サブネットワーク(小さな問題を解決するニューラルネットワーク)をどのように再帰的に分割していくかには触れておらず、またネットワーク内の重みとバイアスを手動で設計するのも実用的ではない。
* その代わりに訓練データから自動的に重みとバイアスを習得できる学習アルゴリズムを使用する。
* 1980年代・1990年代の研究者は確率的勾配降下法と誤差逆伝搬法をディープニューラルネットワークの訓練に使用することを試みたが、学習がとても遅く、現実的に使用できなかったため、よい結果を得られなかった。
* 2006年以降、ディープニューラルネットワークを学習するための一連の技術が開発された。
* これらはこれまでと同様、確率的勾配降下法と誤差逆伝搬法に基づいているが、新しいアイデアが追加されている。
* 追加された新しい技術はより深く、より大きなネットワークの訓練を可能にし、現在では5～10層のネットワークが当然のように訓練されている。
* またこれらのネットワークが浅いニューラルネットワークよりも多くの問題解決に対してよくなっていることもわかっている。これはディープネットワークの方がより概念の複雑な階層構造を構築できるからである。

### 第2章：逆伝播の仕組み
* これまでは勾配降下法を用いてニューラルネットワークが重みとバイアスをどのように学習するかを述べてきたが、その説明にはギャップがあった。
* それはコスト関数の勾配 $\boldsymbol{\nabla C}$ をどのように計算するかについて触れていないからである。
* 以降では逆伝播と呼んでいるコスト関数の勾配を高速に計算するアルゴリズムについて説明する。
* 逆伝播アルゴリズムは1970年代に導入されたが、初めて評価されたのは、David Rumelhart・Geoffrey Hinton・Ronald Williamsによる論文が登場してからになる。
* その論文では、逆伝播を用いることで既存の学習方法よりも早く学習できる事をいくつかのニューラルネットワークに対して示しており、またそれまでニューラルネットワークで解けなかった問題が解けることを示している。
* 今日では逆伝播アルゴリズムはニューラルネットワークを学習させる便利なアルゴリズムとなっている。
* 数学的に難解な逆伝播アルゴリズムをなぜ時間をかけて勉強するのか？それは理解のためである。
* 逆伝播アルゴリズムの本質はコスト関数 $C$ のネットワークの重み $w$ とバイアス $b$ に関する偏微分 $\partial C / \partial w$ ,  $\partial C / \partial b$ である。
* 偏微分することで、重みやバイアスを変化させたときのコスト関数の変化の度合いがわかる。
* 偏微分は複雑ではあるものの、美しい構造があり、式の各要素に自然で直感的な解釈を与えることができる。
* また、逆伝播アルゴリズムを見ることで重みやバイアスを変化させたときのニューラルネットワーク全体の挙動の変化に関して深い洞察が得られる。
* ここに逆伝播アルゴリズムを勉強し、理解する価値があるが、逆伝播アルゴリズムをブラックボックスにして扱うことも可能である。

##### ウォーミングアップ：ニューラルネットワークの出力の行列を用いた高速な計算
* 始めにニューラルネットワーク中の重みを指定する表記方法を整理する。
* $w^{l}_{jk}$ は第 $(l-1)$ 層の $k$ 番目のニューロンから第 $l$ 層の $j$ 番目のニューロンへの接続に対する重みを表わす。
* つまり、 $w^{3}_{24}$ は第2層の4番目のニューロンから第3層の2番目のニューロンへの接続の重みを表わす。
* この表記は面倒で使いこなすのに時間がかかるが、少し経てば、この表記方法が簡単で自然だと感じるようになる。
* この表記方法で少しわかりづらいのは $j$ と $k$ の順番である。 $j$ を入力ニューロン、$k$ を出力ニューロンとする方が自然に感じるが、実際には逆になっている。
* ニューラルネットワーク中のバイアスとニューロンの出力(≒シグモイドニューロンの出力)についても似たような表記方法を導入する。
* $b^{l}_{j}$ は第 $l$ 層の $j$ 番目のニューロンのバイアスを表し、 $a^{l}_{j}$ は第 $l$ 層の $j$ 番目のニューロンの出力を表す。
* これらの表記を用いて、第 $l$ 層の $j$ 番目のニューロンの出力 $a^{l}_{j}$ は、第 $(l-1)$ 層のニューロンの出力を使って、以下のように表現できる。
$$
a^{l}_{j}  =  \sigma(\Sigma_{k} w^{l}_{jk} a^{l-1}_{k} + b^{l}_{j}) \tag{23}
$$
* これは式(22)の行列ベースの表記から各行列の要素ベースの表記に書き直したものである。
* $\Sigma$ による和は第 $l-1$ 層の全てのニューロン $k$ 個のニューロンの出力(第 $l$ 層における入力全て)について足し合わせている。
* 重み行列 $\boldsymbol{w^{l}}$ は各層 $l$ に対して定義され、その各要素は第 $l$ 層のニューロンを終点とする接続の重みとなっている。 $j$ 行目 $k$ 列目の要素は $w^{l}_{jk}$ と表される。
* 同様にバイアスベクトル $\boldsymbol{b^{l}}$ も各層 $l$ に対して定義され、その各要素は $b^{l}_{j}$ で表し、第 $l$ 層の各ニューロンに対するバイアスとなる。
* ニューロンの出力ベクトル $\boldsymbol{a^{l}}$ も同様に定義でき、その各要素は $a^{l}_{j}$ で表される。
* 式(23)を行列形式で表現するためには、 $\sigma$ などの関数をベクトル化する必要がある。
* これは、 $\sigma$ のような関数をベクトル $\boldsymbol{v}$ の各要素に適用することを意味する。
* 各要素への関数の適用には $\sigma(v)$ という表記を用い、 $\sigma(v)_{j} = \sigma(v_{j})$ とする。
* $f(x) = x^2$ を例にとると、以下のようになることを意味する。
$$
f([2,3]) = [f(2), f(3)] = [4, 9] \tag{24}
$$
* このような表記を用いると、式(23)は以下のように、コンパクトなベクトル形式で書くことができる。
$$
a^{l}  =  \sigma(w^{l} a^{l-1} + b^{l}) \tag{25}
$$
* ここで、 $\boldsymbol{\sigma}$ はベクトル化された活性化関数(≒シグモイド関数)となる。
* この表現を用いることで、ある層 $l$ のニューロンの出力とその前の層のニューロンの出力との関係を俯瞰してみることができる。
* つまり、ある層のニューロン出力はその1つ前の層のニューロン出力に対し、重み行列を掛け、バイアスベクトルを足し、最後に $\sigma$ 関数を適用することを意味する。
* この見方はこれまでのニューロン単位での見方よりも簡潔で、添え字が少なくて済む。
* また、この表現方法は実用上も有用で、多くの行列ライブラリでは、高速な行列掛算やベクトル足し算、関数のベクトル化に関する機能が実装されているからである。
* $a^{l}$ を計算するために式(25)を利用すると、その過程で $z^{l} \equiv w^{l} a^{l-1} + b^{l}$ を計算している。
* この値 $z^{l}$ を第 $l$ 層に対する重み付き入力と呼ぶ。

##### コスト関数に必要な2つの仮定
* 逆伝播の目標はコスト関数 $C$ のネットワークの重み $w$ とバイアス $b$ に関する偏微分 $\partial C / \partial w$ ,  $\partial C / \partial b$ をより高速に計算することである。
* 逆伝播を機能させるためにはコスト関数の形について2つの仮定を置く必要がある。
* 前述の式(6)のような2乗コスト関数、式(26)を考える。
$$
C = \frac{1}{2n} \sum_{x}^{} || \boldsymbol{y}(\boldsymbol{x}) - \boldsymbol{a^L(\boldsymbol{x})} ||^2 \tag{26}
$$
* nは訓練データの数、 $\boldsymbol{y}(\boldsymbol{x})$ は目標とするネットワークの出力ベクトル、 $\boldsymbol{a^L(\boldsymbol{x})}$ は $\boldsymbol{x}$ を入力したときの出力ベクトルを表す。
* 1つ目の仮定はコスト関数が個々の訓練データ $x$ に対するコスト関数 $C_{x}$ の平均 $C = \frac{1}{n} \sum_{x}^{} C_{x}$ で表されることである。
* 2乗コスト関数ではこの仮定が成立しており、その場合、1つの訓練データに対するコスト関数 $C_{x}$ を $\frac{1}{2}|| y - a^L ||^2$ とすればよい。
* この仮定が必要となるのは、逆伝播によって計算できるのが個々の訓練データに対する偏微分 $\partial C_{x} / \partial w$ ,  $\partial C_{x} / \partial b$ だからである。
* コスト関数の偏微分 $\partial C / \partial w$ ,  $\partial C / \partial b$ は全訓練データ個々の偏微分の平均を取ることで得られる。
* 以降、訓練データ $x$ を1つに固定していると仮定し、コスト $C_{x}$ の添字を除き、 $C$ と表記する。
* 2つ目の仮定はコスト関数がニューラルネットワークの出力の関数で書かれていることである。
* 2乗コスト関数はこの仮定を満たしている。それは訓練データ $x$ に対する誤差は以下のように書くことができるためである。
$$
C = \frac{1}{2}|| y - a^L ||^2 = \frac{1}{2} \sum_{j}^{} (y_{j} - a_{j}^L)^2 \tag{27}
$$
* このコスト関数は目標とするネットワークの出力ベクトル $y$ にも依存しているが、 $y$ の関数とはみなさない。
* これは訓練データ $x$ を固定することで目標とする出力 $y$ も固定されているからである。
* つまり、目標とする出力 $y$ は重みやバイアスを変化させたところで変化されるものではなく、ニューラルネットワークが学習するものではない、と言える。
* これらより、 $C$ はネットワークの出力 $a^L$ 単独の関数とみなすことができ、一方、 $y$ はコスト関数を定義するための単なるパラメータとみなすことができる。

##### アダマール積　　$\boldsymbol{s} \circ \boldsymbol{t}$
* 逆伝播アルゴリズムはベクトルの足し算やベクトルと行列の掛け算など一般的な代数操作に基づいている。
* ただベクトルの要素ごとの積(アダマール積 or シューア積)はあまり一般的ではない。
* アダマール積は $\boldsymbol{s}$ と $\boldsymbol{t}$ が同じ次元のベクトルとしたとき、$\boldsymbol{s} \circ \boldsymbol{t}$ で表す。
* 要素ごとに表記すると、以下の通りとなる。
$$
\left[\begin{array}{rrr} 1 \\ 2 \end{array}\right] \circ \left[\begin{array}{rrr} 3 \\ 4 \end{array}\right] = \left[\begin{array}{rrr} 1*3 \\ 2*4 \end{array}\right] = \left[\begin{array}{rrr} 3 \\ 8 \end{array}\right]  \tag{28}
$$
* 多くの行列ライブラリにはアダマール積の高速な実装が用意されており、逆伝播アルゴリズムを実装するのに利用できることが多い。

##### 逆伝播の基礎となる4つの式
* 逆伝播アルゴリズムは重みとバイアスの値に変化を加えた時にコスト関数がどのように変化するかを把握する方法である。
* これはつまり、 $\partial C / \partial w_{jk}^{l}$ ,  $\partial C / \partial b_{j}^{l}$ を計算することを意味する。
* この偏微分を計算する過程で中間的な値 $\delta_{j}^l$ を導入し、この値を第 $l$ 層の $j$ 番目のニューロンの誤差と呼ぶ。
* このニューロンの誤差 $\delta_{j}^l$ の定義を理解するために、ニューラルネットワーク上の第 $l$ 層の $j$ 番目のニューロンを考える。
* このニューロンに入力が入ってきたとき、このニューロンに対する重み付き入力 $z_{j}^l$ に小さな変更 $\Delta z_{j}^l$ を加えるとする。
* この変更が入ると、そのニューロンは $\sigma(z_{j}^l)$ の代わりに $\sigma(z_{j}^l+\Delta z_{j}^l)$ を出力することになる。
* この出力の変化はニューラルネットワークの後段に伝播し、ニューラルネットワーク全体のコスト関数の値を $\frac{\partial C}{\partial z_{j}^l} \Delta z_{j}^l$ だけ変化させることになる。
    * 縦軸 $C$ , 横軸 $z_{j}^l$ のグラフにおいて、 グラフ上の任意の点における接線の傾きは $\partial C / \partial z_{j}^l$ となる。この接線に沿って、 $z_{j}^l$ 方向に $\Delta z_{j}^l$ 移動した場合の $C$ の増加分は $\frac{\partial C}{\partial z_{j}^l} \Delta z_{j}^l$ となる。
    * これは $\Delta z_{j}^l$ 変更分に対する $C$ の変化分 $\Delta C$ とは異なるが、 $\Delta z_{j}^l$ が十分小さい変化分と考えると、変化分は同じと近似して考えることができる。
* コスト関数の変化分 $\frac{\partial C}{\partial z_{j}^l} \Delta z_{j}^l$ がコストを減少させる値を取るように、 $\Delta z_{j}^l$ を選ぶことを考える。
* $\partial C / \partial z_{j}^l$ が正負によらず、大きな値だとすると、 $\Delta z_{j}^l$ は $\partial C / \partial z_{j}^l$ と逆の符号の値を選ぶことで、 $\frac{\partial C}{\partial z_{j}^l} \Delta z_{j}^l$ は必ず負の値となり、その大きさは $\partial C / \partial z_{j}^l$ を2乗した大きさとなる。これにより、コストの大きな減少(改善)が行なうことができる。
* 一方、 $\partial C / \partial z_{j}^l$ が正負によらず、小さな値(0に近い値)だとすると、 $\Delta z_{j}^l$ として逆の符号の値を選んだとしても、その値の大きさは小さく、あまりコストを改善することができないと言える。
* ただ、 その場合、既にそのニューロンが最適な状態に近いとも言える。
* 上記から $\partial C / \partial z_{j}^l$ はニューラルネットワークの誤差を測定している、と考えることができる。
    * この値が大きいほど、まだニューラルネットワークの出力と本来目標としている出力に差があり、誤差が大きい状態と言えるためである。
* 以上より、第 $l$ 層の $j$ 番目のニューロンの誤差 $\delta_{j}^l$ を以下のように定義する。
$$
\delta_{j}^l \equiv \frac{\partial C}{\partial z_{j}^l}  \tag{29}
$$
* $\boldsymbol{\delta^l}$ は第 $l$ 層の各ニューロンの誤差を要素として持つベクトルを表わし、逆伝播により、各層でこのベクトルの要素を計算する。
* ここで、なぜこれまで重み付き入力 $z_{j}^l$ に小さな変更 $\Delta z_{j}^l$ を加えてきたのかを考える。
* 重み付き入力 $z_{j}^l$ ではなく、ニューロンの出力である $a_{j}^l$ を変化させ、 $\partial C / \partial a_{j}^l$ をニューラルネットワークの誤差の指標とする方が自然なように考えられる。
* 実際、そのようにしても今後の議論を進めることは可能だが、誤差逆伝播の表示が数学的に複雑になってしまう。
* よってここでは、ニューラルネットワークの誤差の指標として、 $\delta_{j}^l = \partial C / \partial z_{j}^l$ を用いることにする。
* 逆伝播アルゴリズムは4つの基本的な式を基礎とし、これらを組み合わせることで、誤差 $\delta_{j}^l$ とコスト関数の勾配を計算することが可能となる。
* ただ、これらの式をすぐに理解するのは難しい。内容が豊富で相当の時間と忍耐で徐々に理解していく必要がある。

###### 出力層での誤差 $\boldsymbol{\delta^L}$ に関する式：
* 出力層での誤差 $\boldsymbol{\delta^L}$ の各要素 $\delta_{j}^L$ は以下のように表すことができる(大文字 $L$ はニューラルネットワークの最後の層である出力層を表す)。
$$
\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \sigma'(z_{j}^L)  \tag{BP1}
$$
* これは、 $\delta_{j}^{l}$ の定義である式(29)から以下の手順で導出することができる。
* 出力層の $j$ 番目のニューロンの出力誤差 $\delta_{j}^L$ は式(29)より、以下のようになる。
$$
\delta_{j}^L = \frac{\partial C}{\partial z_{j}^L}　　\tag{36}
$$
* これに多変数関数の微分の連鎖律を適用すると、 $\delta_{j}^L$ を出力層の各ニューロンの出力ベクトル $\boldsymbol{a^L}$ に関する偏微分で書き直すことができる。
* さらに各ニューロンの出力ベクトルを要素での表記 $a_{k}^{L}$ を用いて表すと以下のようになる。
$$
\delta_{j}^L = \Sigma_{k} \frac{\partial C}{\partial a_{k}^L} \frac{\partial a_{k}^L}{\partial z_{j}^L}  \tag{37}
$$
* 上記の $\Sigma$ による和は、出力層の $k$ 個全てのニューロンについて偏微分及びその微分の連鎖律を足し合わせている。
* $k=j$ のとき、 $k$ 番目のニューロンの出力 $a_{k}^L$ は  $a_{j}^L$ と書き表され、$j$ 番目のニューロンの重み付き入力 $z_{j}^L$ にのみ依存する。
    * 式(23)より、$a^{L}_{j}  =  \sigma(\Sigma_{k} w^{L}_{jk} a^{L-1}_{k} + b^{L}_{j}) = \sigma(z_{j}^{L})$ となり、 $\delta_{j}^L$ は $z_{j}^L$ にのみ依存することがわかる。
* 逆に $k \neq j$ のとき、 $\partial a_{k}^L / \partial z_{j}^L$ の値は $0$ となる。
* これを踏まえると、式(37)は $k=j$ の項のみが残り、以下のように簡略化して書くことができる。
$$
\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \frac{\partial a_{j}^L}{\partial z_{j}^L}  \tag{38}
$$
* 上述の通り、式(23)は以下のように表すことができる。
$$
a^{L}_{j}  =  \sigma(\Sigma_{k} w^{L}_{jk} a^{L-1}_{k} + b^{L}_{j}) = \sigma(z_{j}^L)  \tag{100}
$$
* 式(100)から $z_{j}^L$ に対する $\sigma(z_{j}^L)$ の偏微分 $\sigma'(z_{j}^L)$ を求めると、以下になる。
$$
\sigma'(z_{j}^L) = \frac{\partial a^{L}_{j}}{\partial z_{j}^L}  \tag{101}
$$
* 式(101)を式(38)に適用すると、以下になり、式(BP1)が導出される。
$$
\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \sigma'(z_{j}^L)  \tag{39}
$$
* 式(BP1)からわかることは以下の通りとなる。
* $\partial C / \partial a_{j}^L$ はコスト関数の値が $j$ 番目のニューロンの出力 $a_{j}^L$ に対して、どの程度敏感に変化するかの程度を示していると言える。
    * $\partial C / \partial a_{j}^L$ が大きければ、コスト関数 $C$ は $a_{j}^L$ に対して敏感に反応し、大きく変化することを意味する。その場合、　$\delta_{j}^L$ も正負を問わず、大きな値となる。
* $\sigma'(z_{j}^L)$ は活性化関数 $\sigma$ の $z_{j}^L$ に対する導関数のため、 活性化関数 $\sigma$ が重み付き入力 $z_{j}^L$ の変化にどの程度敏感に変化するかを示すものである。
* 式(BP1)を構成する項は比較的簡単に計算できる。
    * ニューラルネットワークの挙動を計算する過程で $z_{j}^L$ は計算され、若干の計算時間追加により、 $\sigma'(z_{j}^L)$ も計算できる。
    * $\partial C / \partial a_{j}^L$ の計算はコスト関数の形に依存するが、コスト関数が既知ならば、計算が困難になることはない。
    * 例えば、コスト関数が式(27)のような2乗誤差コスト関数の場合、 $C = \frac{1}{2} \sum_{j}^{} (y_{j} - a_{j}^L)^2$ に対し、 $\partial C / \partial a_{j}^L = (a_{j} - y_{j})$ となり、簡単に計算ができることがわかる。
* 式(BP1)を各要素に関する表記から行列を用いた表記にすると、以下の通りとなる。
$$
\delta^L = \nabla_{a} \boldsymbol{C} \circ \sigma'(z^L)  \tag{BP1a}
$$
* $\nabla_{a} \boldsymbol{C}$ は偏微分 $\partial C / \partial a_{j}^L$ を要素として持つベクトルである。
* 各要素ごとの表記と同様、 $\nabla_{a} \boldsymbol{C}$ はニューロンの出力に対するコスト関数の変化率とみなすことができる。
* ここでも2乗誤差コスト関数に関して考えると、 $\nabla_{a} \boldsymbol{C} = (\boldsymbol{a^L} - \boldsymbol{y})$ となり、行列方式で式(BP1a)を表記すると、以下のようになる。
$$
\delta^L = (\boldsymbol{a^L} - \boldsymbol{y}) \circ \sigma'(z^L)  \tag{30}
$$

###### 誤差 $\boldsymbol{\delta^l}$ の次層での誤差 $\boldsymbol{\delta^{l+1}}$ に関する表式：
* 第 $l$ 層のニューロンの誤差 $\boldsymbol{\delta^l}$ は第 $l+1$ 層のニューロンの誤差 $\boldsymbol{\delta^{l+1}}$ を用いて、以下のように表すことができる。
$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \circ \sigma'(z^l)  \tag{BP2}
$$
* これも、式(29)から以下の手順で導出することができる。
$$
\delta_{j}^l \equiv \frac{\partial C}{\partial z_{j}^l}  \tag{40}
$$
* 上式に多変数関数の微分の連鎖律を適用し、 $\delta_{j}^{l+1} = \partial C / \partial z_{j}^{l+1}$ を用いて書き直すと、以下のようになる。
$$
\delta_{j}^l \equiv \frac{\partial C}{\partial z_{j}^l} = \Sigma_{k} \frac{\partial C}{\partial z_{j}^{l+1}} \frac{\partial z_{j}^{l+1}}{\partial z_{j}^l} = \Sigma_{k} \frac{\partial z_{j}^{l+1}}{\partial z_{j}^l} \delta_{j}^{l+1}   \tag{41-42}
$$
* 次に第 $l+1$ 層の $j$ 番目のニューロンの重み付き入力 $z_{j}^{l+1}$ を式(23)(25)を用い、以下のように変形する。
$$
z_{j}^{l+1} = \Sigma_{k} w_{jk}^{l+1} a_{k}^{l} + b_{j}^{l+1} = \Sigma_{j} w_{jk}^{l+1} \sigma(z_{k}^l) + b_{j}^{l+1}    \tag{43}
$$
* この式を $z_{j}^l$ について微分すると、 $z_{j}^l$ の項のみ残り、それ以外の項の微分は全て0となるため、以下の通りとなる。
$$
\frac{\partial z_{j}^{l+1}}{\partial z_{j}^l} = w_{jk}^{l+1} \sigma'(z_{j}^l)     \tag{44}
$$
* 式(41-42)を式(44)を使って書き換えると、以下のようになる。
$$
\delta_{j}^l  = \Sigma_{k} w_{jk}^{l+1} \delta_{j}^{l+1} \sigma'(z_{j}^l)   \tag{45}
$$
* この式を添え字 $k$, $j$ を用いずに表記すると、式(BP2)となる。
    * ここでは、 $k$ は第 $l$ 層の任意のニューロンを表し、 $j$ は第 $l+1$ 層の任意のニューロンを表している。
    * 行列形式の表記にするあたり、これらは行列の各成分として表されるようになる。
* 式(BR2)の $(w^{l+1})^T$ は第 $l+1$ 層の重み行列 $w^{l+1}$ の転置行列である。
* 第 $l+1$ 層の誤差 $\delta^{l+1}$ が既知とすると、それに $(w^{l+1})^T$ を掛ける操作は、直感的には誤差をネットワークの逆方向に伝播させていると考えることができる。
* この点からも $\delta^l$ は第 $l$ 層の出力の誤差を測る指標の1つと言える。
* $\delta^{l+1}$ に第 $l$ 層の重み転置行列を掛けたあと、 $\sigma'(z^l)$ とのアダマール積をとっている。
* この処理によって、第 $l$ 層の活性化関数を通して、誤差を更に逆方向に伝播させることになる。
* 上記でまとめた行列形式の式(BP1a)、式(BP2)を組み合わせることで、ニューラルネットワークの任意の第 $l$ 層のニューロンの誤差ベクトル $\boldsymbol{\delta^l}$ を計算することが可能となったと言える。
$$
\delta^L = \nabla_{a} \boldsymbol{C} \circ \sigma'(z^L)  \tag{BP1a}
$$
$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \circ \sigma'(z^l)  \tag{BP2}
$$
* まず、出力層(ニューラルネットワークの最終層)における誤差 $\delta^L$ を式(BP1a)で計算する。
* 次に式(BP2)において $l+1=L$ として $\delta^{L-1}$ を計算する。同様に式(BP2)を用い、  $l+1=L-1$ として $\delta^{L-2}$ を計算する。
* これを繰り返すことでニューラルネットワークを逆向きに辿り、各層におけるニューロンの誤差ベクトル $\delta^{l}$ を計算することができる。

###### 任意のバイアスに関するコストの変化率の式：
* 第 $l$ 層の $j$ 番目の任意のニューロンのバイアス $b_{j}^l$ に対するコスト関数の変化率はそのニューロンの出力誤差で表すことができる。
$$
\frac{\partial C}{\partial b_{j}^l} = \delta_{j}^l      \tag{BP3}
$$
* この式も同様に式(29)から導出できる。多変数関数の微分の連鎖律を適用すると、以下のように書き直すことができる。
$$
\delta_{j}^l \equiv \frac{\partial C}{\partial z_{j}^l} = \frac{\partial C}{\partial b_{j}^l} \frac{\partial b_{j}^l}{\partial z_{j}^l}   \tag{102}
$$
* また、式(23)より、$z_{j}^{l} = \Sigma_{k} w^{l}_{jk} a^{l-1}_{k} + b^{l}_{j}$ なので、 $b^{l}_{j} = z_{j}^{l} - \Sigma_{k} w^{l}_{jk} a^{l-1}_{k}$ と表すことができる。
* ここで $b^{l}_{j}$ の $z_{j}^{l}$ に対する偏微分 $\partial b^{l}_{j} / \partial z_{j}^{l}$ は $\Sigma_{k} w^{l}_{jk} a^{l-1}_{k}$ 部分が定数とみなされ、 $0$ となるため、 $1$ となる。
* これを式(102)に適用することで、式(BP3)が得られる。
* 式(BP3)は行列形式の表記にすると、以下の通り、簡潔に書くことができる。
* ただし、 $\boldsymbol{\delta}$ の各成分はコストを同じ層の各ニューロンのバイアス $\boldsymbol{b}$ で偏微分した値と考える。
$$
\frac{\partial \boldsymbol{C}}{\partial \boldsymbol{b}} = \boldsymbol{\delta}      \tag{31}
$$

###### 任意の重みについてのコストの変化率：
* $w_{jk}^l$ (第 $l-1$ 層の $k$ 番目のニューロンから第 $l$ 層の $j$ 番目のニューロンをつなぐ重み)に対するコスト関数の変化率は $a_{k}^{l-1}$ と $\delta_{j}^l$ で以下のように表すことができる。
$$
\frac{\partial C}{\partial w_{jk}^l} = a_{k}^{l-1} \delta_{j}^l      \tag{BP4}
$$
* この式も同様に式(29)から導出する。多変数関数の微分の連鎖律を適用すると、以下のように書き直すことができる。
$$
\delta_{j}^l \equiv \frac{\partial C}{\partial z_{j}^l} = \frac{\partial C}{\partial w_{jk}^l} \frac{\partial w_{jk}^l}{\partial z_{j}^l}   \tag{103}
$$
* 左辺・右辺を入れ替え、両辺に $\partial z_{j}^l / \partial w_{jk}^l$ を掛ける。
$$
\frac{\partial C}{\partial w_{jk}^l} \frac{\partial w_{jk}^l}{\partial z_{j}^l} \frac{\partial z_{j}^l}{\partial w_{jk}^l} = \frac{\partial z_{j}^l}{\partial w_{jk}^l} \delta_{j}^l   \tag{104}
$$
* 左辺は相殺され、 $\partial C / \partial w_{jk}^l$ のみが残り、右辺は式(23)より、 $z_{j}^{l} = \Sigma_{k} w^{l}_{jk} a^{l-1}_{k} + b^{l}_{j}$ から、両辺の $w_{jk}^l$ に対する偏微分を取る。
* $w^{l}_{jk}$ の項のみ残るため、以下の通りとなる。
$$
\frac{\partial z_{j}^{l}}{\partial w^{l}_{jk}} = a_{k}^{l-1}   \tag{105}
$$
* 式(105)を式(104)に適用することで式(BP4)が得られる。
* これまでに導出した逆伝播の基礎となる4つの式は以下の式(BP1)～式(BP4)となる。　※行列形式ではなく、各行列の要素ごとの形式で表記
$$
\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \sigma'(z_{j}^L)  \tag{BP1}
$$
$$
\delta_{j}^l  = \Sigma_{k} w_{jk}^{l+1} \delta_{j}^{l+1} \sigma'(z_{j}^l)      \tag{BP2(45))}
$$
$$
\frac{\partial C}{\partial b_{j}^l} = \delta_{j}^l      \tag{BP3}
$$
$$
\frac{\partial C}{\partial w_{jk}^l} = a_{k}^{l-1} \delta_{j}^l      \tag{BP4}
$$
* これらの式の証明は一見複雑だが、偏微分の連鎖律を適用した結果に過ぎない。
* さらに言えば、逆伝播アルゴリズムは多変数関数の微分で利用される連鎖律をシステマチックに適用することで、コスト関数の勾配を計算する方法とみなすことができる。
* 式(BP4)を添え字を外し、一般的な式で表すと、以下のように表記できる。
$$
\frac{\partial C}{\partial w} = a_{in} \delta_{out}      \tag{32}
$$
* ここで $a_{in}$ は重み $w$ を持つ枝に対する前層のニューロンの活性出力で、 $\delta_{out}$ はその枝に対する出力ニューロンの持つ誤差である。
* 式(32)から $a_{in}$ が小さい($a_{in} \approx 0$)とき、勾配 $\partial C / \partial w$ も小さくなることがわかる。このような状態を「重みの学習が遅い」と表現する。
* つまり、重みの学習が遅い場合、勾配降下法を行っている間にコスト値が重みの微小変化に対し、大きく変化しないことを意味する。
* 言い換えれば、式(BP4)からわかることの1つとして、活性出力の低いニューロンからの入力を受け取ると重みは学習が遅いと言える。
* 式(BP1)～式(BP4)から分かることは他にもある。
* まず、出力層について、式(BP1)の $\sigma'(z_{j}^L)$ の項に注目する。
* 活性化関数 $\sigma$ をシグモイド関数とすると、ニューロンの活性出力 $a_{j}^L = \sigma(z_{j}^L)$ が $0$ or $1$ に近づくと、関数 $\sigma$ の変化は平坦になり、これは $\sigma'(z_{j}^L) \approx 0$ を意味する。
* これより、活性出力が低い($\approx 0$)、もしくは活性出力が高い($\approx 1$)場合、最終層の学習は遅いことがわかる。
    * $\sigma'(z_{j}^L) \approx 0$ の場合、式(BP1)より、 $\delta_{j}^L = 0$ となる。
    * これを式(BP4)もしくは式(32)に $l=L$ として適用すると、勾配 $\partial C / \partial w \approx 0$ となる。
* このような状態は出力ニューロンが飽和し、重みの学習が終了している、もしくは重みの学習が遅い、と表現する。
* 同じことが出力ニューロンのバイアスに対しても成立する。
* 出力層より前の層でも似た考察ができる。式(BP2)の $\sigma'(z_{j}^l)$ の項に注目する。
* ニューロンが飽和状態の場合、 $\sigma'(z_{j}^l) \approx 0$ となり、 $\delta_{j}^l = 0$ となる。
* このような場合、ここからも飽和状態のニューロンに入力される重みの学習も遅くなると言える。
* 式(32)をまとめると、入力ニューロン $a_{in}$ の活性出力が低い状態か、出力ニューロン $\delta_{out}$ が飽和状態(活性出力が低い or 高い)の場合、重みの学習が遅いと言える。
* 上述の考察は全て逆向きに利用することができる。
* また、逆伝播の基礎となる4つの式は任意の活性化関数について成立する。これは前述の各式の証明で活性化関数 $\sigma$ の特別な性質を用いていないことからもわかる。
* よって、これらの式は好きな学習特性を持つ活性化関数を設計して使用したり、既存の任意の活性化関数に対して使用することが可能である。
* 例えば、シグモイド関数ではない関数を活性化関数 $\sigma$ として選び、その関数の $\sigma'$ が常に正で0に漸近しないものとする。
* この場合、 $\sigma$ は常に単調増加で0に漸近しないので、シグモイド関数を用いた場合に発生するニューロンの飽和状態が原因で、学習が減速してしまうことを防ぐことができる。
* 式(BP1)～式(BP4)の4つの式を理解しておくことで、なぜそのような修正を行うのか、修正によりどのような影響が起こるかを説明することに役立つ。

##### 逆伝播アルゴリズム
* 逆伝播の基礎となる4つの式により、コスト関数の勾配の計算が可能となった。
* その方法を具体的にアルゴリズムの形に書き下してみると以下の通りとなる。
    1. 入力 $x$ :
        * 入力層に対応する活性出力 $a^1$ をセットする。
    2. フィードフォワード :
        * 各 $l=2,3,...,L$ に対し、 $z^{l}$, $a^{l}$ を計算する。
            * $z^{l}$ は $z^{l} = w^{l} a^{l-1} + b^{l}$ で計算できる
            * $a^{l}$ は 式(25)より、 $a^{l}  =  \sigma(w^{l} a^{l-1} + b^{l}) = \sigma(z^{l})$ で計算できる
            * $a^{1}$ は入力としてセットされた値を使用し、順に $z^2$, $a^2$, $z^3$, $a^3$,..., $z^L$, $a^L$ をネットワークの順方向に計算していく。
    3. 誤差 $\delta^{L}$ を出力 :
        * ネットワークの最終層(出力層)の誤差ベクトル $\delta^L$ を $z^{L}$, $a^{L}$ を使って計算する。
            * 行列要素表示形式の式(BP1)より、 $\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \sigma'(z_{j}^L)$
            * 行列形式の式(BP1a)より、 $\delta^L = \nabla_{a} \boldsymbol{C} \circ \sigma'(z^L)$
    4. 誤差を逆伝播 :
        * 各 $l=L-1,L-2,...,2$ に対し、 $\delta^l$ を $z^{l}$, $a^{l}$,  $\delta^L$ を使って計算する。
            * 行列要素表示形式の式(45)より、 $\delta_{j}^l  = \Sigma_{k} w_{jk}^{l+1} \delta_{j}^{l+1} \sigma'(z_{j}^l)$
            * 行列形式の式(BP2)より、 $\delta^l = ((w^{l+1})^T \delta^{l+1}) \circ \sigma'(z^l)$
            * $\delta^{L}$ を計算した後、順に $\delta^{L-1}$, $\delta^{L-2}$,..., $\delta^{2}$ をネットワークの逆方向に計算していく。
    5. 出力：
        * コスト関数の勾配 $\partial C / \partial w_{jk}^l$ , $\partial C / \partial b_{j}^l$ を式(BP4), 式(BP3)から計算する。
* 上記のアルゴリズムでは、最終層から逆向きに誤差ベクトル $\delta^l$ を計算していることからも逆伝播アルゴリズムと呼ばれる理由が分かる。
* ネットワークを逆向きに辿るのは奇妙かもしれないが、前段の重みやバイアスにより、コストがどのように変化していくかを見るためには、連鎖律を繰り返し適用しなければならず、欲しい計算式を得るためにはネットワークを逆方向に辿る必要がある。
* 前述の通り、逆伝播のアルゴリズムは単一の訓練データに対するコスト $C=C_x$ の勾配を計算する。
* ただ実際の実装では逆伝播のアルゴリズムを確率的勾配降下法など多数の訓練データに対する勾配を計算する学習アルゴリズムと組み合わせるのが一般的である。
* 以下のアルゴリズムでは、 $m$ 個の訓練データからなるミニバッチに対して勾配降下法を適用して学習を行っているものになる。
    1. 訓練データのセットを入力
    2. 各訓練データ $x$ に対して、対応するニューロンの活性出力 $a^{x,1}$ をセットし、以下のステップを行なう。
        1. フィードフォワード :
            * 各 $l=2,3,...,L$ に対し、 $z^{x,l}$, $a^{x,l}$ を計算する。
        2. 誤差 $\delta^{x,L}$ を出力
            * 誤差ベクトル $\delta^{x,L}$ を $z^{x,l}$, $a^{x,l}$ を使って計算する。
        3. 誤差を逆伝播：
            * 各 $l=L-1,L-2,...,2$ に対し、 $\delta^{x,l}$ を $z^{x,l}$, $a^{x,l}$,  $\delta^{x,L}$ を使って計算する。
    3. 勾配降下：
        * ニューラルネットワークの各層 $l=L-1,L-2,...,2$ に対し、重みとバイアスを更新する
        * 式(20)より、第 $l$ 層の $k$ 番目のニューロンへの重み $w_{k}^{l}$ の勾配降下は、訓練データのセット $x=1,2,...,m$ の各コスト $C_{x}$ の $w_{k}$ に対する変化率 $\partial C_{x} / \partial w_{k}$ を用い、以下のようになる。
        $$
        w_{k}^{l} \rightarrow w_{k}^{l} - \frac{\eta}{m} \Sigma_{x}^{m} \frac{\partial C_{x}}{\partial w_{k}^l} \tag{106}
        $$
        * 式(BP4)より、訓練データのセット $x=1,2,...,m$ の各コスト $C_{x}$ の $w_{k}$ に対する変化率 $\partial C_{x} / \partial w_{k}$ は以下のようになる。
        $$
        \frac{\partial C_{x}}{\partial w_{k}^l} = a_{k}^{x,l-1} \delta^{x,l}      \tag{107}
        $$
        * 式(106)に式(107)を適用することにより、重みの勾配降下は以下のようになる。
        $$
        w_{k}^{l} \rightarrow w_{k}^{l} - \frac{\eta}{m} \Sigma_{x}^{m} a_{k}^{x,l-1} \delta^{x,l}  \tag{108}
        $$
        * 式(21)より、第 $l$ 層のニューロンのバイアス $b^{l}$ の勾配降下は、訓練データのセット $x=1,2,...,m$ の各コスト $C_{x}$ の $b^{l}$ に対する変化率 $\partial C_{x} / \partial b^{l}$ を用い、以下のようになる。
        $$
        b^{l} \rightarrow b^{l} - \frac{\eta}{m} \Sigma_{x}^{m} \frac{\partial C_{x}}{\partial b^{l}}  \tag{109}
        $$
        * 式(BP3)より、訓練データのセット $x=1,2,...,m$ の各コスト $C_{x}$ の $b^{l}$ に対する変化率 $\partial C_{x} / \partial b^{l}$ は以下のようになる。
        $$
        \frac{\partial C_{x}}{\partial b^l} = \delta^{x,l}      \tag{110}
        $$
        * 式(109)に式(110)を適用することにより、バイアスの勾配降下は以下のようになる。
        $$
        b^{l} \rightarrow b^{l} - \frac{\eta}{m} \Sigma_{x}^{m} \delta^{x,l}  \tag{111}
        $$
* 実際に確率的勾配降下法を実装する場合、訓練データのミニバッチを作成するための外部ループと複数回のエポックで訓練を繰り返すための外部ループが必要となる。
* 簡略化のため、上記のアルゴリズムにこれらは含んでいない。

##### 逆伝播の実装
* ここまでで逆伝播の理論が理解できたことで、前述の実装に利用したコードも理解できるようになる。
* 逆伝播の実装はNetworkクラスのupdate_mini_batchメソッドとbacpropメソッドに含まれていた。
* これらのメソッドは前述のアルゴリズムをそのままコードにしたものになっている。
* 以下のupdate_mini_batchメソッドは現在の訓練データのmini_batchについての勾配を計算し、Networkクラスの重みとバイアスを更新している。

```
class Network():
...
    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                        for b, nb in zip(self.biases, nabla_b)]
```

* 多くの処理は ```delta_nabla_b, delta_nabla_w = self.backprop(x, y)``` で行なわれている。
* backpropメソッドで偏微分 $\partial C_{x} / \partial b_{j}^l$ , $\partial C_{x} / \partial w_{jk}^l$ を計算しており、前述のアルゴリズムに従って実装されている。
* 一部、Pythonの負数の添え字を使用してリストの後ろから要素を参照する機能を使用しているため、前述のアルゴリズムと実装が異なる部分がある。
* 加えて、以下がbackpropメソッドの実装コードとなる。 $\sigma$ 関数とそのベクトル化、$\sigma$ 関数の導関数とそのベクトル化、及びコスト関数の微分計算のためのヘルパー関数も含む。

```
class Network():
...
   def backprop(self, x, y):
        """コスト関数の勾配を表すタプル"(nabla_b, nabla_w)"を返却する。
        "self.biases" and "self.weights"と同様に、
        "nabla_b"と"nabla_w"はnumpyのアレイのリストで
        各要素は各層に対応する。"""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # 順伝播
        activation = x
        activations = [x] # 層ごとに活性を格納するリスト
        zs = [] # 層ごとにzベクトルを格納するリスト
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid_vec(z)
            activations.append(activation)
        # 逆伝播
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime_vec(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # 下記のループ変数lは第2章での記法と使用方法が若干異なる。
        # l = 1は最終層を、l = 2は最後から2番目の層を意味する（以下同様）。
        # 本書内での方法から番号付けのルールを変更したのは、
        # Pythonのリストでの負の添字を有効活用するためである。
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            spv = sigmoid_prime_vec(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * spv
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
...
    def cost_derivative(self, output_activations, y):
        """出力活性に対する偏微分\partial C_x / \partial a
        のベクトルを返却する。"""
        return (output_activations-y)

def sigmoid(z):
    """シグモイド関数"""
    return 1.0/(1.0+np.exp(-z))

sigmoid_vec = np.vectorize(sigmoid)

def sigmoid_prime(z):
    """シグモイド関数の導関数"""
    return sigmoid(z)*(1-sigmoid(z))

sigmoid_prime_vec = np.vectorize(sigmoid_prime)
```

##### 逆伝播が速いアルゴリズムであるとはどういう意味か？
* 上述の内容を踏まえ、逆伝播アルゴリズムは高速なアルゴリズムなのかを確認するために、勾配を計算する別のアプローチを考えてみる。
* コスト関数の勾配を計算する方法として、微積分学を用い、勾配の計算に連鎖律が使用できるかを検討してみる。
* ただ少し計算してみると、式が複雑になり、別のアプローチを探す。
* コスト関数を重みのみの関数とみなし、 $C=C(w)$ と考えるとする。
* 重みを $w_{1}, w_{2}, ...$ とし、特定の重み $w_{j}$ について、 $\partial C / \partial w_{j}$ を計算する。
$$
\frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_{j}) - C(w)}{\epsilon}      \tag{46}
$$
* 式(46)の近似を考える。ここで、 $\epsilon > 0$ は微小な正の数で、 $e_{j}$ は $j$ 方向の単位ベクトルとする。
* これは少し異なる $w_{j}$ でコスト $C$ を計算し、式(46)を適用することで、 $\partial C / \partial w_{j}$ を計算することを意味する。
* 同様の手法はバイアスについての偏微分 $\partial C / \partial b$ にも適用できる。
* この手法はよさそうに見える。発想がシンプルであり、実装も数行のコードで実現可能で簡単であるためです。
* むしろ微分の連鎖律を用いて勾配を計算する手法よりも有望に感じる。ただ、実際に実装して動かしてみるととても遅いことがわかる。
* その理由を理解するために、100万個の重みを持つニューラルネットワークを考えてみる。
* この場合、 $\partial C / \partial w_{j}$ を計算する過程で、各重み $w_{j}$ に対して、 $C(w+\epsilon e_{j})$ の計算が必要となる。
* これは全ての勾配の計算には $C(w)$ を1回、 $C(w)$ 以外のコスト関数計算が100万回必要となり、合わせて100万1回のコスト関数計算が必要となることを意味している。
    * まず $z^{l} = w^{l} a^{l-1}$ , $a^{l} = \sigma(z^{l})$ から $z^{l}$ , $a^{l}$ を求め、式(27)のようなコスト関数を $a^{l}$ を使用して計算し、コストを計算する。
    * ここでは、コスト関数を重みのみの関数としているため、 $b^{l}$ は存在しない。
    * この計算は $l=2,3,...,L$ に対し、順に行われ、その方向は順伝播の方向となる。
    * 順伝播における一連の計算のうち、重み行列の掛け算が計算コスト上、支配的になっている。
* 逆伝播アルゴリズムは1回の順伝播とそのあと行われる1回の逆伝播で全ての偏微分 $\partial C / \partial w_{j}$ を計算できる。
* 逆伝播における一連の計算のうち、重み行列の転置の掛け算が計算コスト上、支配的になっている。
* つまり、順伝播と逆伝播の計算コストはほぼ同程度と言える。
* 逆伝播アルゴリズム全体における計算コストは順伝播2回分なのに対し、式(46)に基づく手法の計算コストは順伝播100万1回分であり、逆伝播アルゴリズムは複雑ではあるものの、とても高速なアルゴリズムであると言える。
* 逆伝播アルゴリズムの高速化の真価は1986年ごろに知られるようになり、ニューラルネットワークで解くことができる問題の幅を広げることができた。
* ただ逆伝播アルゴリズムは万能ではなく、特にディープニューラルネットワーク(隠れ層を多く持つニューラルネットワーク)の学習への適用においては、既に1980年代後半に壁にぶつかっていた。
* 現代のコンピュータや新しい別のアイデアにより、逆伝播アルゴリズムを用いてディープニューラルネットワークの学習は可能になっている。その実現方法は後述する。

##### 逆伝播：全体像
* 逆伝播アルゴリズムには2つの謎がある。
    * 1つはアルゴリズムが本当にやっていることは何か？ということである。
        * ニューロンの出力から誤差が逆伝播していく様子を見てきたが、さらに踏み込み、ベクトルに行列を掛けるときに何が起こっているかについてもっと直感的な理解を得られないか？
    * もう1つは、そもそも逆伝播アルゴリズムをどのように発見するか？ということである。
        * アルゴリズムの手順に従ったり、その正しさを示す証明を追うことは可能だが、それと問題を理解して0からアルゴリズムを発見することは別である。
        * 逆伝播アルゴリズムの発見に繋がる妥当な論理付けは何かないか？
* アルゴリズムの挙動に対し、ネットワーク上で起きる変化についての直感を養うために、ニューラルネットワーク内の適当な重み $w_{jk}^{l}$ に微小な変化 $\Delta w_{jk}^{l}$ を施すことを考える。
* 重みの微小変化 $\Delta w_{jk}^{l}$ により、対応するニューロンの出力が変化し、出力の微小な変化 $\Delta a_{j}^{l}$ が生じる。
* この微小変化はさらに次の層の全ての出力活性に変化を引き起こす。
* 最終的にはネットワークの最終層である出力層まで変化が引き起こされ、コスト関数の値に微小な変化 $\Delta C$ が生じる。
* このコスト関数の変化 $\Delta C$ は重みの微小変化 $\Delta w_{jk}^{l}$ と式(47)で関連付けることができる。
    * 縦軸 $C$ , 横軸 $w_{jk}^l$ のグラフにおいて、 グラフ上の任意の点における接線の傾きは $\partial C / \partial w_{jk}^l$ となる。この接線に沿って、 $w_{jk}^l$ 方向に $\Delta w_{jk}^l$ 移動した場合の $C$ の増加分は $\frac{\partial C}{\partial w_{jk}^l} \Delta w_{jk}^l$ となる。
    * これは $\Delta w_{jk}^l$ 変更分に対する $C$ の変化分 $\Delta C$ とは異なるが、 $\Delta w_{jk}^l$ が十分小さい変化分と考えると、変化分は同じと近似して考えることができる。
$$
\Delta C \approx \frac{\partial C}{\partial w_{jk}^{l}} \Delta w_{jk}^{l}       \tag{47}
$$
* $w_{jk}^{l}$ の微小変化がニューラルネットワークを伝播し、その結果、 $C$ の微小変化を引き起こす様子を丁寧に追跡することを考える。
* もしそれができれば、伝播経路の途中にある全てを簡単に計算できる変数で表現し、 $\partial C / \partial w_{jk}^{l}$ を計算できるはずである。
* 重みの微小変化 $\Delta w_{jk}^{l}$ により、対応する第 $l$ 層の $j$ 番目のニューロンの出力が変化し、出力の微小な変化 $\Delta a_{j}^{l}$ が生じるとすると、以下の通り、近似できる。
$$
\Delta a_{j}^{l} \approx \frac{\partial a_{j}^{l}}{\partial w_{jk}^{l}} \Delta w_{jk}^{l}       \tag{48}
$$
* ニューロン出力の微小変化 $\Delta a_{j}^{l}$ は次の第 $l+1$ 層の全てのニューロンの出力に変化を引き起こすことになる。
* これらの変化を引き起こされる第 $l+1$ 層のニューロンのうちの1つを考える。
* そのニューロンの活性出力を $a_{q}^{l+1}$ とすると、その微小変化 $\Delta a_{q}^{l+1}$ は以下のように近似できる。
$$
\Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \Delta a_{j}^{l}       \tag{49}
$$
* 式(49)に式(48)を適用すると、以下が得られる。
$$
\Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{jk}^{l}} \Delta w_{jk}^{l}       \tag{50}
$$
* 式(50)により、重みの微小変化 $\Delta w_{jk}^{l}$ に対する第 $l+1$ 層の1つのニューロンの活性出力の微小変化 $\Delta a_{q}^{l+1}$ との関係を表すことができる。
* 次に $\Delta a_{q}^{l+1}$ は次の第 $l+2$ 層のニューロン活性出力に変化を引き起こす。
* このように $w_{jk}^{l}$ から $C$ までのパスの1つを考えると、そのパス内ではニューロン活性出力のそれぞれの変化が次のニューロン活性出力を変化させ、最終的に出力層でのコストの変化を引き起こしていると言える。
* このパスが $a_{j}^{l}, a_{q}^{l+1}, ..., a_{n}^{L-1}, a_{m}^{L}$ を通るとすると、以下が得られる。
    * 式(49)に式(48)を適用し、式(50)を導出した過程を連続的に行い、重みの微小変化 $\Delta w_{jk}^{l}$ に対する最終層(第 $L$ 層)の1つのニューロン活性出力の微小変化( $=\Delta C$ )との関係を表す。
$$
\Delta C \approx \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}}・・・\frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{jk}^{l}} \Delta w_{jk}^{l}       \tag{51}
$$
* 上式より、各層のニューロンを通過するごとに $\partial a / \partial a$ の形の項が追加され、最後の出力層では $\partial C / \partial a_{m}^{L}$ の項が追加されることがわかる。
* 一方、これは $C$ の微小変化のうち、 $a_{j}^{l}, a_{q}^{l+1}, ..., a_{n}^{L-1}, a_{m}^{L}$ のパスを通った場合の微小変化のみを表している。
* $C$ の微小変化の合計を計算するには、微小変化が生じた最初の重みから最後のコストの間で取り得る全てのパスについての和を取ればよいので、以下のようになる。
$$
\Delta C \approx \Sigma_{m,n,p...q} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}}・・・\frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{jk}^{l}} \Delta w_{jk}^{l}       \tag{52}
$$
* 式(47)を上式に適用し、 $\partial C / \partial w_{jk}^{l}$ を表現すると、以下のようになる。
$$
\frac{\partial C}{\partial w_{jk}^{l}} = \Sigma_{m,n,p...q} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}}・・・\frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{jk}^{l}}       \tag{53}
$$
* 式(53)は複雑そうに見えるが、直感的なよい解釈がある。
    * ニューラルネットワーク内の重みに対する $C$ の変化率を計算する場合、そのネットワーク内の2つのニューロンを繋ぐ全ての枝にその変化の因子が付随していることがこの式から理解できる。
    * その因子とは一方のニューロンの活性出力に関するもう一端のニューロンの活性出力による偏微分であると言える。
    * ただし、最初の重みから次の層のニューロンに接続している枝の始点はニューロンに接続していないが、この枝に対する変化率の因子は $\partial a_{j}^{l} / \partial w_{jk}^{l}$ としている。
    * また、1つのパスに対する変化率の因子は単純にパス内に含まれる全ての変化率に付随する各枝の因子を掛け合わせたものになっている。
    * $\partial C / \partial w_{jk}^l$ に対する変化率の合計は全てのパスについての変化率の因子を足し合わせたものになっている。
* この直感的な解釈により、ニューラルネットワーク内の重みを微小変化させたときに何が起きるかを発見的に考察することができる。
* もう1つの謎、つまり0から誤差逆伝播アルゴリズムを発見するための方法がないか？について考える。
* 以下、省略(これまでの説明が多少わかりずらいが短く簡略化されているため)。

### 第3章：ニューラルネットワークの学習の改善
* ゴルフを始めようとするとき、まずは基本スイングの練習にほとんどの時間を使い、それ以外のスイングの練習は少ししかできないのが普通である。
* 「それ以外のスイング」を身に付けるのは、基本スイングをベースに修正しながら組み立てるものである。
* 同様にこれまで逆伝播アルゴリズムに集中してきた。それは逆伝播が「基本スイング」であり、ニューラルネットワークにおけるほとんどの仕事を理解するための基本だからである。
* 本章では純粋な逆伝播の実装を改善し、ネットワークの学習方法を改善するテクニックを説明する。
* 説明するテクニックは以下の通り。
    * クロスエントロピーと呼ばれるより良いコスト関数
    * ネットワークを学習データによらず汎化するのに役立つ「正規化」法と呼ばれる4つの手法
        * L1正規化、L2正規化、ドロップアウト、人工的な学習データの伸張
    * ネットワークの中の重みを初期化するより良い方法
    * ネットワークに対して良いハイパーパラメータを選択するためのいくつかの発見的方法
    * それ以外のテクニック
* これらの議論はお互いに完全に独立しているので、必要に応じて読み飛ばすこともできる。
* また、多くのテクニックを動くコードとして実装しているので、それらを使うことにより、第1章で説明した手書き文字分類問題の結果を改善することができる。
* ただここで説明されるのは、ニューラルネットワークのために開発された多くの手法のうちのほんの一部である。
* 利用できる手法がたくさんありすぎる場合の最も良い入門の方法は、少ない手法を深く学ぶことである。
* 少ない手法のみをまずは学ぶことにより、その手法がそのまま役に立つというだけでなく、ニューラルネットワークを使うときに起こる問題に対する理解を深めることもできる。
* その結果、必要に応じて他のテクニックを即座に使えるようになるはずである。

##### クロスエントロピーコスト関数
* 人間ははっきりと自分の間違いを認識し、嫌な思いをすると早く学ぶことができる。一方で自分の間違いがはっきりしないと学ぶスピードは遅くなる。
* 入力の重み $w$ とバイアス $b$ を持つある1個のニューロンを考える。そのニューロンを入力1に対して出力0を返すように学習させる。
* 本来、これは単純なため、適切な重みとバイアスを手計算で求めることができ、学習アルゴリズムを使用するまでもないが、ここでは、最急降下法(＝勾配降下法：Gradient descent)を使って重みとバイアスを計算することを考える。
* 学習するための初期値として重み $w=0.6$ 、バイアス $b=0.9$ 選択し、その時のニューロンの出力は $0.82$ だとする。またコスト関数は前述の式(6)のような2次コスト関数を使用しているものとする。
* 学習では実際に勾配を計算し、その勾配を重みとバイアス値を更新していく。このときの学習率は $\eta=0.15$ とする。
* この場合、ニューロンは学習によって急速に重みとバイアスを更新して学習を行ない、コストを下げていく。結果としてニューロンからの出力は $0.09$ まで下がる。望まれる出力 $0$ とは差分があるが、よい結果が得られたと言える。
* 次に学習するための初期値として重み $w=2.0$ 、バイアス $b=2.0$ 選択したとする。コスト関数は同様に式(6)のような2次コスト関数で学習率は $\eta=0.15$ とする。
* この場合、学習のはじめのころはコストの減少分が少なく、学習がゆっくり始まる。実際には最初の150epochくらいまでは重みやバイアスがほとんど変わらない。
* その後、急速に学習のスピードが上がり、急速にニューロンの出力が0に近づいていく。
* この振る舞いは人間の学習による傾向とは異なる。
* 人間は間違えると、早く学習できるのに対し、ニューロンは間違えていた時(より適切ではない重み $w=2.0$ 、バイアス $b=2.0$ 選択した時)の方が学習に時間がかかっている。
* これはこの例のような単純なモデルだけではなく、一般的なニューラルネットワークでも起きることが知られている。
* この振る舞いにおける学習スピードの低下は何か？またそれを防ぐ手段はあるか？
* ここで、コスト関数の重みとバイアスによる偏微分 $\partial C / \partial w$ , $\partial C / \partial b$ によって重みとバイアスが更新されるとする。
* そうすると、「学習が遅い」ということはこの偏微分の値が小さいことを意味している。つまりなぜ偏微分の値が小さいかを理解することが必要である。
* 実際に偏微分を計算してみる。まず2次コスト関数は式(6)から以下のように与えられる。
$$
C = \frac{(y - a)^2}{2}     \tag{54}
$$
* $a$ は学習における入力 $x=1$ に対する実際のニューロンの出力値であり、 $y$ は期待されるニューロンの出力値である。
* 活性化関数がシグモイド関数 $\sigma$ とすると
























##### クロスエントロピーコスト関数の導入
