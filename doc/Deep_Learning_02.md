# 自然言語処理
* 日本語や英語など普段使っている言葉を自然言語(Natural Language)と言い、自然言語処理(NLP：Natural Language Processing)は自然言語を処理する分野、つまりコンピュータに人間の言葉を理解させるための技術のことである。
* 自然言語処理が扱う分野は多岐にわたるが、その本質はコンピュータに人間の言葉を理解させることにある。
* コンピュータが理解できる言語と言えば、プログラミング言語やマークアップ言語がある。
* それらの言語はコードの意味が一意に解釈できるように文法が定義されており、コンピュータはそのルールに従ってコードを解析する。
* 一般的なプログラミング言語は機械的で無機質であり、"固い言語"である。
* 一方、自然言語は同じ意味の文章でも様々な表現が可能であり、曖昧さがあったり、柔軟に意味や形が変わることから"柔らかい言語"であると言える。
* このような柔らかい言語である自然言語をコンピュータに理解させるのは難しい。
* 一方、その問題をクリアできれば、人にとって役立つことをコンピュータに行わせることができると言える。
* 実際にそのような例は多くあり、検索エンジンや機械翻訳、質問応答システム、文章の自動翻訳や感情分析などが挙げられる。
* 自然言語は文字によって構成され、また自然言語の言葉の意味は単語によって構成される。
* 単語は意味の最小単位であり、自然言語をコンピュータに理解させるためには、単語の意味を理解させることが重要であると言える。
* コンピュータに単語の意味を理解させるために、単語の意味をうまく捉えた以下の表現方法について考える。
    * シソーラスによる手法
    * カウントベースによる手法
    * 推論ベースの手法(word2vec)


# シソーラス
* 単語の意味を表す方法として、辞書のように1つ1つの単語に対してその意味を説明する方法が考えられる。
* このように単語の意味を定義することでコンピュータで単語の意味を理解する。
* ただし、これまでの自然言語処理の歴史では、人が使う辞書とは異なるシソーラス(thesaurus)と呼ばれるタイプの辞書が使われてきた。
* シソーラスは基本的には類語辞書であり、同じ意味の単語(同義語)、意味の似た単語(類義語)が同じグループに分類されている。
* 例えば、シソーラスを使うことでcarの同義語にはautomobileやmotocarなどが存在することがわかる。
* また、シソーラスでは、「上位と下位」、「全体と部分」などのより細かい関係性が単語の間で定義されている場合がある。
* 例えば、グラフ構造によって、各単語の関係性が定義されているケースがある。
* carという単語の上位概念としてmotor vehicleがあり、下位概念としてSUVやcompactがある。
* このように全ての単語に対して類義語の集合を作り、それぞれの単語の関係をグラフで表現することで、単語間の繋がりを定義することができる。
* この構造を使用することでコンピュータに単語間の関連性を教えることができる。


# 代表的なシソーラス - WordNet
* 自然言語処理分野におて、有名なシソーラスとして、WirdNetがある。
* WordNetはプリンストン大学で1985年に開発がスタートしており、これまで多くの研究に利用されてきた。
* WordNetを使うことで類義語を取得できたり、単語ネットワークを利用したりすることができる。
* さらに単語ネットワークを利用することで単語間の類似度を算出することも可能。


# シソーラスの問題点
* シソーラスにより、多くの単語に対して、同義語や階層構造の関係性を定義することによってコンピュータに単語の意味を教えることができる。
* 一方、それらを人の手でラベル付けすることには以下のような大きな欠点が存在する。
    * 時代変化に伴う対応が困難
        * 自然言語は生きており、絶えず新しい言葉が生まれ、古い言葉は忘れ去られる。
        * また時代変化に伴い、言葉の意味が変化する場合もある。
        * そのような単語の変化に対応するためにはシソーラスを人手で絶えず更新する必要がある。
    * 人手によるラベル付けのコストが高い
        * シソーラスを作るためには多くのコストがかかる。
        * 現存する英単語の総数は1000万語を超えると言われているが、それら全てに単語の関連づけをする必要がある。
    * 単語の細かいニュアンスを表現できない
        * シソーラスでは類義語として似た単語をグループ化するが、実際には似た単語であっても、それぞれニュアンスが少し異なる。
        * 単語ごとの微妙なニュアンスの差異はシソーラスでは表すことができない。
* 上記の通り、シソーラスを使う手法(単語の意味を人手によって定義する方法)には大きな問題がある。
* この問題を避けるため「カウントベースの手法」、ニューラルネットワークを使った「推論ベースの手法」がよく用いられる。
* これらの手法では、大量のテキストデータから自動的に単語の意味を抽出する。


# カウントベースの手法
* カウントベースの手法ではコーパス(corpus)を使用する。
* コーパスとは大量のテキストデータであるが、やみくもに集められたテキストデータではなく、自然言語処理の研究やアプリケーション開発のために目的をもって収集されたテキストデータのことを呼ぶ.
* コーパスはテキストデータに過ぎず、含まれる文章は人の手によって書かれたものである。
* これはコーパスに自然言語に対する人間の知識(文章の書き方、単語の選び方、単語の意味)が含まれていることを意味する。
* カウントベースの手法では人間の知識が詰まったコーパスから自動的かつ効率的にそのエッセンスを抽出することを行う。
* 自然言語処理分野で用いられるコーパスにはテキストデータに対してさらに追加の情報が付与されていることがある。
* 例えば、テキストデータの個々の単語に対して、品詞がラベル付けされているケースなどがある。
* その場合、コンピュータが扱いやすいようにコーパスが機構造などのデータ形式として構造化されていることが一般的である。
* 自然言語処理分野で用いられるコーパスには様々なものが存在し、WikipediaやGoogle Newsなどのテキストデータや、シェクスピアや夏目漱石などの偉大な作家の作品群がコーパスとして有名である。
* ここではまずはじめに1文からなる単純な以下のようなテキストをコーパスとして利用する。
    ```python
    >>> text = 'You say goodbye and I say hello.'
    ```
* 次にPythonの対話モードで小さなテキストデータに前処理(テキストデータを単語に分割し、その分割した単語を単語IDのリストへ変換)を行う。
    ```python
    >>> text = text.lower()
    >>> text = text.replace('.', '. ')
    >>> text
    'you say goodbye and i say hello .'

    >>> words = text,split(' ')
    >>> words
    ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
    ```
* 上記では、まずlower()で全て小文字に変換し、ピリオドの前にスペースを挿入する。
    * 実際はより賢く汎用的なやり方として正規表現を使用する方法がある。
    * reモジュールをimportし、re.split('(\W+)?', text)とすることで単語単位に分割できる。
* 次にsplit()でスペースを区切り文字として、単語に分割し、wordsというリストに各単語を要素として保持する。
* これで元の文章を単語のリストとして使用できるようになる。
* ただ単語をテキストのまま操作するのは不便なため、単語にIDを振り、IDのリストとして使用できるようにする。
* まず以下の通り、単語のIDと単語の対応表をPythonのディクショナリで作成する。
    ```python
    >>> word_to_id = {}
    >>> id_to_word = {}
    >>>
    >>> for word in words:
            if word not in word_to_id:
                new_id = len(word_to_id)
                word_to_id[word] = new_id
                id_to_word[new_id] = word
    ```
* id_to_wordにはキーを単語ID、値を単語として保持し、word_to_idにはキーを単語、値を単語IDとして保持する。
* if文では単語がword_to_idにまだ保持されていない場合にword_to_idの要素数をnew_idとして払い出し、word_to_idにそのnew_idを値として追加する。
* id_to_wordにはそのnew_idをキーとしてwordを追加する。
* 上述のtextの文はid_to_word、word_to_idにそれぞれ以下のように保持される。
    ```python
    >>> id_to_word
    {0:'you', 1:'say', 2:'goodbye', 3:'and', 4:'i', 5:'hello', 6:'.'}
    >>> word_to_id
    {'you':0, 'say':1, 'goodbye':2, 'and':3, 'i':4, 'hello':5, '.':6}
    ```
* これらのディクショナリを使うことにより、単語から単語IDを検索したり、単語IDから単語を検索したりできる。
    ```python
    >>> id_to_word[1]
    'say'
    >>> word_to_id['hello ']
    5
    ```
* 最後に以下の通り、単語のリストwordsを単語IDのリストcorpusに変換し、その結果をNumpy配列にして保持する。
    ```python
    >>> import numpy as np
    >>> corpus = [word_to_id[w] for w in words]
    >>> corpus = np.array(corpus)
    >>> corpus
    array([0,1, 2, 3, 4, 1, 5, 6])
    ```
* for文の内包表記ではwordsの先頭から要素の値を順に取り出してwに格納し、そのwをキーとしてword_to_idの値、つまり単語IDを取得する。
* 取得した単語idはcorpusにリストとして保持される。
* 次の処理でcorpusはnp.array()でNumpy配列に変換される。
* ここまでの処理を前処理として、preprocess()という関数にしてまとめて実装すると、以下のようになる。
    ```python
    def preprocess(text):
        text = text.lower()
        text = text.replace('.', '. ')
        words = text,split(' ')

        word_to_id = {}
        id_to_word = {}
        for word in words:
            if word not in word_to_id:
                new_id = len(word_to_id)
                word_to_id[word] = new_id
                id_to_word[new_id] = word

        corpus = np.array([word_to_id[w] for w in words])

    return corpus, word_to_id, id_to_word
    ```
* この関数を使用すると、コーパスの前処理は以下のようになる。
    ```python
    >>> text = 'You say goodbye and I say hello.'
    >>> corpus, word_to_id, id_to_word = preprocess(text)
    ```
* ここまででコーパスを扱う準備が整ったので、これを用い、単語の意味を抽出する。
* ここでカウントベースの手法を用い、単語をベクトルで表すことを行なう。
* ベクトル表現の例として「色」を考える。
* ★★～P.66★★




# Method
* P.64：text.lower()
* P.64：text.replace('.', '. ')
* P.64：text,split(' ')
* P.66：[word_to_id[w] for w in words]　※内包表記








